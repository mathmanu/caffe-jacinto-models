Logging output to training/cifar10_jacintonet11v2_2017-06-28_18-56-45/train-log_2017-06-28_18-56-45.txt
I0628 18:56:54.560250  7562 caffe.cpp:608] This is NVCaffe 0.16.2 started at Wed Jun 28 18:56:53 2017
I0628 18:56:54.560612  7562 caffe.cpp:611] CuDNN version: 6.0.21
I0628 18:56:54.560632  7562 caffe.cpp:612] CuBLAS version: 8000
I0628 18:56:54.560642  7562 caffe.cpp:613] CUDA version: 8000
I0628 18:56:54.560652  7562 caffe.cpp:614] CUDA driver version: 8000
I0628 18:56:54.807449  7562 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0628 18:56:54.807942  7562 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8277393408, dev_info[0]: total=8506769408 free=8277393408
I0628 18:56:54.808387  7562 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8277393408, dev_info[1]: total=8508145664 free=8379236352
I0628 18:56:54.808394  7562 caffe.cpp:208] Using GPUs 0, 1
I0628 18:56:54.808652  7562 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0628 18:56:54.808908  7562 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0628 18:56:54.823622  7562 solver.cpp:42] Solver data type: FLOAT
I0628 18:56:54.823729  7562 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
I0628 18:56:54.849927  7562 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/train.prototxt
I0628 18:56:54.852391  7562 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0628 18:56:54.852425  7562 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0628 18:56:54.853091  7562 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 32
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0628 18:56:54.853525  7562 net.cpp:108] Using FLOAT as default forward math type
I0628 18:56:54.853585  7562 net.cpp:114] Using FLOAT as default backward math type
I0628 18:56:54.853600  7562 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0628 18:56:54.853613  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:54.858359  7562 net.cpp:183] Created Layer data (0)
I0628 18:56:54.858391  7562 net.cpp:529] data -> data
I0628 18:56:54.858417  7562 net.cpp:529] data -> label
I0628 18:56:54.858469  7562 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 32
I0628 18:56:54.858508  7562 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 18:56:54.893507  7613 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0628 18:56:54.911458  7562 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 18:56:54.911697  7562 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 18:56:54.911722  7562 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 18:56:54.911837  7562 net.cpp:244] Setting up data
I0628 18:56:54.911880  7562 net.cpp:251] TRAIN Top shape for layer 0 'data' 32 3 32 32 (98304)
I0628 18:56:54.911906  7562 net.cpp:251] TRAIN Top shape for layer 0 'data' 32 (32)
I0628 18:56:54.911929  7562 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0628 18:56:54.911947  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:54.912303  7562 net.cpp:183] Created Layer data/bias (1)
I0628 18:56:54.912329  7562 net.cpp:560] data/bias <- data
I0628 18:56:54.912354  7562 net.cpp:529] data/bias -> data/bias
I0628 18:56:54.919833  7562 net.cpp:244] Setting up data/bias
I0628 18:56:54.919883  7562 net.cpp:251] TRAIN Top shape for layer 1 'data/bias' 32 3 32 32 (98304)
I0628 18:56:54.919914  7562 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0628 18:56:54.919929  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:54.919977  7562 net.cpp:183] Created Layer conv1a (2)
I0628 18:56:54.919994  7562 net.cpp:560] conv1a <- data/bias
I0628 18:56:54.920008  7562 net.cpp:529] conv1a -> conv1a
I0628 18:56:55.692991  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 8.15G, req 0G)
I0628 18:56:55.693027  7562 net.cpp:244] Setting up conv1a
I0628 18:56:55.693035  7562 net.cpp:251] TRAIN Top shape for layer 2 'conv1a' 32 32 32 32 (1048576)
I0628 18:56:55.693044  7562 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0628 18:56:55.693049  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.693059  7562 net.cpp:183] Created Layer conv1a/bn (3)
I0628 18:56:55.693063  7562 net.cpp:560] conv1a/bn <- conv1a
I0628 18:56:55.693068  7562 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0628 18:56:55.694069  7562 net.cpp:244] Setting up conv1a/bn
I0628 18:56:55.694077  7562 net.cpp:251] TRAIN Top shape for layer 3 'conv1a/bn' 32 32 32 32 (1048576)
I0628 18:56:55.694085  7562 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0628 18:56:55.694088  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.694093  7562 net.cpp:183] Created Layer conv1a/relu (4)
I0628 18:56:55.694095  7562 net.cpp:560] conv1a/relu <- conv1a
I0628 18:56:55.694098  7562 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0628 18:56:55.694650  7562 net.cpp:244] Setting up conv1a/relu
I0628 18:56:55.694655  7562 net.cpp:251] TRAIN Top shape for layer 4 'conv1a/relu' 32 32 32 32 (1048576)
I0628 18:56:55.694658  7562 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0628 18:56:55.694660  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.694672  7562 net.cpp:183] Created Layer conv1b (5)
I0628 18:56:55.694674  7562 net.cpp:560] conv1b <- conv1a
I0628 18:56:55.694677  7562 net.cpp:529] conv1b -> conv1b
I0628 18:56:55.702595  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0628 18:56:55.702610  7562 net.cpp:244] Setting up conv1b
I0628 18:56:55.702615  7562 net.cpp:251] TRAIN Top shape for layer 5 'conv1b' 32 32 32 32 (1048576)
I0628 18:56:55.702621  7562 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0628 18:56:55.702623  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.702628  7562 net.cpp:183] Created Layer conv1b/bn (6)
I0628 18:56:55.702630  7562 net.cpp:560] conv1b/bn <- conv1b
I0628 18:56:55.702632  7562 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0628 18:56:55.703147  7562 net.cpp:244] Setting up conv1b/bn
I0628 18:56:55.703155  7562 net.cpp:251] TRAIN Top shape for layer 6 'conv1b/bn' 32 32 32 32 (1048576)
I0628 18:56:55.703161  7562 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0628 18:56:55.703171  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.703174  7562 net.cpp:183] Created Layer conv1b/relu (7)
I0628 18:56:55.703176  7562 net.cpp:560] conv1b/relu <- conv1b
I0628 18:56:55.703181  7562 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0628 18:56:55.703183  7562 net.cpp:244] Setting up conv1b/relu
I0628 18:56:55.703186  7562 net.cpp:251] TRAIN Top shape for layer 7 'conv1b/relu' 32 32 32 32 (1048576)
I0628 18:56:55.703188  7562 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0628 18:56:55.703191  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.703204  7562 net.cpp:183] Created Layer pool1 (8)
I0628 18:56:55.703208  7562 net.cpp:560] pool1 <- conv1b
I0628 18:56:55.703212  7562 net.cpp:529] pool1 -> pool1
I0628 18:56:55.703276  7562 net.cpp:244] Setting up pool1
I0628 18:56:55.703284  7562 net.cpp:251] TRAIN Top shape for layer 8 'pool1' 32 32 32 32 (1048576)
I0628 18:56:55.703287  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0628 18:56:55.703291  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.703301  7562 net.cpp:183] Created Layer res2a_branch2a (9)
I0628 18:56:55.703305  7562 net.cpp:560] res2a_branch2a <- pool1
I0628 18:56:55.703308  7562 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0628 18:56:55.716311  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0G)
I0628 18:56:55.716329  7562 net.cpp:244] Setting up res2a_branch2a
I0628 18:56:55.716336  7562 net.cpp:251] TRAIN Top shape for layer 9 'res2a_branch2a' 32 64 32 32 (2097152)
I0628 18:56:55.716346  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.716351  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.716359  7562 net.cpp:183] Created Layer res2a_branch2a/bn (10)
I0628 18:56:55.716362  7562 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0628 18:56:55.716367  7562 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0628 18:56:55.716892  7562 net.cpp:244] Setting up res2a_branch2a/bn
I0628 18:56:55.716900  7562 net.cpp:251] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 32 64 32 32 (2097152)
I0628 18:56:55.716909  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.716912  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.716917  7562 net.cpp:183] Created Layer res2a_branch2a/relu (11)
I0628 18:56:55.716922  7562 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0628 18:56:55.716925  7562 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0628 18:56:55.716930  7562 net.cpp:244] Setting up res2a_branch2a/relu
I0628 18:56:55.716935  7562 net.cpp:251] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 32 64 32 32 (2097152)
I0628 18:56:55.716939  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0628 18:56:55.716943  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.716951  7562 net.cpp:183] Created Layer res2a_branch2b (12)
I0628 18:56:55.716955  7562 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0628 18:56:55.716958  7562 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0628 18:56:55.722749  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.09G, req 0G)
I0628 18:56:55.722776  7562 net.cpp:244] Setting up res2a_branch2b
I0628 18:56:55.722784  7562 net.cpp:251] TRAIN Top shape for layer 12 'res2a_branch2b' 32 64 32 32 (2097152)
I0628 18:56:55.722793  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.722798  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.722818  7562 net.cpp:183] Created Layer res2a_branch2b/bn (13)
I0628 18:56:55.722825  7562 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0628 18:56:55.722831  7562 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0628 18:56:55.723387  7562 net.cpp:244] Setting up res2a_branch2b/bn
I0628 18:56:55.723394  7562 net.cpp:251] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 32 64 32 32 (2097152)
I0628 18:56:55.723403  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.723407  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.723413  7562 net.cpp:183] Created Layer res2a_branch2b/relu (14)
I0628 18:56:55.723417  7562 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0628 18:56:55.723420  7562 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0628 18:56:55.723428  7562 net.cpp:244] Setting up res2a_branch2b/relu
I0628 18:56:55.723433  7562 net.cpp:251] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 32 64 32 32 (2097152)
I0628 18:56:55.723436  7562 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0628 18:56:55.723440  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.723446  7562 net.cpp:183] Created Layer pool2 (15)
I0628 18:56:55.723450  7562 net.cpp:560] pool2 <- res2a_branch2b
I0628 18:56:55.723454  7562 net.cpp:529] pool2 -> pool2
I0628 18:56:55.723505  7562 net.cpp:244] Setting up pool2
I0628 18:56:55.723510  7562 net.cpp:251] TRAIN Top shape for layer 15 'pool2' 32 64 16 16 (524288)
I0628 18:56:55.723515  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0628 18:56:55.723520  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.723529  7562 net.cpp:183] Created Layer res3a_branch2a (16)
I0628 18:56:55.723532  7562 net.cpp:560] res3a_branch2a <- pool2
I0628 18:56:55.723536  7562 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0628 18:56:55.735329  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.08G, req 0.02G)
I0628 18:56:55.735349  7562 net.cpp:244] Setting up res3a_branch2a
I0628 18:56:55.735357  7562 net.cpp:251] TRAIN Top shape for layer 16 'res3a_branch2a' 32 128 16 16 (1048576)
I0628 18:56:55.735365  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.735370  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.735379  7562 net.cpp:183] Created Layer res3a_branch2a/bn (17)
I0628 18:56:55.735383  7562 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0628 18:56:55.735388  7562 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0628 18:56:55.735903  7562 net.cpp:244] Setting up res3a_branch2a/bn
I0628 18:56:55.735910  7562 net.cpp:251] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 32 128 16 16 (1048576)
I0628 18:56:55.735924  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.735927  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.735932  7562 net.cpp:183] Created Layer res3a_branch2a/relu (18)
I0628 18:56:55.735936  7562 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0628 18:56:55.735940  7562 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0628 18:56:55.735946  7562 net.cpp:244] Setting up res3a_branch2a/relu
I0628 18:56:55.735951  7562 net.cpp:251] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 32 128 16 16 (1048576)
I0628 18:56:55.735955  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0628 18:56:55.735960  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.735970  7562 net.cpp:183] Created Layer res3a_branch2b (19)
I0628 18:56:55.735972  7562 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0628 18:56:55.735976  7562 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0628 18:56:55.740900  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.07G, req 0.02G)
I0628 18:56:55.740916  7562 net.cpp:244] Setting up res3a_branch2b
I0628 18:56:55.740924  7562 net.cpp:251] TRAIN Top shape for layer 19 'res3a_branch2b' 32 128 16 16 (1048576)
I0628 18:56:55.740931  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.740936  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.740943  7562 net.cpp:183] Created Layer res3a_branch2b/bn (20)
I0628 18:56:55.740947  7562 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0628 18:56:55.740950  7562 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0628 18:56:55.741466  7562 net.cpp:244] Setting up res3a_branch2b/bn
I0628 18:56:55.741473  7562 net.cpp:251] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 32 128 16 16 (1048576)
I0628 18:56:55.741482  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.741485  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.741490  7562 net.cpp:183] Created Layer res3a_branch2b/relu (21)
I0628 18:56:55.741494  7562 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0628 18:56:55.741497  7562 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0628 18:56:55.741503  7562 net.cpp:244] Setting up res3a_branch2b/relu
I0628 18:56:55.741508  7562 net.cpp:251] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 32 128 16 16 (1048576)
I0628 18:56:55.741513  7562 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0628 18:56:55.741518  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.741523  7562 net.cpp:183] Created Layer pool3 (22)
I0628 18:56:55.741526  7562 net.cpp:560] pool3 <- res3a_branch2b
I0628 18:56:55.741530  7562 net.cpp:529] pool3 -> pool3
I0628 18:56:55.741586  7562 net.cpp:244] Setting up pool3
I0628 18:56:55.741592  7562 net.cpp:251] TRAIN Top shape for layer 22 'pool3' 32 128 16 16 (1048576)
I0628 18:56:55.741597  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0628 18:56:55.741601  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.741610  7562 net.cpp:183] Created Layer res4a_branch2a (23)
I0628 18:56:55.741613  7562 net.cpp:560] res4a_branch2a <- pool3
I0628 18:56:55.741616  7562 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0628 18:56:55.764475  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.04G, req 0.02G)
I0628 18:56:55.764503  7562 net.cpp:244] Setting up res4a_branch2a
I0628 18:56:55.764513  7562 net.cpp:251] TRAIN Top shape for layer 23 'res4a_branch2a' 32 256 16 16 (2097152)
I0628 18:56:55.764524  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.764529  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.764541  7562 net.cpp:183] Created Layer res4a_branch2a/bn (24)
I0628 18:56:55.764545  7562 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0628 18:56:55.764550  7562 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0628 18:56:55.765112  7562 net.cpp:244] Setting up res4a_branch2a/bn
I0628 18:56:55.765120  7562 net.cpp:251] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 32 256 16 16 (2097152)
I0628 18:56:55.765130  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.765133  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.765138  7562 net.cpp:183] Created Layer res4a_branch2a/relu (25)
I0628 18:56:55.765142  7562 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0628 18:56:55.765146  7562 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0628 18:56:55.765161  7562 net.cpp:244] Setting up res4a_branch2a/relu
I0628 18:56:55.765166  7562 net.cpp:251] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 32 256 16 16 (2097152)
I0628 18:56:55.765171  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0628 18:56:55.765174  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.765184  7562 net.cpp:183] Created Layer res4a_branch2b (26)
I0628 18:56:55.765187  7562 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0628 18:56:55.765192  7562 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0628 18:56:55.775107  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.02G, req 0.02G)
I0628 18:56:55.775125  7562 net.cpp:244] Setting up res4a_branch2b
I0628 18:56:55.775132  7562 net.cpp:251] TRAIN Top shape for layer 26 'res4a_branch2b' 32 256 16 16 (2097152)
I0628 18:56:55.775140  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.775143  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.775151  7562 net.cpp:183] Created Layer res4a_branch2b/bn (27)
I0628 18:56:55.775154  7562 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0628 18:56:55.775158  7562 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0628 18:56:55.775673  7562 net.cpp:244] Setting up res4a_branch2b/bn
I0628 18:56:55.775681  7562 net.cpp:251] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 32 256 16 16 (2097152)
I0628 18:56:55.775689  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.775693  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.775698  7562 net.cpp:183] Created Layer res4a_branch2b/relu (28)
I0628 18:56:55.775702  7562 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0628 18:56:55.775707  7562 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0628 18:56:55.775712  7562 net.cpp:244] Setting up res4a_branch2b/relu
I0628 18:56:55.775717  7562 net.cpp:251] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 32 256 16 16 (2097152)
I0628 18:56:55.775720  7562 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0628 18:56:55.775724  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.775730  7562 net.cpp:183] Created Layer pool4 (29)
I0628 18:56:55.775733  7562 net.cpp:560] pool4 <- res4a_branch2b
I0628 18:56:55.775738  7562 net.cpp:529] pool4 -> pool4
I0628 18:56:55.775784  7562 net.cpp:244] Setting up pool4
I0628 18:56:55.775790  7562 net.cpp:251] TRAIN Top shape for layer 29 'pool4' 32 256 8 8 (524288)
I0628 18:56:55.775794  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0628 18:56:55.775799  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.775807  7562 net.cpp:183] Created Layer res5a_branch2a (30)
I0628 18:56:55.775810  7562 net.cpp:560] res5a_branch2a <- pool4
I0628 18:56:55.775815  7562 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0628 18:56:55.821928  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8G, req 0.02G)
I0628 18:56:55.821954  7562 net.cpp:244] Setting up res5a_branch2a
I0628 18:56:55.821964  7562 net.cpp:251] TRAIN Top shape for layer 30 'res5a_branch2a' 32 512 8 8 (1048576)
I0628 18:56:55.821974  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.821979  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.821988  7562 net.cpp:183] Created Layer res5a_branch2a/bn (31)
I0628 18:56:55.821992  7562 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0628 18:56:55.821997  7562 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0628 18:56:55.822541  7562 net.cpp:244] Setting up res5a_branch2a/bn
I0628 18:56:55.822556  7562 net.cpp:251] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 32 512 8 8 (1048576)
I0628 18:56:55.822566  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.822571  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.822576  7562 net.cpp:183] Created Layer res5a_branch2a/relu (32)
I0628 18:56:55.822580  7562 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0628 18:56:55.822584  7562 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0628 18:56:55.822590  7562 net.cpp:244] Setting up res5a_branch2a/relu
I0628 18:56:55.822595  7562 net.cpp:251] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 32 512 8 8 (1048576)
I0628 18:56:55.822599  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0628 18:56:55.822603  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.822613  7562 net.cpp:183] Created Layer res5a_branch2b (33)
I0628 18:56:55.822615  7562 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0628 18:56:55.822619  7562 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0628 18:56:55.842761  7562 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 5 5  (limit 7.98G, req 0.02G)
I0628 18:56:55.842782  7562 net.cpp:244] Setting up res5a_branch2b
I0628 18:56:55.842789  7562 net.cpp:251] TRAIN Top shape for layer 33 'res5a_branch2b' 32 512 8 8 (1048576)
I0628 18:56:55.842802  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.842805  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.842815  7562 net.cpp:183] Created Layer res5a_branch2b/bn (34)
I0628 18:56:55.842819  7562 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0628 18:56:55.842824  7562 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0628 18:56:55.843360  7562 net.cpp:244] Setting up res5a_branch2b/bn
I0628 18:56:55.843369  7562 net.cpp:251] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 32 512 8 8 (1048576)
I0628 18:56:55.843376  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.843380  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.843385  7562 net.cpp:183] Created Layer res5a_branch2b/relu (35)
I0628 18:56:55.843389  7562 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0628 18:56:55.843394  7562 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0628 18:56:55.843400  7562 net.cpp:244] Setting up res5a_branch2b/relu
I0628 18:56:55.843403  7562 net.cpp:251] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 32 512 8 8 (1048576)
I0628 18:56:55.843407  7562 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0628 18:56:55.843412  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.843418  7562 net.cpp:183] Created Layer pool5 (36)
I0628 18:56:55.843421  7562 net.cpp:560] pool5 <- res5a_branch2b
I0628 18:56:55.843425  7562 net.cpp:529] pool5 -> pool5
I0628 18:56:55.843451  7562 net.cpp:244] Setting up pool5
I0628 18:56:55.843456  7562 net.cpp:251] TRAIN Top shape for layer 36 'pool5' 32 512 1 1 (16384)
I0628 18:56:55.843461  7562 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0628 18:56:55.843464  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.843472  7562 net.cpp:183] Created Layer fc10 (37)
I0628 18:56:55.843477  7562 net.cpp:560] fc10 <- pool5
I0628 18:56:55.843479  7562 net.cpp:529] fc10 -> fc10
I0628 18:56:55.843708  7562 net.cpp:244] Setting up fc10
I0628 18:56:55.843715  7562 net.cpp:251] TRAIN Top shape for layer 37 'fc10' 32 10 (320)
I0628 18:56:55.843722  7562 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0628 18:56:55.843727  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.843992  7562 net.cpp:183] Created Layer loss (38)
I0628 18:56:55.843998  7562 net.cpp:560] loss <- fc10
I0628 18:56:55.844002  7562 net.cpp:560] loss <- label
I0628 18:56:55.844008  7562 net.cpp:529] loss -> loss
I0628 18:56:55.844141  7562 net.cpp:244] Setting up loss
I0628 18:56:55.844147  7562 net.cpp:251] TRAIN Top shape for layer 38 'loss' (1)
I0628 18:56:55.844151  7562 net.cpp:255]     with loss weight 1
I0628 18:56:55.844156  7562 net.cpp:322] loss needs backward computation.
I0628 18:56:55.844161  7562 net.cpp:322] fc10 needs backward computation.
I0628 18:56:55.844164  7562 net.cpp:322] pool5 needs backward computation.
I0628 18:56:55.844167  7562 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0628 18:56:55.844171  7562 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0628 18:56:55.844174  7562 net.cpp:322] res5a_branch2b needs backward computation.
I0628 18:56:55.844178  7562 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0628 18:56:55.844182  7562 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0628 18:56:55.844187  7562 net.cpp:322] res5a_branch2a needs backward computation.
I0628 18:56:55.844190  7562 net.cpp:322] pool4 needs backward computation.
I0628 18:56:55.844194  7562 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0628 18:56:55.844198  7562 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0628 18:56:55.844202  7562 net.cpp:322] res4a_branch2b needs backward computation.
I0628 18:56:55.844207  7562 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0628 18:56:55.844210  7562 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0628 18:56:55.844214  7562 net.cpp:322] res4a_branch2a needs backward computation.
I0628 18:56:55.844218  7562 net.cpp:322] pool3 needs backward computation.
I0628 18:56:55.844221  7562 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0628 18:56:55.844225  7562 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0628 18:56:55.844229  7562 net.cpp:322] res3a_branch2b needs backward computation.
I0628 18:56:55.844233  7562 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0628 18:56:55.844235  7562 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0628 18:56:55.844239  7562 net.cpp:322] res3a_branch2a needs backward computation.
I0628 18:56:55.844244  7562 net.cpp:322] pool2 needs backward computation.
I0628 18:56:55.844247  7562 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0628 18:56:55.844251  7562 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0628 18:56:55.844254  7562 net.cpp:322] res2a_branch2b needs backward computation.
I0628 18:56:55.844259  7562 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0628 18:56:55.844262  7562 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0628 18:56:55.844265  7562 net.cpp:322] res2a_branch2a needs backward computation.
I0628 18:56:55.844269  7562 net.cpp:322] pool1 needs backward computation.
I0628 18:56:55.844272  7562 net.cpp:322] conv1b/relu needs backward computation.
I0628 18:56:55.844276  7562 net.cpp:322] conv1b/bn needs backward computation.
I0628 18:56:55.844280  7562 net.cpp:322] conv1b needs backward computation.
I0628 18:56:55.844285  7562 net.cpp:322] conv1a/relu needs backward computation.
I0628 18:56:55.844288  7562 net.cpp:322] conv1a/bn needs backward computation.
I0628 18:56:55.844291  7562 net.cpp:322] conv1a needs backward computation.
I0628 18:56:55.844296  7562 net.cpp:324] data/bias does not need backward computation.
I0628 18:56:55.844300  7562 net.cpp:324] data does not need backward computation.
I0628 18:56:55.844305  7562 net.cpp:366] This network produces output loss
I0628 18:56:55.844333  7562 net.cpp:388] Top memory (TRAIN) required for data: 176160768 diff: 176160776
I0628 18:56:55.844336  7562 net.cpp:391] Bottom memory (TRAIN) required for data: 176160768 diff: 176160768
I0628 18:56:55.844339  7562 net.cpp:394] Shared (in-place) memory (TRAIN) by data: 117440512 diff: 117440512
I0628 18:56:55.844347  7562 net.cpp:397] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0628 18:56:55.844352  7562 net.cpp:400] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0628 18:56:55.844354  7562 net.cpp:406] Network initialization done.
I0628 18:56:55.844703  7562 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/test.prototxt
I0628 18:56:55.844857  7562 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 25
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0628 18:56:55.844946  7562 net.cpp:108] Using FLOAT as default forward math type
I0628 18:56:55.844951  7562 net.cpp:114] Using FLOAT as default backward math type
I0628 18:56:55.844954  7562 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0628 18:56:55.844959  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.844970  7562 net.cpp:183] Created Layer data (0)
I0628 18:56:55.844974  7562 net.cpp:529] data -> data
I0628 18:56:55.844980  7562 net.cpp:529] data -> label
I0628 18:56:55.844987  7562 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 25
I0628 18:56:55.844996  7562 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 18:56:55.864421  7626 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0628 18:56:55.867936  7562 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 18:56:55.868002  7562 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 18:56:55.868007  7562 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 18:56:55.868029  7562 net.cpp:244] Setting up data
I0628 18:56:55.868036  7562 net.cpp:251] TEST Top shape for layer 0 'data' 25 3 32 32 (76800)
I0628 18:56:55.868041  7562 net.cpp:251] TEST Top shape for layer 0 'data' 25 (25)
I0628 18:56:55.868046  7562 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0628 18:56:55.868049  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.868057  7562 net.cpp:183] Created Layer label_data_1_split (1)
I0628 18:56:55.868062  7562 net.cpp:560] label_data_1_split <- label
I0628 18:56:55.868064  7562 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0628 18:56:55.868068  7562 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0628 18:56:55.868070  7562 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0628 18:56:55.868118  7562 net.cpp:244] Setting up label_data_1_split
I0628 18:56:55.868121  7562 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 18:56:55.868124  7562 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 18:56:55.868126  7562 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 18:56:55.868139  7562 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0628 18:56:55.868141  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.868147  7562 net.cpp:183] Created Layer data/bias (2)
I0628 18:56:55.868149  7562 net.cpp:560] data/bias <- data
I0628 18:56:55.868152  7562 net.cpp:529] data/bias -> data/bias
I0628 18:56:55.868289  7562 net.cpp:244] Setting up data/bias
I0628 18:56:55.868295  7562 net.cpp:251] TEST Top shape for layer 2 'data/bias' 25 3 32 32 (76800)
I0628 18:56:55.868301  7562 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0628 18:56:55.868304  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.868311  7562 net.cpp:183] Created Layer conv1a (3)
I0628 18:56:55.868314  7562 net.cpp:560] conv1a <- data/bias
I0628 18:56:55.868316  7562 net.cpp:529] conv1a -> conv1a
I0628 18:56:55.869148  7627 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 18:56:55.869156  7627 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 18:56:55.870039  7627 data_layer.cpp:110] [0] Parser threads: 1
I0628 18:56:55.870048  7627 data_layer.cpp:112] [0] Transformer threads: 1
I0628 18:56:55.871743  7562 net.cpp:244] Setting up conv1a
I0628 18:56:55.871754  7562 net.cpp:251] TEST Top shape for layer 3 'conv1a' 25 32 32 32 (819200)
I0628 18:56:55.871762  7562 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0628 18:56:55.871764  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.871770  7562 net.cpp:183] Created Layer conv1a/bn (4)
I0628 18:56:55.871773  7562 net.cpp:560] conv1a/bn <- conv1a
I0628 18:56:55.871776  7562 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0628 18:56:55.872804  7562 net.cpp:244] Setting up conv1a/bn
I0628 18:56:55.872814  7562 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 25 32 32 32 (819200)
I0628 18:56:55.872822  7562 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0628 18:56:55.872824  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.872833  7562 net.cpp:183] Created Layer conv1a/relu (5)
I0628 18:56:55.872835  7562 net.cpp:560] conv1a/relu <- conv1a
I0628 18:56:55.872838  7562 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0628 18:56:55.872843  7562 net.cpp:244] Setting up conv1a/relu
I0628 18:56:55.872845  7562 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 25 32 32 32 (819200)
I0628 18:56:55.872849  7562 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0628 18:56:55.872853  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.872860  7562 net.cpp:183] Created Layer conv1b (6)
I0628 18:56:55.872864  7562 net.cpp:560] conv1b <- conv1a
I0628 18:56:55.872865  7562 net.cpp:529] conv1b -> conv1b
I0628 18:56:55.875077  7562 net.cpp:244] Setting up conv1b
I0628 18:56:55.875088  7562 net.cpp:251] TEST Top shape for layer 6 'conv1b' 25 32 32 32 (819200)
I0628 18:56:55.875095  7562 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0628 18:56:55.875098  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.875103  7562 net.cpp:183] Created Layer conv1b/bn (7)
I0628 18:56:55.875107  7562 net.cpp:560] conv1b/bn <- conv1b
I0628 18:56:55.875109  7562 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0628 18:56:55.876184  7562 net.cpp:244] Setting up conv1b/bn
I0628 18:56:55.876194  7562 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 25 32 32 32 (819200)
I0628 18:56:55.876201  7562 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0628 18:56:55.876205  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.876209  7562 net.cpp:183] Created Layer conv1b/relu (8)
I0628 18:56:55.876220  7562 net.cpp:560] conv1b/relu <- conv1b
I0628 18:56:55.876224  7562 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0628 18:56:55.876229  7562 net.cpp:244] Setting up conv1b/relu
I0628 18:56:55.876231  7562 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 25 32 32 32 (819200)
I0628 18:56:55.876235  7562 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0628 18:56:55.876237  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.876243  7562 net.cpp:183] Created Layer pool1 (9)
I0628 18:56:55.876247  7562 net.cpp:560] pool1 <- conv1b
I0628 18:56:55.876250  7562 net.cpp:529] pool1 -> pool1
I0628 18:56:55.876307  7562 net.cpp:244] Setting up pool1
I0628 18:56:55.876312  7562 net.cpp:251] TEST Top shape for layer 9 'pool1' 25 32 32 32 (819200)
I0628 18:56:55.876314  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0628 18:56:55.876317  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.876329  7562 net.cpp:183] Created Layer res2a_branch2a (10)
I0628 18:56:55.876332  7562 net.cpp:560] res2a_branch2a <- pool1
I0628 18:56:55.876335  7562 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0628 18:56:55.879525  7562 net.cpp:244] Setting up res2a_branch2a
I0628 18:56:55.879537  7562 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 25 64 32 32 (1638400)
I0628 18:56:55.879545  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.879549  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.879554  7562 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0628 18:56:55.879557  7562 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0628 18:56:55.879560  7562 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0628 18:56:55.880750  7562 net.cpp:244] Setting up res2a_branch2a/bn
I0628 18:56:55.880761  7562 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 25 64 32 32 (1638400)
I0628 18:56:55.880769  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.880772  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.880776  7562 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0628 18:56:55.880779  7562 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0628 18:56:55.880781  7562 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0628 18:56:55.880786  7562 net.cpp:244] Setting up res2a_branch2a/relu
I0628 18:56:55.880790  7562 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 25 64 32 32 (1638400)
I0628 18:56:55.880794  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0628 18:56:55.880795  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.880808  7562 net.cpp:183] Created Layer res2a_branch2b (13)
I0628 18:56:55.880812  7562 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0628 18:56:55.880815  7562 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0628 18:56:55.883154  7562 net.cpp:244] Setting up res2a_branch2b
I0628 18:56:55.883168  7562 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 25 64 32 32 (1638400)
I0628 18:56:55.883175  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.883179  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.883191  7562 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0628 18:56:55.883194  7562 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0628 18:56:55.883198  7562 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0628 18:56:55.884416  7562 net.cpp:244] Setting up res2a_branch2b/bn
I0628 18:56:55.884428  7562 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 25 64 32 32 (1638400)
I0628 18:56:55.884435  7562 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.884450  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.884455  7562 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0628 18:56:55.884459  7562 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0628 18:56:55.884461  7562 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0628 18:56:55.884466  7562 net.cpp:244] Setting up res2a_branch2b/relu
I0628 18:56:55.884470  7562 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 25 64 32 32 (1638400)
I0628 18:56:55.884474  7562 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0628 18:56:55.884476  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.884481  7562 net.cpp:183] Created Layer pool2 (16)
I0628 18:56:55.884485  7562 net.cpp:560] pool2 <- res2a_branch2b
I0628 18:56:55.884487  7562 net.cpp:529] pool2 -> pool2
I0628 18:56:55.884563  7562 net.cpp:244] Setting up pool2
I0628 18:56:55.884569  7562 net.cpp:251] TEST Top shape for layer 16 'pool2' 25 64 16 16 (409600)
I0628 18:56:55.884572  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0628 18:56:55.884575  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.884582  7562 net.cpp:183] Created Layer res3a_branch2a (17)
I0628 18:56:55.884585  7562 net.cpp:560] res3a_branch2a <- pool2
I0628 18:56:55.884588  7562 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0628 18:56:55.889614  7562 net.cpp:244] Setting up res3a_branch2a
I0628 18:56:55.889626  7562 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 25 128 16 16 (819200)
I0628 18:56:55.889631  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.889633  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.889638  7562 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0628 18:56:55.889641  7562 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0628 18:56:55.889643  7562 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0628 18:56:55.891186  7562 net.cpp:244] Setting up res3a_branch2a/bn
I0628 18:56:55.891196  7562 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 25 128 16 16 (819200)
I0628 18:56:55.891204  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.891207  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.891211  7562 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0628 18:56:55.891213  7562 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0628 18:56:55.891216  7562 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0628 18:56:55.891219  7562 net.cpp:244] Setting up res3a_branch2a/relu
I0628 18:56:55.891222  7562 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 25 128 16 16 (819200)
I0628 18:56:55.891225  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0628 18:56:55.891228  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.891238  7562 net.cpp:183] Created Layer res3a_branch2b (20)
I0628 18:56:55.891243  7562 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0628 18:56:55.891247  7562 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0628 18:56:55.893456  7562 net.cpp:244] Setting up res3a_branch2b
I0628 18:56:55.893466  7562 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 25 128 16 16 (819200)
I0628 18:56:55.893471  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.893473  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.893478  7562 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0628 18:56:55.893481  7562 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0628 18:56:55.893483  7562 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0628 18:56:55.894455  7562 net.cpp:244] Setting up res3a_branch2b/bn
I0628 18:56:55.894465  7562 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 25 128 16 16 (819200)
I0628 18:56:55.894471  7562 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.894474  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.894476  7562 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0628 18:56:55.894479  7562 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0628 18:56:55.894481  7562 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0628 18:56:55.894485  7562 net.cpp:244] Setting up res3a_branch2b/relu
I0628 18:56:55.894487  7562 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 25 128 16 16 (819200)
I0628 18:56:55.894490  7562 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0628 18:56:55.894492  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.894496  7562 net.cpp:183] Created Layer pool3 (23)
I0628 18:56:55.894500  7562 net.cpp:560] pool3 <- res3a_branch2b
I0628 18:56:55.894505  7562 net.cpp:529] pool3 -> pool3
I0628 18:56:55.894560  7562 net.cpp:244] Setting up pool3
I0628 18:56:55.894567  7562 net.cpp:251] TEST Top shape for layer 23 'pool3' 25 128 16 16 (819200)
I0628 18:56:55.894568  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0628 18:56:55.894572  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.894577  7562 net.cpp:183] Created Layer res4a_branch2a (24)
I0628 18:56:55.894578  7562 net.cpp:560] res4a_branch2a <- pool3
I0628 18:56:55.894582  7562 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0628 18:56:55.905725  7562 net.cpp:244] Setting up res4a_branch2a
I0628 18:56:55.905735  7562 net.cpp:251] TEST Top shape for layer 24 'res4a_branch2a' 25 256 16 16 (1638400)
I0628 18:56:55.905740  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.905743  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.905747  7562 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0628 18:56:55.905750  7562 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0628 18:56:55.905752  7562 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0628 18:56:55.906728  7562 net.cpp:244] Setting up res4a_branch2a/bn
I0628 18:56:55.906735  7562 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a/bn' 25 256 16 16 (1638400)
I0628 18:56:55.906743  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.906744  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.906749  7562 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0628 18:56:55.906750  7562 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0628 18:56:55.906752  7562 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0628 18:56:55.906756  7562 net.cpp:244] Setting up res4a_branch2a/relu
I0628 18:56:55.906759  7562 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/relu' 25 256 16 16 (1638400)
I0628 18:56:55.906761  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0628 18:56:55.906764  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.906771  7562 net.cpp:183] Created Layer res4a_branch2b (27)
I0628 18:56:55.906775  7562 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0628 18:56:55.906780  7562 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0628 18:56:55.911975  7562 net.cpp:244] Setting up res4a_branch2b
I0628 18:56:55.911988  7562 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2b' 25 256 16 16 (1638400)
I0628 18:56:55.911995  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.911999  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.912014  7562 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0628 18:56:55.912019  7562 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0628 18:56:55.912024  7562 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0628 18:56:55.913183  7562 net.cpp:244] Setting up res4a_branch2b/bn
I0628 18:56:55.913193  7562 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b/bn' 25 256 16 16 (1638400)
I0628 18:56:55.913199  7562 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.913203  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.913206  7562 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0628 18:56:55.913208  7562 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0628 18:56:55.913211  7562 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0628 18:56:55.913215  7562 net.cpp:244] Setting up res4a_branch2b/relu
I0628 18:56:55.913218  7562 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/relu' 25 256 16 16 (1638400)
I0628 18:56:55.913221  7562 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0628 18:56:55.913223  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.913228  7562 net.cpp:183] Created Layer pool4 (30)
I0628 18:56:55.913230  7562 net.cpp:560] pool4 <- res4a_branch2b
I0628 18:56:55.913233  7562 net.cpp:529] pool4 -> pool4
I0628 18:56:55.913285  7562 net.cpp:244] Setting up pool4
I0628 18:56:55.913290  7562 net.cpp:251] TEST Top shape for layer 30 'pool4' 25 256 8 8 (409600)
I0628 18:56:55.913292  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0628 18:56:55.913295  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.913307  7562 net.cpp:183] Created Layer res5a_branch2a (31)
I0628 18:56:55.913310  7562 net.cpp:560] res5a_branch2a <- pool4
I0628 18:56:55.913312  7562 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0628 18:56:55.944370  7562 net.cpp:244] Setting up res5a_branch2a
I0628 18:56:55.944391  7562 net.cpp:251] TEST Top shape for layer 31 'res5a_branch2a' 25 512 8 8 (819200)
I0628 18:56:55.944398  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0628 18:56:55.944402  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.944412  7562 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0628 18:56:55.944416  7562 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0628 18:56:55.944418  7562 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0628 18:56:55.945488  7562 net.cpp:244] Setting up res5a_branch2a/bn
I0628 18:56:55.945497  7562 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a/bn' 25 512 8 8 (819200)
I0628 18:56:55.945503  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0628 18:56:55.945507  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.945509  7562 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0628 18:56:55.945513  7562 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0628 18:56:55.945514  7562 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0628 18:56:55.945519  7562 net.cpp:244] Setting up res5a_branch2a/relu
I0628 18:56:55.945521  7562 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/relu' 25 512 8 8 (819200)
I0628 18:56:55.945524  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0628 18:56:55.945526  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.945533  7562 net.cpp:183] Created Layer res5a_branch2b (34)
I0628 18:56:55.945536  7562 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0628 18:56:55.945538  7562 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0628 18:56:55.961493  7562 net.cpp:244] Setting up res5a_branch2b
I0628 18:56:55.961519  7562 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2b' 25 512 8 8 (819200)
I0628 18:56:55.961532  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0628 18:56:55.961535  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.961542  7562 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0628 18:56:55.961545  7562 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0628 18:56:55.961557  7562 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0628 18:56:55.962556  7562 net.cpp:244] Setting up res5a_branch2b/bn
I0628 18:56:55.962564  7562 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b/bn' 25 512 8 8 (819200)
I0628 18:56:55.962571  7562 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0628 18:56:55.962574  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.962577  7562 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0628 18:56:55.962579  7562 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0628 18:56:55.962581  7562 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0628 18:56:55.962586  7562 net.cpp:244] Setting up res5a_branch2b/relu
I0628 18:56:55.962589  7562 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/relu' 25 512 8 8 (819200)
I0628 18:56:55.962591  7562 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0628 18:56:55.962594  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.962604  7562 net.cpp:183] Created Layer pool5 (37)
I0628 18:56:55.962606  7562 net.cpp:560] pool5 <- res5a_branch2b
I0628 18:56:55.962610  7562 net.cpp:529] pool5 -> pool5
I0628 18:56:55.962635  7562 net.cpp:244] Setting up pool5
I0628 18:56:55.962637  7562 net.cpp:251] TEST Top shape for layer 37 'pool5' 25 512 1 1 (12800)
I0628 18:56:55.962640  7562 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0628 18:56:55.962642  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.962646  7562 net.cpp:183] Created Layer fc10 (38)
I0628 18:56:55.962648  7562 net.cpp:560] fc10 <- pool5
I0628 18:56:55.962651  7562 net.cpp:529] fc10 -> fc10
I0628 18:56:55.962893  7562 net.cpp:244] Setting up fc10
I0628 18:56:55.962899  7562 net.cpp:251] TEST Top shape for layer 38 'fc10' 25 10 (250)
I0628 18:56:55.962903  7562 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0628 18:56:55.962906  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.962909  7562 net.cpp:183] Created Layer fc10_fc10_0_split (39)
I0628 18:56:55.962911  7562 net.cpp:560] fc10_fc10_0_split <- fc10
I0628 18:56:55.962913  7562 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0628 18:56:55.962916  7562 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0628 18:56:55.962919  7562 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0628 18:56:55.962968  7562 net.cpp:244] Setting up fc10_fc10_0_split
I0628 18:56:55.962972  7562 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 18:56:55.962975  7562 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 18:56:55.962977  7562 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 18:56:55.962980  7562 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0628 18:56:55.962981  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.962985  7562 net.cpp:183] Created Layer loss (40)
I0628 18:56:55.962987  7562 net.cpp:560] loss <- fc10_fc10_0_split_0
I0628 18:56:55.962990  7562 net.cpp:560] loss <- label_data_1_split_0
I0628 18:56:55.962992  7562 net.cpp:529] loss -> loss
I0628 18:56:55.963107  7562 net.cpp:244] Setting up loss
I0628 18:56:55.963114  7562 net.cpp:251] TEST Top shape for layer 40 'loss' (1)
I0628 18:56:55.963121  7562 net.cpp:255]     with loss weight 1
I0628 18:56:55.963126  7562 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0628 18:56:55.963129  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.963137  7562 net.cpp:183] Created Layer accuracy/top1 (41)
I0628 18:56:55.963140  7562 net.cpp:560] accuracy/top1 <- fc10_fc10_0_split_1
I0628 18:56:55.963142  7562 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0628 18:56:55.963145  7562 net.cpp:529] accuracy/top1 -> accuracy/top1
I0628 18:56:55.963150  7562 net.cpp:244] Setting up accuracy/top1
I0628 18:56:55.963152  7562 net.cpp:251] TEST Top shape for layer 41 'accuracy/top1' (1)
I0628 18:56:55.963155  7562 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0628 18:56:55.963157  7562 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 18:56:55.963160  7562 net.cpp:183] Created Layer accuracy/top5 (42)
I0628 18:56:55.963163  7562 net.cpp:560] accuracy/top5 <- fc10_fc10_0_split_2
I0628 18:56:55.963165  7562 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0628 18:56:55.963167  7562 net.cpp:529] accuracy/top5 -> accuracy/top5
I0628 18:56:55.963171  7562 net.cpp:244] Setting up accuracy/top5
I0628 18:56:55.963174  7562 net.cpp:251] TEST Top shape for layer 42 'accuracy/top5' (1)
I0628 18:56:55.963176  7562 net.cpp:324] accuracy/top5 does not need backward computation.
I0628 18:56:55.963178  7562 net.cpp:324] accuracy/top1 does not need backward computation.
I0628 18:56:55.963181  7562 net.cpp:322] loss needs backward computation.
I0628 18:56:55.963182  7562 net.cpp:322] fc10_fc10_0_split needs backward computation.
I0628 18:56:55.963184  7562 net.cpp:322] fc10 needs backward computation.
I0628 18:56:55.963186  7562 net.cpp:322] pool5 needs backward computation.
I0628 18:56:55.963189  7562 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0628 18:56:55.963191  7562 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0628 18:56:55.963192  7562 net.cpp:322] res5a_branch2b needs backward computation.
I0628 18:56:55.963194  7562 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0628 18:56:55.963197  7562 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0628 18:56:55.963199  7562 net.cpp:322] res5a_branch2a needs backward computation.
I0628 18:56:55.963201  7562 net.cpp:322] pool4 needs backward computation.
I0628 18:56:55.963203  7562 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0628 18:56:55.963205  7562 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0628 18:56:55.963207  7562 net.cpp:322] res4a_branch2b needs backward computation.
I0628 18:56:55.963209  7562 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0628 18:56:55.963212  7562 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0628 18:56:55.963213  7562 net.cpp:322] res4a_branch2a needs backward computation.
I0628 18:56:55.963217  7562 net.cpp:322] pool3 needs backward computation.
I0628 18:56:55.963218  7562 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0628 18:56:55.963220  7562 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0628 18:56:55.963222  7562 net.cpp:322] res3a_branch2b needs backward computation.
I0628 18:56:55.963224  7562 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0628 18:56:55.963227  7562 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0628 18:56:55.963229  7562 net.cpp:322] res3a_branch2a needs backward computation.
I0628 18:56:55.963232  7562 net.cpp:322] pool2 needs backward computation.
I0628 18:56:55.963233  7562 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0628 18:56:55.963237  7562 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0628 18:56:55.963238  7562 net.cpp:322] res2a_branch2b needs backward computation.
I0628 18:56:55.963240  7562 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0628 18:56:55.963243  7562 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0628 18:56:55.963248  7562 net.cpp:322] res2a_branch2a needs backward computation.
I0628 18:56:55.963250  7562 net.cpp:322] pool1 needs backward computation.
I0628 18:56:55.963253  7562 net.cpp:322] conv1b/relu needs backward computation.
I0628 18:56:55.963255  7562 net.cpp:322] conv1b/bn needs backward computation.
I0628 18:56:55.963258  7562 net.cpp:322] conv1b needs backward computation.
I0628 18:56:55.963260  7562 net.cpp:322] conv1a/relu needs backward computation.
I0628 18:56:55.963263  7562 net.cpp:322] conv1a/bn needs backward computation.
I0628 18:56:55.963265  7562 net.cpp:322] conv1a needs backward computation.
I0628 18:56:55.963268  7562 net.cpp:324] data/bias does not need backward computation.
I0628 18:56:55.963270  7562 net.cpp:324] label_data_1_split does not need backward computation.
I0628 18:56:55.963275  7562 net.cpp:324] data does not need backward computation.
I0628 18:56:55.963277  7562 net.cpp:366] This network produces output accuracy/top1
I0628 18:56:55.963279  7562 net.cpp:366] This network produces output accuracy/top5
I0628 18:56:55.963282  7562 net.cpp:366] This network produces output loss
I0628 18:56:55.963309  7562 net.cpp:388] Top memory (TEST) required for data: 137625600 diff: 91750408
I0628 18:56:55.963311  7562 net.cpp:391] Bottom memory (TEST) required for data: 137625600 diff: 137625600
I0628 18:56:55.963313  7562 net.cpp:394] Shared (in-place) memory (TEST) by data: 91750400 diff: 91750400
I0628 18:56:55.963316  7562 net.cpp:397] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0628 18:56:55.963318  7562 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0628 18:56:55.963320  7562 net.cpp:406] Network initialization done.
I0628 18:56:55.963368  7562 solver.cpp:56] Solver scaffolding done.
I0628 18:56:55.966418  7562 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0628 18:56:55.966425  7562 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0628 18:56:55.966428  7562 parallel.cpp:59] Starting Optimization
I0628 18:56:55.966430  7562 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 18:56:55.966450  7562 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 18:56:55.967025  7628 device_alternate.hpp:116] NVML initialized on thread 140325007435520
I0628 18:56:55.983276  7628 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0628 18:56:55.983342  7629 device_alternate.hpp:116] NVML initialized on thread 140324999042816
I0628 18:56:55.984500  7629 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0628 18:56:55.989423  7629 solver.cpp:42] Solver data type: FLOAT
I0628 18:56:55.989903  7629 net.cpp:108] Using FLOAT as default forward math type
I0628 18:56:55.989910  7629 net.cpp:114] Using FLOAT as default backward math type
I0628 18:56:55.989933  7629 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 32
I0628 18:56:55.989941  7629 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 18:56:55.990581  7630 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0628 18:56:55.991341  7629 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 18:56:55.991407  7629 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 18:56:55.991412  7629 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 18:56:56.275199  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0628 18:56:56.282495  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0628 18:56:56.294962  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.2G, req 0G)
I0628 18:56:56.301175  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0628 18:56:56.315212  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.17G, req 0.02G)
I0628 18:56:56.321156  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.02G)
I0628 18:56:56.345001  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.14G, req 0.02G)
I0628 18:56:56.355793  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.12G, req 0.02G)
I0628 18:56:56.402783  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0.02G)
I0628 18:56:56.423377  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 8.08G, req 0.02G)
I0628 18:56:56.424752  7629 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/test.prototxt
I0628 18:56:56.424896  7629 net.cpp:108] Using FLOAT as default forward math type
I0628 18:56:56.424901  7629 net.cpp:114] Using FLOAT as default backward math type
I0628 18:56:56.424923  7629 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 25
I0628 18:56:56.424932  7629 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 18:56:56.425591  7642 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0628 18:56:56.425664  7629 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 18:56:56.425725  7629 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 18:56:56.425731  7629 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 18:56:56.426736  7643 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 18:56:56.426745  7643 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 18:56:56.427556  7643 data_layer.cpp:110] [1] Parser threads: 1
I0628 18:56:56.427563  7643 data_layer.cpp:112] [1] Transformer threads: 1
I0628 18:56:56.526877  7629 solver.cpp:56] Solver scaffolding done.
I0628 18:56:56.543489  7628 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0628 18:56:56.543489  7629 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0628 18:56:56.644317  7628 solver.cpp:474] Solving jacintonet11v2_train
I0628 18:56:56.644317  7629 solver.cpp:474] Solving jacintonet11v2_train
I0628 18:56:56.644340  7628 solver.cpp:475] Learning Rate Policy: poly
I0628 18:56:56.644340  7629 solver.cpp:475] Learning Rate Policy: poly
I0628 18:56:56.647692  7628 solver.cpp:268] Starting Optimization on GPU 0
I0628 18:56:56.647692  7629 solver.cpp:268] Starting Optimization on GPU 1
I0628 18:56:56.647727  7628 solver.cpp:545] Iteration 0, Testing net (#0)
I0628 18:56:56.647778  7661 device_alternate.hpp:116] NVML initialized on thread 140324615075584
I0628 18:56:56.647797  7661 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0628 18:56:56.648401  7662 device_alternate.hpp:116] NVML initialized on thread 140324606682880
I0628 18:56:56.648411  7662 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0628 18:56:56.697265  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.16
I0628 18:56:56.697281  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.36
I0628 18:56:56.697286  7628 solver.cpp:630]     Test net output #2: loss = 73.3627 (* 1 = 73.3627 loss)
I0628 18:56:56.697291  7628 solver.cpp:295] [MultiGPU] Initial Test completed
I0628 18:56:56.697301  7628 blocking_queue.cpp:40] Data layer prefetch queue empty
I0628 18:56:56.703909  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.78G, req 0.02G)
I0628 18:56:56.704222  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.69G, req 0.01G)
I0628 18:56:56.710994  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.68G, req 0.01G)
I0628 18:56:56.711364  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.77G, req 0.02G)
I0628 18:56:56.731009  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 7.76G, req 0.02G)
I0628 18:56:56.732504  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 7.67G, req 0.01G)
I0628 18:56:56.736830  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0.02G)
I0628 18:56:56.738157  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.65G, req 0.01G)
I0628 18:56:56.746064  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.72G, req 0.02G)
I0628 18:56:56.748677  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.64G, req 0.02G)
I0628 18:56:56.751243  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.02G)
I0628 18:56:56.753705  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.63G, req 0.02G)
I0628 18:56:56.767408  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.7G, req 0.02G)
I0628 18:56:56.770647  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.61G, req 0.02G)
I0628 18:56:56.773895  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.68G, req 0.02G)
I0628 18:56:56.777537  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.6G, req 0.02G)
I0628 18:56:56.793442  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.66G, req 0.02G)
I0628 18:56:56.799634  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.57G, req 0.02G)
I0628 18:56:56.799988  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.66G, req 0.02G)
I0628 18:56:56.807359  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.56G, req 0.02G)
I0628 18:56:56.822409  7631 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 18:56:56.822422  7631 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 18:56:56.822933  7631 data_layer.cpp:110] [1] Parser threads: 1
I0628 18:56:56.822943  7631 data_layer.cpp:112] [1] Transformer threads: 1
I0628 18:56:56.828666  7614 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 18:56:56.828681  7614 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 18:56:56.828797  7614 data_layer.cpp:110] [0] Parser threads: 1
I0628 18:56:56.828804  7614 data_layer.cpp:112] [0] Transformer threads: 1
I0628 18:56:56.829016  7628 solver.cpp:354] Iteration 0 (0.131709 s), loss = 2.41522
I0628 18:56:56.829031  7628 solver.cpp:371]     Train net output #0: loss = 2.41522 (* 1 = 2.41522 loss)
I0628 18:56:56.829035  7628 sgd_solver.cpp:137] Iteration 0, lr = 0.1, m = 0.9
I0628 18:56:56.837764  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.57G, req 0.02G)
I0628 18:56:56.838003  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.48G, req 0.02G)
I0628 18:56:56.842315  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.57G, req 0.02G)
I0628 18:56:56.842878  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.48G, req 0.02G)
I0628 18:56:56.849628  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.57G, req 0.02G)
I0628 18:56:56.850136  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0.02G)
I0628 18:56:56.853710  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.57G, req 0.02G)
I0628 18:56:56.854269  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.48G, req 0.02G)
I0628 18:56:56.860527  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.57G, req 0.02G)
I0628 18:56:56.861380  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.48G, req 0.02G)
I0628 18:56:56.864367  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.57G, req 0.02G)
I0628 18:56:56.865006  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.48G, req 0.02G)
I0628 18:56:56.877744  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.57G, req 0.02G)
I0628 18:56:56.879294  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0.02G)
I0628 18:56:56.883482  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.57G, req 0.02G)
I0628 18:56:56.884305  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.48G, req 0.02G)
I0628 18:56:56.904563  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.57G, req 0.02G)
I0628 18:56:56.905021  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.48G, req 0.02G)
I0628 18:56:56.910465  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.57G, req 0.02G)
I0628 18:56:56.910751  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.48G, req 0.02G)
I0628 18:56:56.923387  7628 solver.cpp:354] Iteration 1 (0.0943329 s), loss = 2.20263
I0628 18:56:56.923408  7628 solver.cpp:371]     Train net output #0: loss = 2.20263 (* 1 = 2.20263 loss)
I0628 18:56:56.932822  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.68G/1 1 0 3  (limit 6.91G, req 0.02G)
I0628 18:56:56.932979  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.68G/1 1 0 0  (limit 6.82G, req 0.02G)
I0628 18:56:56.939556  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.35G/2 1 1 3  (limit 6.23G, req 0.02G)
I0628 18:56:56.941026  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.35G/2 1 1 3  (limit 6.14G, req 0.02G)
I0628 18:56:56.955101  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.35G/1 6 4 3  (limit 6.23G, req 0.02G)
I0628 18:56:56.957144  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.35G/1 6 4 1  (limit 6.14G, req 0.02G)
I0628 18:56:56.963120  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.35G/2 6 4 3  (limit 6.23G, req 0.02G)
I0628 18:56:56.965086  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.35G/2 6 4 3  (limit 6.14G, req 0.02G)
I0628 18:56:56.976718  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.35G/1 6 4 5  (limit 6.23G, req 0.02G)
I0628 18:56:56.976999  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.35G/1 6 4 5  (limit 6.14G, req 0.02G)
I0628 18:56:56.981122  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.35G/2 6 4 0  (limit 6.23G, req 0.02G)
I0628 18:56:56.982252  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.35G/2 6 4 0  (limit 6.14G, req 0.02G)
I0628 18:56:57.011062  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.35G/1 6 4 5  (limit 6.14G, req 0.03G)
I0628 18:56:57.013208  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.35G/1 6 4 5  (limit 6.23G, req 0.03G)
I0628 18:56:57.019074  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.35G/2 6 4 3  (limit 6.14G, req 0.03G)
I0628 18:56:57.020934  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.35G/2 6 4 3  (limit 6.23G, req 0.03G)
I0628 18:56:57.061944  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.35G/1 7 5 5  (limit 6.14G, req 0.03G)
I0628 18:56:57.064550  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.35G/1 7 5 5  (limit 6.23G, req 0.03G)
I0628 18:56:57.075949  7629 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.35G/2 7 5 5  (limit 6.23G, req 0.03G)
I0628 18:56:57.079802  7628 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.35G/2 7 4 5  (limit 6.14G, req 0.03G)
I0628 18:56:57.092095  7628 solver.cpp:349] Iteration 2 (5.92891 iter/s, 0.168665s/100 iter), loss = 2.47595
I0628 18:56:57.092124  7628 solver.cpp:371]     Train net output #0: loss = 2.47595 (* 1 = 2.47595 loss)
I0628 18:56:57.126940  7628 cudnn_conv_layer.cpp:283] [0] Layer 'conv1a' reallocating workspace: 1.35G -> 0.07G
I0628 18:56:57.126940  7629 cudnn_conv_layer.cpp:283] [1] Layer 'conv1a' reallocating workspace: 1.35G -> 0.07G
I0628 18:56:58.368986  7614 blocking_queue.cpp:40] Waiting for datum
I0628 18:56:58.760920  7628 solver.cpp:349] Iteration 100 (58.7274 iter/s, 1.66873s/100 iter), loss = 1.65823
I0628 18:56:58.760939  7628 solver.cpp:371]     Train net output #0: loss = 1.65823 (* 1 = 1.65823 loss)
I0628 18:56:58.760944  7628 sgd_solver.cpp:137] Iteration 100, lr = 0.0998438, m = 0.9
I0628 18:57:00.776150  7628 solver.cpp:349] Iteration 200 (49.6245 iter/s, 2.01513s/100 iter), loss = 1.63203
I0628 18:57:00.776175  7628 solver.cpp:371]     Train net output #0: loss = 1.63203 (* 1 = 1.63203 loss)
I0628 18:57:00.776183  7628 sgd_solver.cpp:137] Iteration 200, lr = 0.0996875, m = 0.9
I0628 18:57:02.557868  7628 solver.cpp:349] Iteration 300 (56.1287 iter/s, 1.78162s/100 iter), loss = 1.58065
I0628 18:57:02.557891  7628 solver.cpp:371]     Train net output #0: loss = 1.58065 (* 1 = 1.58065 loss)
I0628 18:57:02.557898  7628 sgd_solver.cpp:137] Iteration 300, lr = 0.0995313, m = 0.9
I0628 18:57:04.259845  7628 solver.cpp:349] Iteration 400 (58.7585 iter/s, 1.70188s/100 iter), loss = 1.43131
I0628 18:57:04.259866  7628 solver.cpp:371]     Train net output #0: loss = 1.43131 (* 1 = 1.43131 loss)
I0628 18:57:04.259871  7628 sgd_solver.cpp:137] Iteration 400, lr = 0.099375, m = 0.9
I0628 18:57:05.963021  7628 solver.cpp:349] Iteration 500 (58.7169 iter/s, 1.70309s/100 iter), loss = 1.1501
I0628 18:57:05.963043  7628 solver.cpp:371]     Train net output #0: loss = 1.1501 (* 1 = 1.1501 loss)
I0628 18:57:05.963047  7628 sgd_solver.cpp:137] Iteration 500, lr = 0.0992187, m = 0.9
I0628 18:57:07.668576  7628 solver.cpp:349] Iteration 600 (58.635 iter/s, 1.70546s/100 iter), loss = 0.984433
I0628 18:57:07.668597  7628 solver.cpp:371]     Train net output #0: loss = 0.984433 (* 1 = 0.984433 loss)
I0628 18:57:07.668602  7628 sgd_solver.cpp:137] Iteration 600, lr = 0.0990625, m = 0.9
I0628 18:57:09.372469  7628 solver.cpp:349] Iteration 700 (58.6922 iter/s, 1.7038s/100 iter), loss = 1.16149
I0628 18:57:09.372491  7628 solver.cpp:371]     Train net output #0: loss = 1.16149 (* 1 = 1.16149 loss)
I0628 18:57:09.372496  7628 sgd_solver.cpp:137] Iteration 700, lr = 0.0989062, m = 0.9
I0628 18:57:10.710011  7613 data_reader.cpp:262] Starting prefetch of epoch 1
I0628 18:57:11.084947  7628 solver.cpp:349] Iteration 800 (58.3981 iter/s, 1.71239s/100 iter), loss = 0.884332
I0628 18:57:11.084972  7628 solver.cpp:371]     Train net output #0: loss = 0.884332 (* 1 = 0.884332 loss)
I0628 18:57:11.084976  7628 sgd_solver.cpp:137] Iteration 800, lr = 0.09875, m = 0.9
I0628 18:57:12.787426  7628 solver.cpp:349] Iteration 900 (58.7411 iter/s, 1.70239s/100 iter), loss = 0.932051
I0628 18:57:12.787466  7628 solver.cpp:371]     Train net output #0: loss = 0.932051 (* 1 = 0.932051 loss)
I0628 18:57:12.787470  7628 sgd_solver.cpp:137] Iteration 900, lr = 0.0985937, m = 0.9
I0628 18:57:14.476846  7628 solver.cpp:545] Iteration 1000, Testing net (#0)
I0628 18:57:15.483477  7626 data_reader.cpp:262] Starting prefetch of epoch 1
I0628 18:57:15.503803  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.534
I0628 18:57:15.503815  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.966201
I0628 18:57:15.503819  7628 solver.cpp:630]     Test net output #2: loss = 1.30125 (* 1 = 1.30125 loss)
I0628 18:57:15.503834  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02696s
I0628 18:57:15.521096  7628 solver.cpp:349] Iteration 1000 (36.5827 iter/s, 2.73353s/100 iter), loss = 0.97534
I0628 18:57:15.521118  7628 solver.cpp:371]     Train net output #0: loss = 0.97534 (* 1 = 0.97534 loss)
I0628 18:57:15.521124  7628 sgd_solver.cpp:137] Iteration 1000, lr = 0.0984375, m = 0.9
I0628 18:57:17.228981  7628 solver.cpp:349] Iteration 1100 (58.5552 iter/s, 1.70779s/100 iter), loss = 1.43777
I0628 18:57:17.229002  7628 solver.cpp:371]     Train net output #0: loss = 1.43777 (* 1 = 1.43777 loss)
I0628 18:57:17.229007  7628 sgd_solver.cpp:137] Iteration 1100, lr = 0.0982813, m = 0.9
I0628 18:57:18.932545  7628 solver.cpp:349] Iteration 1200 (58.7036 iter/s, 1.70347s/100 iter), loss = 1.07396
I0628 18:57:18.932565  7628 solver.cpp:371]     Train net output #0: loss = 1.07396 (* 1 = 1.07396 loss)
I0628 18:57:18.932569  7628 sgd_solver.cpp:137] Iteration 1200, lr = 0.098125, m = 0.9
I0628 18:57:20.636276  7628 solver.cpp:349] Iteration 1300 (58.6978 iter/s, 1.70364s/100 iter), loss = 0.599833
I0628 18:57:20.636301  7628 solver.cpp:371]     Train net output #0: loss = 0.599833 (* 1 = 0.599833 loss)
I0628 18:57:20.636306  7628 sgd_solver.cpp:137] Iteration 1300, lr = 0.0979687, m = 0.9
I0628 18:57:22.339951  7628 solver.cpp:349] Iteration 1400 (58.6999 iter/s, 1.70358s/100 iter), loss = 0.676795
I0628 18:57:22.339975  7628 solver.cpp:371]     Train net output #0: loss = 0.676795 (* 1 = 0.676795 loss)
I0628 18:57:22.339982  7628 sgd_solver.cpp:137] Iteration 1400, lr = 0.0978125, m = 0.9
I0628 18:57:24.038277  7628 solver.cpp:349] Iteration 1500 (58.8848 iter/s, 1.69823s/100 iter), loss = 0.575772
I0628 18:57:24.038300  7628 solver.cpp:371]     Train net output #0: loss = 0.575772 (* 1 = 0.575772 loss)
I0628 18:57:24.038305  7628 sgd_solver.cpp:137] Iteration 1500, lr = 0.0976562, m = 0.9
I0628 18:57:25.043442  7613 data_reader.cpp:262] Starting prefetch of epoch 2
I0628 18:57:25.740754  7628 solver.cpp:349] Iteration 1600 (58.7412 iter/s, 1.70238s/100 iter), loss = 0.791276
I0628 18:57:25.740778  7628 solver.cpp:371]     Train net output #0: loss = 0.791276 (* 1 = 0.791276 loss)
I0628 18:57:25.740783  7628 sgd_solver.cpp:137] Iteration 1600, lr = 0.0975, m = 0.9
I0628 18:57:27.445808  7628 solver.cpp:349] Iteration 1700 (58.6524 iter/s, 1.70496s/100 iter), loss = 0.509466
I0628 18:57:27.445832  7628 solver.cpp:371]     Train net output #0: loss = 0.509466 (* 1 = 0.509466 loss)
I0628 18:57:27.445837  7628 sgd_solver.cpp:137] Iteration 1700, lr = 0.0973438, m = 0.9
I0628 18:57:29.151619  7628 solver.cpp:349] Iteration 1800 (58.6263 iter/s, 1.70572s/100 iter), loss = 0.849034
I0628 18:57:29.151640  7628 solver.cpp:371]     Train net output #0: loss = 0.849034 (* 1 = 0.849034 loss)
I0628 18:57:29.151644  7628 sgd_solver.cpp:137] Iteration 1800, lr = 0.0971875, m = 0.9
I0628 18:57:30.855134  7628 solver.cpp:349] Iteration 1900 (58.7053 iter/s, 1.70342s/100 iter), loss = 0.438497
I0628 18:57:30.855156  7628 solver.cpp:371]     Train net output #0: loss = 0.438497 (* 1 = 0.438497 loss)
I0628 18:57:30.855161  7628 sgd_solver.cpp:137] Iteration 1900, lr = 0.0970313, m = 0.9
I0628 18:57:32.544883  7628 solver.cpp:545] Iteration 2000, Testing net (#0)
I0628 18:57:33.548089  7626 data_reader.cpp:262] Starting prefetch of epoch 2
I0628 18:57:33.568892  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.6804
I0628 18:57:33.568907  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.974801
I0628 18:57:33.568912  7628 solver.cpp:630]     Test net output #2: loss = 0.918189 (* 1 = 0.918189 loss)
I0628 18:57:33.568925  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02401s
I0628 18:57:33.586102  7628 solver.cpp:349] Iteration 2000 (36.6187 iter/s, 2.73085s/100 iter), loss = 0.637231
I0628 18:57:33.586125  7628 solver.cpp:371]     Train net output #0: loss = 0.637231 (* 1 = 0.637231 loss)
I0628 18:57:33.586129  7628 sgd_solver.cpp:137] Iteration 2000, lr = 0.096875, m = 0.9
I0628 18:57:35.293756  7628 solver.cpp:349] Iteration 2100 (58.5631 iter/s, 1.70756s/100 iter), loss = 0.558987
I0628 18:57:35.293777  7628 solver.cpp:371]     Train net output #0: loss = 0.558987 (* 1 = 0.558987 loss)
I0628 18:57:35.293781  7628 sgd_solver.cpp:137] Iteration 2100, lr = 0.0967188, m = 0.9
I0628 18:57:36.998931  7628 solver.cpp:349] Iteration 2200 (58.6481 iter/s, 1.70508s/100 iter), loss = 0.855461
I0628 18:57:36.998952  7628 solver.cpp:371]     Train net output #0: loss = 0.855461 (* 1 = 0.855461 loss)
I0628 18:57:36.998956  7628 sgd_solver.cpp:137] Iteration 2200, lr = 0.0965625, m = 0.9
I0628 18:57:38.704803  7628 solver.cpp:349] Iteration 2300 (58.6242 iter/s, 1.70578s/100 iter), loss = 0.448389
I0628 18:57:38.704825  7628 solver.cpp:371]     Train net output #0: loss = 0.448389 (* 1 = 0.448389 loss)
I0628 18:57:38.704829  7628 sgd_solver.cpp:137] Iteration 2300, lr = 0.0964063, m = 0.9
I0628 18:57:39.391474  7613 data_reader.cpp:262] Starting prefetch of epoch 3
I0628 18:57:40.414566  7628 solver.cpp:349] Iteration 2400 (58.4909 iter/s, 1.70967s/100 iter), loss = 0.533496
I0628 18:57:40.414588  7628 solver.cpp:371]     Train net output #0: loss = 0.533496 (* 1 = 0.533496 loss)
I0628 18:57:40.414592  7628 sgd_solver.cpp:137] Iteration 2400, lr = 0.09625, m = 0.9
I0628 18:57:42.118415  7628 solver.cpp:349] Iteration 2500 (58.6939 iter/s, 1.70375s/100 iter), loss = 0.776698
I0628 18:57:42.118440  7628 solver.cpp:371]     Train net output #0: loss = 0.776698 (* 1 = 0.776698 loss)
I0628 18:57:42.118448  7628 sgd_solver.cpp:137] Iteration 2500, lr = 0.0960938, m = 0.9
I0628 18:57:43.828238  7628 solver.cpp:349] Iteration 2600 (58.4891 iter/s, 1.70972s/100 iter), loss = 0.788647
I0628 18:57:43.828258  7628 solver.cpp:371]     Train net output #0: loss = 0.788647 (* 1 = 0.788647 loss)
I0628 18:57:43.828263  7628 sgd_solver.cpp:137] Iteration 2600, lr = 0.0959375, m = 0.9
I0628 18:57:45.536448  7628 solver.cpp:349] Iteration 2700 (58.5439 iter/s, 1.70812s/100 iter), loss = 0.466793
I0628 18:57:45.536469  7628 solver.cpp:371]     Train net output #0: loss = 0.466793 (* 1 = 0.466793 loss)
I0628 18:57:45.536483  7628 sgd_solver.cpp:137] Iteration 2700, lr = 0.0957813, m = 0.9
I0628 18:57:47.241859  7628 solver.cpp:349] Iteration 2800 (58.6404 iter/s, 1.70531s/100 iter), loss = 0.518772
I0628 18:57:47.241879  7628 solver.cpp:371]     Train net output #0: loss = 0.518772 (* 1 = 0.518772 loss)
I0628 18:57:47.241883  7628 sgd_solver.cpp:137] Iteration 2800, lr = 0.095625, m = 0.9
I0628 18:57:48.951158  7628 solver.cpp:349] Iteration 2900 (58.5067 iter/s, 1.70921s/100 iter), loss = 0.435994
I0628 18:57:48.951179  7628 solver.cpp:371]     Train net output #0: loss = 0.435994 (* 1 = 0.435994 loss)
I0628 18:57:48.951184  7628 sgd_solver.cpp:137] Iteration 2900, lr = 0.0954688, m = 0.9
I0628 18:57:50.643957  7628 solver.cpp:545] Iteration 3000, Testing net (#0)
I0628 18:57:51.646183  7626 data_reader.cpp:262] Starting prefetch of epoch 3
I0628 18:57:51.667026  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7472
I0628 18:57:51.667040  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.978
I0628 18:57:51.667045  7628 solver.cpp:630]     Test net output #2: loss = 0.757618 (* 1 = 0.757618 loss)
I0628 18:57:51.667059  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02307s
I0628 18:57:51.684321  7628 solver.cpp:349] Iteration 3000 (36.5893 iter/s, 2.73304s/100 iter), loss = 0.468012
I0628 18:57:51.684343  7628 solver.cpp:371]     Train net output #0: loss = 0.468012 (* 1 = 0.468012 loss)
I0628 18:57:51.684347  7628 sgd_solver.cpp:137] Iteration 3000, lr = 0.0953125, m = 0.9
I0628 18:57:53.394284  7628 solver.cpp:349] Iteration 3100 (58.4839 iter/s, 1.70987s/100 iter), loss = 0.716954
I0628 18:57:53.394305  7628 solver.cpp:371]     Train net output #0: loss = 0.716954 (* 1 = 0.716954 loss)
I0628 18:57:53.394310  7628 sgd_solver.cpp:137] Iteration 3100, lr = 0.0951563, m = 0.9
I0628 18:57:53.752128  7613 data_reader.cpp:262] Starting prefetch of epoch 4
I0628 18:57:55.100917  7628 solver.cpp:349] Iteration 3200 (58.598 iter/s, 1.70654s/100 iter), loss = 0.527757
I0628 18:57:55.100996  7628 solver.cpp:371]     Train net output #0: loss = 0.527757 (* 1 = 0.527757 loss)
I0628 18:57:55.101004  7628 sgd_solver.cpp:137] Iteration 3200, lr = 0.095, m = 0.9
I0628 18:57:56.813741  7628 solver.cpp:349] Iteration 3300 (58.3885 iter/s, 1.71267s/100 iter), loss = 0.512567
I0628 18:57:56.813765  7628 solver.cpp:371]     Train net output #0: loss = 0.512567 (* 1 = 0.512567 loss)
I0628 18:57:56.813769  7628 sgd_solver.cpp:137] Iteration 3300, lr = 0.0948438, m = 0.9
I0628 18:57:58.523288  7628 solver.cpp:349] Iteration 3400 (58.4982 iter/s, 1.70945s/100 iter), loss = 0.442209
I0628 18:57:58.523311  7628 solver.cpp:371]     Train net output #0: loss = 0.442209 (* 1 = 0.442209 loss)
I0628 18:57:58.523315  7628 sgd_solver.cpp:137] Iteration 3400, lr = 0.0946875, m = 0.9
I0628 18:58:00.232589  7628 solver.cpp:349] Iteration 3500 (58.5067 iter/s, 1.70921s/100 iter), loss = 0.280772
I0628 18:58:00.232610  7628 solver.cpp:371]     Train net output #0: loss = 0.280772 (* 1 = 0.280772 loss)
I0628 18:58:00.232614  7628 sgd_solver.cpp:137] Iteration 3500, lr = 0.0945313, m = 0.9
I0628 18:58:01.941632  7628 solver.cpp:349] Iteration 3600 (58.5154 iter/s, 1.70895s/100 iter), loss = 0.540488
I0628 18:58:01.941653  7628 solver.cpp:371]     Train net output #0: loss = 0.540488 (* 1 = 0.540488 loss)
I0628 18:58:01.941658  7628 sgd_solver.cpp:137] Iteration 3600, lr = 0.094375, m = 0.9
I0628 18:58:03.650167  7628 solver.cpp:349] Iteration 3700 (58.5328 iter/s, 1.70844s/100 iter), loss = 0.637194
I0628 18:58:03.650188  7628 solver.cpp:371]     Train net output #0: loss = 0.637194 (* 1 = 0.637194 loss)
I0628 18:58:03.650192  7628 sgd_solver.cpp:137] Iteration 3700, lr = 0.0942188, m = 0.9
I0628 18:58:05.360621  7628 solver.cpp:349] Iteration 3800 (58.4671 iter/s, 1.71036s/100 iter), loss = 0.445415
I0628 18:58:05.360644  7628 solver.cpp:371]     Train net output #0: loss = 0.445415 (* 1 = 0.445415 loss)
I0628 18:58:05.360648  7628 sgd_solver.cpp:137] Iteration 3800, lr = 0.0940625, m = 0.9
I0628 18:58:07.068754  7628 solver.cpp:349] Iteration 3900 (58.5467 iter/s, 1.70804s/100 iter), loss = 0.294246
I0628 18:58:07.068775  7628 solver.cpp:371]     Train net output #0: loss = 0.294246 (* 1 = 0.294246 loss)
I0628 18:58:07.068779  7628 sgd_solver.cpp:137] Iteration 3900, lr = 0.0939062, m = 0.9
I0628 18:58:07.120252  7613 data_reader.cpp:262] Starting prefetch of epoch 5
I0628 18:58:08.760432  7628 solver.cpp:545] Iteration 4000, Testing net (#0)
I0628 18:58:09.767961  7626 data_reader.cpp:262] Starting prefetch of epoch 4
I0628 18:58:09.788322  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7006
I0628 18:58:09.788336  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.979401
I0628 18:58:09.788343  7628 solver.cpp:630]     Test net output #2: loss = 0.879205 (* 1 = 0.879205 loss)
I0628 18:58:09.788360  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.0279s
I0628 18:58:09.805625  7628 solver.cpp:349] Iteration 4000 (36.5397 iter/s, 2.73675s/100 iter), loss = 0.60244
I0628 18:58:09.805650  7628 solver.cpp:371]     Train net output #0: loss = 0.60244 (* 1 = 0.60244 loss)
I0628 18:58:09.805655  7628 sgd_solver.cpp:137] Iteration 4000, lr = 0.09375, m = 0.9
I0628 18:58:11.516031  7628 solver.cpp:349] Iteration 4100 (58.4691 iter/s, 1.71031s/100 iter), loss = 0.421332
I0628 18:58:11.516054  7628 solver.cpp:371]     Train net output #0: loss = 0.421332 (* 1 = 0.421332 loss)
I0628 18:58:11.516060  7628 sgd_solver.cpp:137] Iteration 4100, lr = 0.0935938, m = 0.9
I0628 18:58:13.229832  7628 solver.cpp:349] Iteration 4200 (58.3532 iter/s, 1.7137s/100 iter), loss = 0.131802
I0628 18:58:13.229856  7628 solver.cpp:371]     Train net output #0: loss = 0.131802 (* 1 = 0.131802 loss)
I0628 18:58:13.229862  7628 sgd_solver.cpp:137] Iteration 4200, lr = 0.0934375, m = 0.9
I0628 18:58:14.938930  7628 solver.cpp:349] Iteration 4300 (58.5137 iter/s, 1.709s/100 iter), loss = 0.831636
I0628 18:58:14.938954  7628 solver.cpp:371]     Train net output #0: loss = 0.831636 (* 1 = 0.831636 loss)
I0628 18:58:14.938959  7628 sgd_solver.cpp:137] Iteration 4300, lr = 0.0932813, m = 0.9
I0628 18:58:16.651981  7628 solver.cpp:349] Iteration 4400 (58.3792 iter/s, 1.71294s/100 iter), loss = 0.251769
I0628 18:58:16.652005  7628 solver.cpp:371]     Train net output #0: loss = 0.251768 (* 1 = 0.251768 loss)
I0628 18:58:16.652009  7628 sgd_solver.cpp:137] Iteration 4400, lr = 0.093125, m = 0.9
I0628 18:58:18.363818  7628 solver.cpp:349] Iteration 4500 (58.42 iter/s, 1.71174s/100 iter), loss = 0.311408
I0628 18:58:18.363840  7628 solver.cpp:371]     Train net output #0: loss = 0.311407 (* 1 = 0.311407 loss)
I0628 18:58:18.363845  7628 sgd_solver.cpp:137] Iteration 4500, lr = 0.0929688, m = 0.9
I0628 18:58:20.075278  7628 solver.cpp:349] Iteration 4600 (58.4328 iter/s, 1.71137s/100 iter), loss = 0.313337
I0628 18:58:20.075299  7628 solver.cpp:371]     Train net output #0: loss = 0.313337 (* 1 = 0.313337 loss)
I0628 18:58:20.075304  7628 sgd_solver.cpp:137] Iteration 4600, lr = 0.0928125, m = 0.9
I0628 18:58:21.513731  7613 data_reader.cpp:262] Starting prefetch of epoch 6
I0628 18:58:21.788449  7628 solver.cpp:349] Iteration 4700 (58.3745 iter/s, 1.71308s/100 iter), loss = 0.450369
I0628 18:58:21.788471  7628 solver.cpp:371]     Train net output #0: loss = 0.450369 (* 1 = 0.450369 loss)
I0628 18:58:21.788475  7628 sgd_solver.cpp:137] Iteration 4700, lr = 0.0926562, m = 0.9
I0628 18:58:23.496522  7628 solver.cpp:349] Iteration 4800 (58.5486 iter/s, 1.70798s/100 iter), loss = 0.558355
I0628 18:58:23.496543  7628 solver.cpp:371]     Train net output #0: loss = 0.558355 (* 1 = 0.558355 loss)
I0628 18:58:23.496547  7628 sgd_solver.cpp:137] Iteration 4800, lr = 0.0925, m = 0.9
I0628 18:58:25.205067  7628 solver.cpp:349] Iteration 4900 (58.5325 iter/s, 1.70845s/100 iter), loss = 0.239262
I0628 18:58:25.205127  7628 solver.cpp:371]     Train net output #0: loss = 0.239262 (* 1 = 0.239262 loss)
I0628 18:58:25.205133  7628 sgd_solver.cpp:137] Iteration 4900, lr = 0.0923437, m = 0.9
I0628 18:58:26.899996  7628 solver.cpp:545] Iteration 5000, Testing net (#0)
I0628 18:58:27.905269  7626 data_reader.cpp:262] Starting prefetch of epoch 5
I0628 18:58:27.925591  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7434
I0628 18:58:27.925606  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.976201
I0628 18:58:27.925611  7628 solver.cpp:630]     Test net output #2: loss = 0.799174 (* 1 = 0.799174 loss)
I0628 18:58:27.925624  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.0256s
I0628 18:58:27.943014  7628 solver.cpp:349] Iteration 5000 (36.5259 iter/s, 2.73778s/100 iter), loss = 0.350767
I0628 18:58:27.943037  7628 solver.cpp:371]     Train net output #0: loss = 0.350767 (* 1 = 0.350767 loss)
I0628 18:58:27.943040  7628 sgd_solver.cpp:137] Iteration 5000, lr = 0.0921875, m = 0.9
I0628 18:58:29.651821  7628 solver.cpp:349] Iteration 5100 (58.5235 iter/s, 1.70872s/100 iter), loss = 0.251671
I0628 18:58:29.651841  7628 solver.cpp:371]     Train net output #0: loss = 0.251671 (* 1 = 0.251671 loss)
I0628 18:58:29.651846  7628 sgd_solver.cpp:137] Iteration 5100, lr = 0.0920313, m = 0.9
I0628 18:58:31.367152  7628 solver.cpp:349] Iteration 5200 (58.3008 iter/s, 1.71524s/100 iter), loss = 0.696482
I0628 18:58:31.367175  7628 solver.cpp:371]     Train net output #0: loss = 0.696482 (* 1 = 0.696482 loss)
I0628 18:58:31.367179  7628 sgd_solver.cpp:137] Iteration 5200, lr = 0.091875, m = 0.9
I0628 18:58:33.077075  7628 solver.cpp:349] Iteration 5300 (58.4853 iter/s, 1.70983s/100 iter), loss = 0.366943
I0628 18:58:33.077096  7628 solver.cpp:371]     Train net output #0: loss = 0.366943 (* 1 = 0.366943 loss)
I0628 18:58:33.077100  7628 sgd_solver.cpp:137] Iteration 5300, lr = 0.0917188, m = 0.9
I0628 18:58:34.786497  7628 solver.cpp:349] Iteration 5400 (58.5024 iter/s, 1.70933s/100 iter), loss = 0.254856
I0628 18:58:34.786520  7628 solver.cpp:371]     Train net output #0: loss = 0.254856 (* 1 = 0.254856 loss)
I0628 18:58:34.786525  7628 sgd_solver.cpp:137] Iteration 5400, lr = 0.0915625, m = 0.9
I0628 18:58:35.897660  7613 data_reader.cpp:262] Starting prefetch of epoch 7
I0628 18:58:36.495265  7628 solver.cpp:349] Iteration 5500 (58.525 iter/s, 1.70867s/100 iter), loss = 0.331896
I0628 18:58:36.495286  7628 solver.cpp:371]     Train net output #0: loss = 0.331896 (* 1 = 0.331896 loss)
I0628 18:58:36.495291  7628 sgd_solver.cpp:137] Iteration 5500, lr = 0.0914062, m = 0.9
I0628 18:58:38.204319  7628 solver.cpp:349] Iteration 5600 (58.5151 iter/s, 1.70896s/100 iter), loss = 0.195107
I0628 18:58:38.204339  7628 solver.cpp:371]     Train net output #0: loss = 0.195107 (* 1 = 0.195107 loss)
I0628 18:58:38.204344  7628 sgd_solver.cpp:137] Iteration 5600, lr = 0.09125, m = 0.9
I0628 18:58:39.915815  7628 solver.cpp:349] Iteration 5700 (58.4315 iter/s, 1.71141s/100 iter), loss = 0.2654
I0628 18:58:39.915837  7628 solver.cpp:371]     Train net output #0: loss = 0.2654 (* 1 = 0.2654 loss)
I0628 18:58:39.915840  7628 sgd_solver.cpp:137] Iteration 5700, lr = 0.0910937, m = 0.9
I0628 18:58:41.629324  7628 solver.cpp:349] Iteration 5800 (58.3629 iter/s, 1.71342s/100 iter), loss = 0.405319
I0628 18:58:41.629348  7628 solver.cpp:371]     Train net output #0: loss = 0.405319 (* 1 = 0.405319 loss)
I0628 18:58:41.629354  7628 sgd_solver.cpp:137] Iteration 5800, lr = 0.0909375, m = 0.9
I0628 18:58:43.337491  7628 solver.cpp:349] Iteration 5900 (58.5456 iter/s, 1.70807s/100 iter), loss = 0.4415
I0628 18:58:43.337512  7628 solver.cpp:371]     Train net output #0: loss = 0.4415 (* 1 = 0.4415 loss)
I0628 18:58:43.337517  7628 sgd_solver.cpp:137] Iteration 5900, lr = 0.0907812, m = 0.9
I0628 18:58:45.026653  7628 solver.cpp:545] Iteration 6000, Testing net (#0)
I0628 18:58:46.033563  7626 data_reader.cpp:262] Starting prefetch of epoch 6
I0628 18:58:46.055513  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.739
I0628 18:58:46.055538  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9826
I0628 18:58:46.055544  7628 solver.cpp:630]     Test net output #2: loss = 0.816913 (* 1 = 0.816913 loss)
I0628 18:58:46.055558  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02887s
I0628 18:58:46.072798  7628 solver.cpp:349] Iteration 6000 (36.5606 iter/s, 2.73518s/100 iter), loss = 0.498414
I0628 18:58:46.072821  7628 solver.cpp:371]     Train net output #0: loss = 0.498414 (* 1 = 0.498414 loss)
I0628 18:58:46.072825  7628 sgd_solver.cpp:137] Iteration 6000, lr = 0.090625, m = 0.9
I0628 18:58:47.784095  7628 solver.cpp:349] Iteration 6100 (58.4384 iter/s, 1.7112s/100 iter), loss = 0.299796
I0628 18:58:47.784116  7628 solver.cpp:371]     Train net output #0: loss = 0.299796 (* 1 = 0.299796 loss)
I0628 18:58:47.784119  7628 sgd_solver.cpp:137] Iteration 6100, lr = 0.0904688, m = 0.9
I0628 18:58:49.495116  7628 solver.cpp:349] Iteration 6200 (58.4478 iter/s, 1.71093s/100 iter), loss = 0.148227
I0628 18:58:49.495139  7628 solver.cpp:371]     Train net output #0: loss = 0.148227 (* 1 = 0.148227 loss)
I0628 18:58:49.495143  7628 sgd_solver.cpp:137] Iteration 6200, lr = 0.0903125, m = 0.9
I0628 18:58:50.280009  7613 data_reader.cpp:262] Starting prefetch of epoch 8
I0628 18:58:51.203538  7628 solver.cpp:349] Iteration 6300 (58.5368 iter/s, 1.70833s/100 iter), loss = 0.507794
I0628 18:58:51.203558  7628 solver.cpp:371]     Train net output #0: loss = 0.507794 (* 1 = 0.507794 loss)
I0628 18:58:51.203562  7628 sgd_solver.cpp:137] Iteration 6300, lr = 0.0901562, m = 0.9
I0628 18:58:52.917884  7628 solver.cpp:349] Iteration 6400 (58.3344 iter/s, 1.71425s/100 iter), loss = 0.208579
I0628 18:58:52.917906  7628 solver.cpp:371]     Train net output #0: loss = 0.208579 (* 1 = 0.208579 loss)
I0628 18:58:52.917910  7628 sgd_solver.cpp:137] Iteration 6400, lr = 0.09, m = 0.9
I0628 18:58:54.627974  7628 solver.cpp:349] Iteration 6500 (58.4796 iter/s, 1.71s/100 iter), loss = 0.650913
I0628 18:58:54.627997  7628 solver.cpp:371]     Train net output #0: loss = 0.650913 (* 1 = 0.650913 loss)
I0628 18:58:54.628002  7628 sgd_solver.cpp:137] Iteration 6500, lr = 0.0898438, m = 0.9
I0628 18:58:56.339025  7628 solver.cpp:349] Iteration 6600 (58.4468 iter/s, 1.71096s/100 iter), loss = 0.325468
I0628 18:58:56.339128  7628 solver.cpp:371]     Train net output #0: loss = 0.325468 (* 1 = 0.325468 loss)
I0628 18:58:56.339134  7628 sgd_solver.cpp:137] Iteration 6600, lr = 0.0896875, m = 0.9
I0628 18:58:58.059311  7628 solver.cpp:349] Iteration 6700 (58.136 iter/s, 1.72011s/100 iter), loss = 0.232637
I0628 18:58:58.059334  7628 solver.cpp:371]     Train net output #0: loss = 0.232637 (* 1 = 0.232637 loss)
I0628 18:58:58.059339  7628 sgd_solver.cpp:137] Iteration 6700, lr = 0.0895313, m = 0.9
I0628 18:58:59.771634  7628 solver.cpp:349] Iteration 6800 (58.4034 iter/s, 1.71223s/100 iter), loss = 0.352915
I0628 18:58:59.771657  7628 solver.cpp:371]     Train net output #0: loss = 0.352915 (* 1 = 0.352915 loss)
I0628 18:58:59.771663  7628 sgd_solver.cpp:137] Iteration 6800, lr = 0.089375, m = 0.9
I0628 18:59:01.486265  7628 solver.cpp:349] Iteration 6900 (58.325 iter/s, 1.71453s/100 iter), loss = 0.293398
I0628 18:59:01.486287  7628 solver.cpp:371]     Train net output #0: loss = 0.293398 (* 1 = 0.293398 loss)
I0628 18:59:01.486292  7628 sgd_solver.cpp:137] Iteration 6900, lr = 0.0892188, m = 0.9
I0628 18:59:03.179867  7628 solver.cpp:545] Iteration 7000, Testing net (#0)
I0628 18:59:04.187443  7626 data_reader.cpp:262] Starting prefetch of epoch 7
I0628 18:59:04.208379  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.6106
I0628 18:59:04.208395  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9572
I0628 18:59:04.208400  7628 solver.cpp:630]     Test net output #2: loss = 1.54411 (* 1 = 1.54411 loss)
I0628 18:59:04.208418  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02852s
I0628 18:59:04.229830  7628 solver.cpp:349] Iteration 7000 (36.4506 iter/s, 2.74344s/100 iter), loss = 0.504363
I0628 18:59:04.229852  7628 solver.cpp:371]     Train net output #0: loss = 0.504363 (* 1 = 0.504363 loss)
I0628 18:59:04.229857  7628 sgd_solver.cpp:137] Iteration 7000, lr = 0.0890625, m = 0.9
I0628 18:59:04.708396  7613 data_reader.cpp:262] Starting prefetch of epoch 9
I0628 18:59:05.942025  7628 solver.cpp:349] Iteration 7100 (58.4078 iter/s, 1.7121s/100 iter), loss = 0.124974
I0628 18:59:05.942046  7628 solver.cpp:371]     Train net output #0: loss = 0.124974 (* 1 = 0.124974 loss)
I0628 18:59:05.942050  7628 sgd_solver.cpp:137] Iteration 7100, lr = 0.0889063, m = 0.9
I0628 18:59:07.654798  7628 solver.cpp:349] Iteration 7200 (58.388 iter/s, 1.71268s/100 iter), loss = 0.255013
I0628 18:59:07.654820  7628 solver.cpp:371]     Train net output #0: loss = 0.255013 (* 1 = 0.255013 loss)
I0628 18:59:07.654826  7628 sgd_solver.cpp:137] Iteration 7200, lr = 0.08875, m = 0.9
I0628 18:59:09.372557  7628 solver.cpp:349] Iteration 7300 (58.2187 iter/s, 1.71766s/100 iter), loss = 0.422509
I0628 18:59:09.372579  7628 solver.cpp:371]     Train net output #0: loss = 0.422509 (* 1 = 0.422509 loss)
I0628 18:59:09.372584  7628 sgd_solver.cpp:137] Iteration 7300, lr = 0.0885938, m = 0.9
I0628 18:59:11.081279  7628 solver.cpp:349] Iteration 7400 (58.5266 iter/s, 1.70863s/100 iter), loss = 0.20628
I0628 18:59:11.081301  7628 solver.cpp:371]     Train net output #0: loss = 0.20628 (* 1 = 0.20628 loss)
I0628 18:59:11.081306  7628 sgd_solver.cpp:137] Iteration 7400, lr = 0.0884375, m = 0.9
I0628 18:59:12.794934  7628 solver.cpp:349] Iteration 7500 (58.3581 iter/s, 1.71356s/100 iter), loss = 0.184212
I0628 18:59:12.794955  7628 solver.cpp:371]     Train net output #0: loss = 0.184212 (* 1 = 0.184212 loss)
I0628 18:59:12.794960  7628 sgd_solver.cpp:137] Iteration 7500, lr = 0.0882813, m = 0.9
I0628 18:59:14.505671  7628 solver.cpp:349] Iteration 7600 (58.4576 iter/s, 1.71064s/100 iter), loss = 0.448168
I0628 18:59:14.505692  7628 solver.cpp:371]     Train net output #0: loss = 0.448168 (* 1 = 0.448168 loss)
I0628 18:59:14.505697  7628 sgd_solver.cpp:137] Iteration 7600, lr = 0.088125, m = 0.9
I0628 18:59:16.216424  7628 solver.cpp:349] Iteration 7700 (58.457 iter/s, 1.71066s/100 iter), loss = 0.345105
I0628 18:59:16.216444  7628 solver.cpp:371]     Train net output #0: loss = 0.345105 (* 1 = 0.345105 loss)
I0628 18:59:16.216449  7628 sgd_solver.cpp:137] Iteration 7700, lr = 0.0879688, m = 0.9
I0628 18:59:17.934273  7628 solver.cpp:349] Iteration 7800 (58.2161 iter/s, 1.71774s/100 iter), loss = 0.294949
I0628 18:59:17.934293  7628 solver.cpp:371]     Train net output #0: loss = 0.294949 (* 1 = 0.294949 loss)
I0628 18:59:17.934298  7628 sgd_solver.cpp:137] Iteration 7800, lr = 0.0878125, m = 0.9
I0628 18:59:18.088512  7613 data_reader.cpp:262] Starting prefetch of epoch 10
I0628 18:59:19.648531  7628 solver.cpp:349] Iteration 7900 (58.3374 iter/s, 1.71416s/100 iter), loss = 0.183327
I0628 18:59:19.648555  7628 solver.cpp:371]     Train net output #0: loss = 0.183327 (* 1 = 0.183327 loss)
I0628 18:59:19.648558  7628 sgd_solver.cpp:137] Iteration 7900, lr = 0.0876563, m = 0.9
I0628 18:59:21.345592  7628 solver.cpp:545] Iteration 8000, Testing net (#0)
I0628 18:59:22.353842  7626 data_reader.cpp:262] Starting prefetch of epoch 8
I0628 18:59:22.374259  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7918
I0628 18:59:22.374274  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9844
I0628 18:59:22.374279  7628 solver.cpp:630]     Test net output #2: loss = 0.668494 (* 1 = 0.668494 loss)
I0628 18:59:22.374292  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02867s
I0628 18:59:22.391546  7628 solver.cpp:349] Iteration 8000 (36.458 iter/s, 2.74289s/100 iter), loss = 0.0587083
I0628 18:59:22.391568  7628 solver.cpp:371]     Train net output #0: loss = 0.0587083 (* 1 = 0.0587083 loss)
I0628 18:59:22.391572  7628 sgd_solver.cpp:137] Iteration 8000, lr = 0.0875, m = 0.9
I0628 18:59:24.111915  7628 solver.cpp:349] Iteration 8100 (58.1302 iter/s, 1.72027s/100 iter), loss = 0.172685
I0628 18:59:24.111937  7628 solver.cpp:371]     Train net output #0: loss = 0.172685 (* 1 = 0.172685 loss)
I0628 18:59:24.111940  7628 sgd_solver.cpp:137] Iteration 8100, lr = 0.0873438, m = 0.9
I0628 18:59:25.824093  7628 solver.cpp:349] Iteration 8200 (58.4083 iter/s, 1.71209s/100 iter), loss = 0.356011
I0628 18:59:25.824116  7628 solver.cpp:371]     Train net output #0: loss = 0.356011 (* 1 = 0.356011 loss)
I0628 18:59:25.824120  7628 sgd_solver.cpp:137] Iteration 8200, lr = 0.0871875, m = 0.9
I0628 18:59:27.535820  7628 solver.cpp:349] Iteration 8300 (58.4238 iter/s, 1.71163s/100 iter), loss = 0.258547
I0628 18:59:27.535892  7628 solver.cpp:371]     Train net output #0: loss = 0.258547 (* 1 = 0.258547 loss)
I0628 18:59:27.535897  7628 sgd_solver.cpp:137] Iteration 8300, lr = 0.0870313, m = 0.9
I0628 18:59:29.250907  7628 solver.cpp:349] Iteration 8400 (58.311 iter/s, 1.71494s/100 iter), loss = 0.353219
I0628 18:59:29.250926  7628 solver.cpp:371]     Train net output #0: loss = 0.353219 (* 1 = 0.353219 loss)
I0628 18:59:29.250931  7628 sgd_solver.cpp:137] Iteration 8400, lr = 0.086875, m = 0.9
I0628 18:59:30.961786  7628 solver.cpp:349] Iteration 8500 (58.4526 iter/s, 1.71079s/100 iter), loss = 0.197068
I0628 18:59:30.961807  7628 solver.cpp:371]     Train net output #0: loss = 0.197069 (* 1 = 0.197069 loss)
I0628 18:59:30.961812  7628 sgd_solver.cpp:137] Iteration 8500, lr = 0.0867188, m = 0.9
I0628 18:59:32.503006  7613 data_reader.cpp:262] Starting prefetch of epoch 11
I0628 18:59:32.673987  7628 solver.cpp:349] Iteration 8600 (58.4075 iter/s, 1.71211s/100 iter), loss = 0.0931155
I0628 18:59:32.674010  7628 solver.cpp:371]     Train net output #0: loss = 0.0931156 (* 1 = 0.0931156 loss)
I0628 18:59:32.674013  7628 sgd_solver.cpp:137] Iteration 8600, lr = 0.0865625, m = 0.9
I0628 18:59:34.387147  7628 solver.cpp:349] Iteration 8700 (58.3748 iter/s, 1.71307s/100 iter), loss = 0.122131
I0628 18:59:34.387169  7628 solver.cpp:371]     Train net output #0: loss = 0.122131 (* 1 = 0.122131 loss)
I0628 18:59:34.387173  7628 sgd_solver.cpp:137] Iteration 8700, lr = 0.0864063, m = 0.9
I0628 18:59:36.094295  7628 solver.cpp:349] Iteration 8800 (58.5805 iter/s, 1.70705s/100 iter), loss = 0.385177
I0628 18:59:36.094314  7628 solver.cpp:371]     Train net output #0: loss = 0.385177 (* 1 = 0.385177 loss)
I0628 18:59:36.094319  7628 sgd_solver.cpp:137] Iteration 8800, lr = 0.08625, m = 0.9
I0628 18:59:37.809098  7628 solver.cpp:349] Iteration 8900 (58.319 iter/s, 1.71471s/100 iter), loss = 0.160747
I0628 18:59:37.809120  7628 solver.cpp:371]     Train net output #0: loss = 0.160747 (* 1 = 0.160747 loss)
I0628 18:59:37.809124  7628 sgd_solver.cpp:137] Iteration 8900, lr = 0.0860937, m = 0.9
I0628 18:59:39.504081  7628 solver.cpp:545] Iteration 9000, Testing net (#0)
I0628 18:59:40.510597  7626 data_reader.cpp:262] Starting prefetch of epoch 9
I0628 18:59:40.531467  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7536
I0628 18:59:40.531481  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.977601
I0628 18:59:40.531488  7628 solver.cpp:630]     Test net output #2: loss = 0.837314 (* 1 = 0.837314 loss)
I0628 18:59:40.531504  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02739s
I0628 18:59:40.548750  7628 solver.cpp:349] Iteration 9000 (36.5027 iter/s, 2.73953s/100 iter), loss = 0.147756
I0628 18:59:40.548774  7628 solver.cpp:371]     Train net output #0: loss = 0.147757 (* 1 = 0.147757 loss)
I0628 18:59:40.548779  7628 sgd_solver.cpp:137] Iteration 9000, lr = 0.0859375, m = 0.9
I0628 18:59:42.258196  7628 solver.cpp:349] Iteration 9100 (58.5018 iter/s, 1.70935s/100 iter), loss = 0.115033
I0628 18:59:42.258218  7628 solver.cpp:371]     Train net output #0: loss = 0.115033 (* 1 = 0.115033 loss)
I0628 18:59:42.258222  7628 sgd_solver.cpp:137] Iteration 9100, lr = 0.0857813, m = 0.9
I0628 18:59:43.968487  7628 solver.cpp:349] Iteration 9200 (58.4728 iter/s, 1.7102s/100 iter), loss = 0.188793
I0628 18:59:43.968509  7628 solver.cpp:371]     Train net output #0: loss = 0.188793 (* 1 = 0.188793 loss)
I0628 18:59:43.968514  7628 sgd_solver.cpp:137] Iteration 9200, lr = 0.085625, m = 0.9
I0628 18:59:45.677125  7628 solver.cpp:349] Iteration 9300 (58.5295 iter/s, 1.70854s/100 iter), loss = 0.232534
I0628 18:59:45.677150  7628 solver.cpp:371]     Train net output #0: loss = 0.232534 (* 1 = 0.232534 loss)
I0628 18:59:45.677155  7628 sgd_solver.cpp:137] Iteration 9300, lr = 0.0854688, m = 0.9
I0628 18:59:46.896631  7613 data_reader.cpp:262] Starting prefetch of epoch 12
I0628 18:59:47.392678  7628 solver.cpp:349] Iteration 9400 (58.2936 iter/s, 1.71545s/100 iter), loss = 0.240122
I0628 18:59:47.392701  7628 solver.cpp:371]     Train net output #0: loss = 0.240122 (* 1 = 0.240122 loss)
I0628 18:59:47.392722  7628 sgd_solver.cpp:137] Iteration 9400, lr = 0.0853125, m = 0.9
I0628 18:59:49.103521  7628 solver.cpp:349] Iteration 9500 (58.4547 iter/s, 1.71073s/100 iter), loss = 0.276256
I0628 18:59:49.103543  7628 solver.cpp:371]     Train net output #0: loss = 0.276256 (* 1 = 0.276256 loss)
I0628 18:59:49.103548  7628 sgd_solver.cpp:137] Iteration 9500, lr = 0.0851563, m = 0.9
I0628 18:59:50.815011  7628 solver.cpp:349] Iteration 9600 (58.4318 iter/s, 1.7114s/100 iter), loss = 0.215347
I0628 18:59:50.815032  7628 solver.cpp:371]     Train net output #0: loss = 0.215348 (* 1 = 0.215348 loss)
I0628 18:59:50.815037  7628 sgd_solver.cpp:137] Iteration 9600, lr = 0.085, m = 0.9
I0628 18:59:52.528447  7628 solver.cpp:349] Iteration 9700 (58.3654 iter/s, 1.71334s/100 iter), loss = 0.113045
I0628 18:59:52.528470  7628 solver.cpp:371]     Train net output #0: loss = 0.113046 (* 1 = 0.113046 loss)
I0628 18:59:52.528473  7628 sgd_solver.cpp:137] Iteration 9700, lr = 0.0848437, m = 0.9
I0628 18:59:54.238328  7628 solver.cpp:349] Iteration 9800 (58.4868 iter/s, 1.70979s/100 iter), loss = 0.215752
I0628 18:59:54.238351  7628 solver.cpp:371]     Train net output #0: loss = 0.215752 (* 1 = 0.215752 loss)
I0628 18:59:54.238355  7628 sgd_solver.cpp:137] Iteration 9800, lr = 0.0846875, m = 0.9
I0628 18:59:55.954592  7628 solver.cpp:349] Iteration 9900 (58.2693 iter/s, 1.71617s/100 iter), loss = 0.237879
I0628 18:59:55.954614  7628 solver.cpp:371]     Train net output #0: loss = 0.23788 (* 1 = 0.23788 loss)
I0628 18:59:55.954618  7628 sgd_solver.cpp:137] Iteration 9900, lr = 0.0845312, m = 0.9
I0628 18:59:57.649991  7628 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_10000.caffemodel
I0628 18:59:57.664676  7628 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_10000.solverstate
I0628 18:59:57.668166  7628 solver.cpp:545] Iteration 10000, Testing net (#0)
I0628 18:59:58.677172  7626 data_reader.cpp:262] Starting prefetch of epoch 10
I0628 18:59:58.697583  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7392
I0628 18:59:58.697597  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9838
I0628 18:59:58.697602  7628 solver.cpp:630]     Test net output #2: loss = 0.99326 (* 1 = 0.99326 loss)
I0628 18:59:58.697614  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02941s
I0628 18:59:58.714846  7628 solver.cpp:349] Iteration 10000 (36.2302 iter/s, 2.76013s/100 iter), loss = 0.114256
I0628 18:59:58.714869  7628 solver.cpp:371]     Train net output #0: loss = 0.114256 (* 1 = 0.114256 loss)
I0628 18:59:58.714874  7628 sgd_solver.cpp:137] Iteration 10000, lr = 0.084375, m = 0.9
I0628 19:00:00.433503  7628 solver.cpp:349] Iteration 10100 (58.1882 iter/s, 1.71856s/100 iter), loss = 0.186613
I0628 19:00:00.433528  7628 solver.cpp:371]     Train net output #0: loss = 0.186614 (* 1 = 0.186614 loss)
I0628 19:00:00.433532  7628 sgd_solver.cpp:137] Iteration 10100, lr = 0.0842188, m = 0.9
I0628 19:00:01.339294  7613 data_reader.cpp:262] Starting prefetch of epoch 13
I0628 19:00:02.142325  7628 solver.cpp:349] Iteration 10200 (58.5232 iter/s, 1.70873s/100 iter), loss = 0.126598
I0628 19:00:02.142348  7628 solver.cpp:371]     Train net output #0: loss = 0.126598 (* 1 = 0.126598 loss)
I0628 19:00:02.142351  7628 sgd_solver.cpp:137] Iteration 10200, lr = 0.0840625, m = 0.9
I0628 19:00:03.856747  7628 solver.cpp:349] Iteration 10300 (58.3319 iter/s, 1.71433s/100 iter), loss = 0.136496
I0628 19:00:03.856768  7628 solver.cpp:371]     Train net output #0: loss = 0.136496 (* 1 = 0.136496 loss)
I0628 19:00:03.856772  7628 sgd_solver.cpp:137] Iteration 10300, lr = 0.0839063, m = 0.9
I0628 19:00:05.575719  7628 solver.cpp:349] Iteration 10400 (58.1775 iter/s, 1.71888s/100 iter), loss = 0.321512
I0628 19:00:05.575742  7628 solver.cpp:371]     Train net output #0: loss = 0.321513 (* 1 = 0.321513 loss)
I0628 19:00:05.575747  7628 sgd_solver.cpp:137] Iteration 10400, lr = 0.08375, m = 0.9
I0628 19:00:07.286770  7628 solver.cpp:349] Iteration 10500 (58.4468 iter/s, 1.71096s/100 iter), loss = 0.249485
I0628 19:00:07.286789  7628 solver.cpp:371]     Train net output #0: loss = 0.249485 (* 1 = 0.249485 loss)
I0628 19:00:07.286793  7628 sgd_solver.cpp:137] Iteration 10500, lr = 0.0835937, m = 0.9
I0628 19:00:08.998852  7628 solver.cpp:349] Iteration 10600 (58.4115 iter/s, 1.71199s/100 iter), loss = 0.270804
I0628 19:00:08.998874  7628 solver.cpp:371]     Train net output #0: loss = 0.270805 (* 1 = 0.270805 loss)
I0628 19:00:08.998878  7628 sgd_solver.cpp:137] Iteration 10600, lr = 0.0834375, m = 0.9
I0628 19:00:10.711431  7628 solver.cpp:349] Iteration 10700 (58.3947 iter/s, 1.71248s/100 iter), loss = 0.0600235
I0628 19:00:10.711452  7628 solver.cpp:371]     Train net output #0: loss = 0.0600239 (* 1 = 0.0600239 loss)
I0628 19:00:10.711457  7628 sgd_solver.cpp:137] Iteration 10700, lr = 0.0832812, m = 0.9
I0628 19:00:12.427345  7628 solver.cpp:349] Iteration 10800 (58.2811 iter/s, 1.71582s/100 iter), loss = 0.0713831
I0628 19:00:12.427367  7628 solver.cpp:371]     Train net output #0: loss = 0.0713836 (* 1 = 0.0713836 loss)
I0628 19:00:12.427372  7628 sgd_solver.cpp:137] Iteration 10800, lr = 0.083125, m = 0.9
I0628 19:00:14.138257  7628 solver.cpp:349] Iteration 10900 (58.4516 iter/s, 1.71082s/100 iter), loss = 0.143183
I0628 19:00:14.138280  7628 solver.cpp:371]     Train net output #0: loss = 0.143184 (* 1 = 0.143184 loss)
I0628 19:00:14.138284  7628 sgd_solver.cpp:137] Iteration 10900, lr = 0.0829687, m = 0.9
I0628 19:00:14.724583  7613 data_reader.cpp:262] Starting prefetch of epoch 14
I0628 19:00:15.836019  7628 solver.cpp:545] Iteration 11000, Testing net (#0)
I0628 19:00:16.846747  7626 data_reader.cpp:262] Starting prefetch of epoch 11
I0628 19:00:16.867131  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.819
I0628 19:00:16.867142  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9898
I0628 19:00:16.867147  7628 solver.cpp:630]     Test net output #2: loss = 0.626114 (* 1 = 0.626114 loss)
I0628 19:00:16.867161  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03111s
I0628 19:00:16.884371  7628 solver.cpp:349] Iteration 11000 (36.4168 iter/s, 2.74599s/100 iter), loss = 0.408234
I0628 19:00:16.884393  7628 solver.cpp:371]     Train net output #0: loss = 0.408234 (* 1 = 0.408234 loss)
I0628 19:00:16.884397  7628 sgd_solver.cpp:137] Iteration 11000, lr = 0.0828125, m = 0.9
I0628 19:00:18.599318  7628 solver.cpp:349] Iteration 11100 (58.3141 iter/s, 1.71485s/100 iter), loss = 0.106473
I0628 19:00:18.599341  7628 solver.cpp:371]     Train net output #0: loss = 0.106473 (* 1 = 0.106473 loss)
I0628 19:00:18.599346  7628 sgd_solver.cpp:137] Iteration 11100, lr = 0.0826563, m = 0.9
I0628 19:00:20.316668  7628 solver.cpp:349] Iteration 11200 (58.2325 iter/s, 1.71725s/100 iter), loss = 0.192165
I0628 19:00:20.316689  7628 solver.cpp:371]     Train net output #0: loss = 0.192165 (* 1 = 0.192165 loss)
I0628 19:00:20.316694  7628 sgd_solver.cpp:137] Iteration 11200, lr = 0.0825, m = 0.9
I0628 19:00:22.034867  7628 solver.cpp:349] Iteration 11300 (58.2037 iter/s, 1.7181s/100 iter), loss = 0.190229
I0628 19:00:22.034886  7628 solver.cpp:371]     Train net output #0: loss = 0.190229 (* 1 = 0.190229 loss)
I0628 19:00:22.034891  7628 sgd_solver.cpp:137] Iteration 11300, lr = 0.0823437, m = 0.9
I0628 19:00:23.747957  7628 solver.cpp:349] Iteration 11400 (58.3772 iter/s, 1.713s/100 iter), loss = 0.207648
I0628 19:00:23.747980  7628 solver.cpp:371]     Train net output #0: loss = 0.207648 (* 1 = 0.207648 loss)
I0628 19:00:23.747984  7628 sgd_solver.cpp:137] Iteration 11400, lr = 0.0821875, m = 0.9
I0628 19:00:25.460595  7628 solver.cpp:349] Iteration 11500 (58.3927 iter/s, 1.71254s/100 iter), loss = 0.258353
I0628 19:00:25.460616  7628 solver.cpp:371]     Train net output #0: loss = 0.258354 (* 1 = 0.258354 loss)
I0628 19:00:25.460621  7628 sgd_solver.cpp:137] Iteration 11500, lr = 0.0820312, m = 0.9
I0628 19:00:27.173782  7628 solver.cpp:349] Iteration 11600 (58.374 iter/s, 1.71309s/100 iter), loss = 0.312742
I0628 19:00:27.173805  7628 solver.cpp:371]     Train net output #0: loss = 0.312743 (* 1 = 0.312743 loss)
I0628 19:00:27.173810  7628 sgd_solver.cpp:137] Iteration 11600, lr = 0.081875, m = 0.9
I0628 19:00:28.887151  7628 solver.cpp:349] Iteration 11700 (58.3678 iter/s, 1.71327s/100 iter), loss = 0.0974607
I0628 19:00:28.887200  7628 solver.cpp:371]     Train net output #0: loss = 0.0974611 (* 1 = 0.0974611 loss)
I0628 19:00:28.887207  7628 sgd_solver.cpp:137] Iteration 11700, lr = 0.0817188, m = 0.9
I0628 19:00:29.144381  7613 data_reader.cpp:262] Starting prefetch of epoch 15
I0628 19:00:30.602329  7628 solver.cpp:349] Iteration 11800 (58.3073 iter/s, 1.71505s/100 iter), loss = 0.236051
I0628 19:00:30.602351  7628 solver.cpp:371]     Train net output #0: loss = 0.236051 (* 1 = 0.236051 loss)
I0628 19:00:30.602357  7628 sgd_solver.cpp:137] Iteration 11800, lr = 0.0815625, m = 0.9
I0628 19:00:32.316040  7628 solver.cpp:349] Iteration 11900 (58.3562 iter/s, 1.71361s/100 iter), loss = 0.0998526
I0628 19:00:32.316059  7628 solver.cpp:371]     Train net output #0: loss = 0.0998531 (* 1 = 0.0998531 loss)
I0628 19:00:32.316064  7628 sgd_solver.cpp:137] Iteration 11900, lr = 0.0814063, m = 0.9
I0628 19:00:34.009630  7628 solver.cpp:545] Iteration 12000, Testing net (#0)
I0628 19:00:35.016335  7626 data_reader.cpp:262] Starting prefetch of epoch 12
I0628 19:00:35.036870  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7586
I0628 19:00:35.036885  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9766
I0628 19:00:35.036890  7628 solver.cpp:630]     Test net output #2: loss = 0.901653 (* 1 = 0.901653 loss)
I0628 19:00:35.036905  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02724s
I0628 19:00:35.054491  7628 solver.cpp:349] Iteration 12000 (36.5187 iter/s, 2.73833s/100 iter), loss = 0.204839
I0628 19:00:35.054514  7628 solver.cpp:371]     Train net output #0: loss = 0.204839 (* 1 = 0.204839 loss)
I0628 19:00:35.054518  7628 sgd_solver.cpp:137] Iteration 12000, lr = 0.08125, m = 0.9
I0628 19:00:36.768368  7628 solver.cpp:349] Iteration 12100 (58.3505 iter/s, 1.71378s/100 iter), loss = 0.225372
I0628 19:00:36.768391  7628 solver.cpp:371]     Train net output #0: loss = 0.225372 (* 1 = 0.225372 loss)
I0628 19:00:36.768395  7628 sgd_solver.cpp:137] Iteration 12100, lr = 0.0810938, m = 0.9
I0628 19:00:38.482367  7628 solver.cpp:349] Iteration 12200 (58.3463 iter/s, 1.7139s/100 iter), loss = 0.185573
I0628 19:00:38.482390  7628 solver.cpp:371]     Train net output #0: loss = 0.185573 (* 1 = 0.185573 loss)
I0628 19:00:38.482394  7628 sgd_solver.cpp:137] Iteration 12200, lr = 0.0809375, m = 0.9
I0628 19:00:40.195760  7628 solver.cpp:349] Iteration 12300 (58.367 iter/s, 1.7133s/100 iter), loss = 0.0656828
I0628 19:00:40.195782  7628 solver.cpp:371]     Train net output #0: loss = 0.0656833 (* 1 = 0.0656833 loss)
I0628 19:00:40.195786  7628 sgd_solver.cpp:137] Iteration 12300, lr = 0.0807813, m = 0.9
I0628 19:00:41.905248  7628 solver.cpp:349] Iteration 12400 (58.5002 iter/s, 1.70939s/100 iter), loss = 0.0676406
I0628 19:00:41.905272  7628 solver.cpp:371]     Train net output #0: loss = 0.0676411 (* 1 = 0.0676411 loss)
I0628 19:00:41.905275  7628 sgd_solver.cpp:137] Iteration 12400, lr = 0.080625, m = 0.9
I0628 19:00:43.549695  7613 data_reader.cpp:262] Starting prefetch of epoch 16
I0628 19:00:43.618424  7628 solver.cpp:349] Iteration 12500 (58.3743 iter/s, 1.71308s/100 iter), loss = 0.0564481
I0628 19:00:43.618448  7628 solver.cpp:371]     Train net output #0: loss = 0.0564486 (* 1 = 0.0564486 loss)
I0628 19:00:43.618451  7628 sgd_solver.cpp:137] Iteration 12500, lr = 0.0804688, m = 0.9
I0628 19:00:45.331581  7628 solver.cpp:349] Iteration 12600 (58.375 iter/s, 1.71306s/100 iter), loss = 0.174829
I0628 19:00:45.331603  7628 solver.cpp:371]     Train net output #0: loss = 0.17483 (* 1 = 0.17483 loss)
I0628 19:00:45.331606  7628 sgd_solver.cpp:137] Iteration 12600, lr = 0.0803125, m = 0.9
I0628 19:00:47.042168  7628 solver.cpp:349] Iteration 12700 (58.4627 iter/s, 1.71049s/100 iter), loss = 0.115948
I0628 19:00:47.042189  7628 solver.cpp:371]     Train net output #0: loss = 0.115949 (* 1 = 0.115949 loss)
I0628 19:00:47.042194  7628 sgd_solver.cpp:137] Iteration 12700, lr = 0.0801563, m = 0.9
I0628 19:00:48.756283  7628 solver.cpp:349] Iteration 12800 (58.3423 iter/s, 1.71402s/100 iter), loss = 0.332341
I0628 19:00:48.756305  7628 solver.cpp:371]     Train net output #0: loss = 0.332342 (* 1 = 0.332342 loss)
I0628 19:00:48.756325  7628 sgd_solver.cpp:137] Iteration 12800, lr = 0.08, m = 0.9
I0628 19:00:50.468256  7628 solver.cpp:349] Iteration 12900 (58.416 iter/s, 1.71186s/100 iter), loss = 0.105072
I0628 19:00:50.468281  7628 solver.cpp:371]     Train net output #0: loss = 0.105073 (* 1 = 0.105073 loss)
I0628 19:00:50.468286  7628 sgd_solver.cpp:137] Iteration 12900, lr = 0.0798438, m = 0.9
I0628 19:00:52.163018  7628 solver.cpp:545] Iteration 13000, Testing net (#0)
I0628 19:00:53.172190  7626 data_reader.cpp:262] Starting prefetch of epoch 13
I0628 19:00:53.192456  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8054
I0628 19:00:53.192468  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.979
I0628 19:00:53.192473  7628 solver.cpp:630]     Test net output #2: loss = 0.759524 (* 1 = 0.759524 loss)
I0628 19:00:53.192487  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02944s
I0628 19:00:53.209723  7628 solver.cpp:349] Iteration 13000 (36.4786 iter/s, 2.74134s/100 iter), loss = 0.243238
I0628 19:00:53.209746  7628 solver.cpp:371]     Train net output #0: loss = 0.243238 (* 1 = 0.243238 loss)
I0628 19:00:53.209750  7628 sgd_solver.cpp:137] Iteration 13000, lr = 0.0796875, m = 0.9
I0628 19:00:54.919363  7628 solver.cpp:349] Iteration 13100 (58.4951 iter/s, 1.70954s/100 iter), loss = 0.047774
I0628 19:00:54.919384  7628 solver.cpp:371]     Train net output #0: loss = 0.0477745 (* 1 = 0.0477745 loss)
I0628 19:00:54.919389  7628 sgd_solver.cpp:137] Iteration 13100, lr = 0.0795313, m = 0.9
I0628 19:00:56.635632  7628 solver.cpp:349] Iteration 13200 (58.2691 iter/s, 1.71618s/100 iter), loss = 0.0969247
I0628 19:00:56.635654  7628 solver.cpp:371]     Train net output #0: loss = 0.0969252 (* 1 = 0.0969252 loss)
I0628 19:00:56.635658  7628 sgd_solver.cpp:137] Iteration 13200, lr = 0.079375, m = 0.9
I0628 19:00:57.970258  7613 data_reader.cpp:262] Starting prefetch of epoch 17
I0628 19:00:58.347604  7628 solver.cpp:349] Iteration 13300 (58.4154 iter/s, 1.71188s/100 iter), loss = 0.162249
I0628 19:00:58.347625  7628 solver.cpp:371]     Train net output #0: loss = 0.16225 (* 1 = 0.16225 loss)
I0628 19:00:58.347628  7628 sgd_solver.cpp:137] Iteration 13300, lr = 0.0792188, m = 0.9
I0628 19:01:00.057996  7628 solver.cpp:349] Iteration 13400 (58.4693 iter/s, 1.7103s/100 iter), loss = 0.0838849
I0628 19:01:00.058080  7628 solver.cpp:371]     Train net output #0: loss = 0.0838855 (* 1 = 0.0838855 loss)
I0628 19:01:00.058087  7628 sgd_solver.cpp:137] Iteration 13400, lr = 0.0790625, m = 0.9
I0628 19:01:01.773780  7628 solver.cpp:349] Iteration 13500 (58.2879 iter/s, 1.71562s/100 iter), loss = 0.253003
I0628 19:01:01.773803  7628 solver.cpp:371]     Train net output #0: loss = 0.253003 (* 1 = 0.253003 loss)
I0628 19:01:01.773808  7628 sgd_solver.cpp:137] Iteration 13500, lr = 0.0789063, m = 0.9
I0628 19:01:03.484941  7628 solver.cpp:349] Iteration 13600 (58.4432 iter/s, 1.71106s/100 iter), loss = 0.231076
I0628 19:01:03.484964  7628 solver.cpp:371]     Train net output #0: loss = 0.231077 (* 1 = 0.231077 loss)
I0628 19:01:03.484971  7628 sgd_solver.cpp:137] Iteration 13600, lr = 0.07875, m = 0.9
I0628 19:01:05.197497  7628 solver.cpp:349] Iteration 13700 (58.3956 iter/s, 1.71246s/100 iter), loss = 0.201843
I0628 19:01:05.197518  7628 solver.cpp:371]     Train net output #0: loss = 0.201844 (* 1 = 0.201844 loss)
I0628 19:01:05.197525  7628 sgd_solver.cpp:137] Iteration 13700, lr = 0.0785938, m = 0.9
I0628 19:01:06.908433  7628 solver.cpp:349] Iteration 13800 (58.4509 iter/s, 1.71084s/100 iter), loss = 0.255508
I0628 19:01:06.908455  7628 solver.cpp:371]     Train net output #0: loss = 0.255509 (* 1 = 0.255509 loss)
I0628 19:01:06.908459  7628 sgd_solver.cpp:137] Iteration 13800, lr = 0.0784375, m = 0.9
I0628 19:01:08.629220  7628 solver.cpp:349] Iteration 13900 (58.1162 iter/s, 1.72069s/100 iter), loss = 0.216105
I0628 19:01:08.629242  7628 solver.cpp:371]     Train net output #0: loss = 0.216106 (* 1 = 0.216106 loss)
I0628 19:01:08.629246  7628 sgd_solver.cpp:137] Iteration 13900, lr = 0.0782812, m = 0.9
I0628 19:01:10.327332  7628 solver.cpp:545] Iteration 14000, Testing net (#0)
I0628 19:01:11.334349  7626 data_reader.cpp:262] Starting prefetch of epoch 14
I0628 19:01:11.357941  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7944
I0628 19:01:11.357961  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.983801
I0628 19:01:11.357966  7628 solver.cpp:630]     Test net output #2: loss = 0.834006 (* 1 = 0.834006 loss)
I0628 19:01:11.357983  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03062s
I0628 19:01:11.375288  7628 solver.cpp:349] Iteration 14000 (36.4174 iter/s, 2.74594s/100 iter), loss = 0.209353
I0628 19:01:11.375304  7628 solver.cpp:371]     Train net output #0: loss = 0.209353 (* 1 = 0.209353 loss)
I0628 19:01:11.375308  7628 sgd_solver.cpp:137] Iteration 14000, lr = 0.078125, m = 0.9
I0628 19:01:12.385795  7613 data_reader.cpp:262] Starting prefetch of epoch 18
I0628 19:01:13.085840  7628 solver.cpp:349] Iteration 14100 (58.4637 iter/s, 1.71046s/100 iter), loss = 0.299395
I0628 19:01:13.085862  7628 solver.cpp:371]     Train net output #0: loss = 0.299395 (* 1 = 0.299395 loss)
I0628 19:01:13.085866  7628 sgd_solver.cpp:137] Iteration 14100, lr = 0.0779688, m = 0.9
I0628 19:01:14.797941  7628 solver.cpp:349] Iteration 14200 (58.411 iter/s, 1.71201s/100 iter), loss = 0.110924
I0628 19:01:14.797963  7628 solver.cpp:371]     Train net output #0: loss = 0.110924 (* 1 = 0.110924 loss)
I0628 19:01:14.797968  7628 sgd_solver.cpp:137] Iteration 14200, lr = 0.0778125, m = 0.9
I0628 19:01:16.513340  7628 solver.cpp:349] Iteration 14300 (58.2987 iter/s, 1.71531s/100 iter), loss = 0.141536
I0628 19:01:16.513363  7628 solver.cpp:371]     Train net output #0: loss = 0.141536 (* 1 = 0.141536 loss)
I0628 19:01:16.513367  7628 sgd_solver.cpp:137] Iteration 14300, lr = 0.0776563, m = 0.9
I0628 19:01:18.222049  7628 solver.cpp:349] Iteration 14400 (58.527 iter/s, 1.70861s/100 iter), loss = 0.0302711
I0628 19:01:18.222074  7628 solver.cpp:371]     Train net output #0: loss = 0.0302717 (* 1 = 0.0302717 loss)
I0628 19:01:18.222077  7628 sgd_solver.cpp:137] Iteration 14400, lr = 0.0775, m = 0.9
I0628 19:01:19.934074  7628 solver.cpp:349] Iteration 14500 (58.4137 iter/s, 1.71193s/100 iter), loss = 0.0424164
I0628 19:01:19.934096  7628 solver.cpp:371]     Train net output #0: loss = 0.042417 (* 1 = 0.042417 loss)
I0628 19:01:19.934115  7628 sgd_solver.cpp:137] Iteration 14500, lr = 0.0773438, m = 0.9
I0628 19:01:21.644227  7628 solver.cpp:349] Iteration 14600 (58.4781 iter/s, 1.71004s/100 iter), loss = 0.201706
I0628 19:01:21.644249  7628 solver.cpp:371]     Train net output #0: loss = 0.201707 (* 1 = 0.201707 loss)
I0628 19:01:21.644253  7628 sgd_solver.cpp:137] Iteration 14600, lr = 0.0771875, m = 0.9
I0628 19:01:23.364027  7628 solver.cpp:349] Iteration 14700 (58.1496 iter/s, 1.7197s/100 iter), loss = 0.107836
I0628 19:01:23.364049  7628 solver.cpp:371]     Train net output #0: loss = 0.107836 (* 1 = 0.107836 loss)
I0628 19:01:23.364055  7628 sgd_solver.cpp:137] Iteration 14700, lr = 0.0770312, m = 0.9
I0628 19:01:25.075682  7628 solver.cpp:349] Iteration 14800 (58.4263 iter/s, 1.71156s/100 iter), loss = 0.145064
I0628 19:01:25.075703  7628 solver.cpp:371]     Train net output #0: loss = 0.145064 (* 1 = 0.145064 loss)
I0628 19:01:25.075707  7628 sgd_solver.cpp:137] Iteration 14800, lr = 0.076875, m = 0.9
I0628 19:01:25.761139  7613 data_reader.cpp:262] Starting prefetch of epoch 19
I0628 19:01:26.787044  7628 solver.cpp:349] Iteration 14900 (58.4362 iter/s, 1.71127s/100 iter), loss = 0.0956827
I0628 19:01:26.787065  7628 solver.cpp:371]     Train net output #0: loss = 0.0956833 (* 1 = 0.0956833 loss)
I0628 19:01:26.787070  7628 sgd_solver.cpp:137] Iteration 14900, lr = 0.0767187, m = 0.9
I0628 19:01:28.487818  7628 solver.cpp:545] Iteration 15000, Testing net (#0)
I0628 19:01:29.495463  7626 data_reader.cpp:262] Starting prefetch of epoch 15
I0628 19:01:29.516189  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7782
I0628 19:01:29.516202  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9876
I0628 19:01:29.516208  7628 solver.cpp:630]     Test net output #2: loss = 0.760733 (* 1 = 0.760733 loss)
I0628 19:01:29.516225  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02837s
I0628 19:01:29.533433  7628 solver.cpp:349] Iteration 15000 (36.4131 iter/s, 2.74626s/100 iter), loss = 0.150676
I0628 19:01:29.533457  7628 solver.cpp:371]     Train net output #0: loss = 0.150677 (* 1 = 0.150677 loss)
I0628 19:01:29.533463  7628 sgd_solver.cpp:137] Iteration 15000, lr = 0.0765625, m = 0.9
I0628 19:01:31.248615  7628 solver.cpp:349] Iteration 15100 (58.3061 iter/s, 1.71509s/100 iter), loss = 0.247647
I0628 19:01:31.248690  7628 solver.cpp:371]     Train net output #0: loss = 0.247647 (* 1 = 0.247647 loss)
I0628 19:01:31.248698  7628 sgd_solver.cpp:137] Iteration 15100, lr = 0.0764063, m = 0.9
I0628 19:01:32.961988  7628 solver.cpp:349] Iteration 15200 (58.3697 iter/s, 1.71322s/100 iter), loss = 0.220546
I0628 19:01:32.962011  7628 solver.cpp:371]     Train net output #0: loss = 0.220547 (* 1 = 0.220547 loss)
I0628 19:01:32.962018  7628 sgd_solver.cpp:137] Iteration 15200, lr = 0.07625, m = 0.9
I0628 19:01:34.673091  7628 solver.cpp:349] Iteration 15300 (58.4452 iter/s, 1.711s/100 iter), loss = 0.199043
I0628 19:01:34.673115  7628 solver.cpp:371]     Train net output #0: loss = 0.199044 (* 1 = 0.199044 loss)
I0628 19:01:34.673118  7628 sgd_solver.cpp:137] Iteration 15300, lr = 0.0760938, m = 0.9
I0628 19:01:36.387403  7628 solver.cpp:349] Iteration 15400 (58.3358 iter/s, 1.71421s/100 iter), loss = 0.10147
I0628 19:01:36.387423  7628 solver.cpp:371]     Train net output #0: loss = 0.101471 (* 1 = 0.101471 loss)
I0628 19:01:36.387426  7628 sgd_solver.cpp:137] Iteration 15400, lr = 0.0759375, m = 0.9
I0628 19:01:38.098556  7628 solver.cpp:349] Iteration 15500 (58.4433 iter/s, 1.71106s/100 iter), loss = 0.201609
I0628 19:01:38.098577  7628 solver.cpp:371]     Train net output #0: loss = 0.201609 (* 1 = 0.201609 loss)
I0628 19:01:38.098582  7628 sgd_solver.cpp:137] Iteration 15500, lr = 0.0757812, m = 0.9
I0628 19:01:39.810825  7628 solver.cpp:349] Iteration 15600 (58.4052 iter/s, 1.71218s/100 iter), loss = 0.0450677
I0628 19:01:39.810848  7628 solver.cpp:371]     Train net output #0: loss = 0.0450682 (* 1 = 0.0450682 loss)
I0628 19:01:39.810853  7628 sgd_solver.cpp:137] Iteration 15600, lr = 0.075625, m = 0.9
I0628 19:01:40.170361  7613 data_reader.cpp:262] Starting prefetch of epoch 20
I0628 19:01:41.521721  7628 solver.cpp:349] Iteration 15700 (58.4522 iter/s, 1.7108s/100 iter), loss = 0.15556
I0628 19:01:41.521742  7628 solver.cpp:371]     Train net output #0: loss = 0.15556 (* 1 = 0.15556 loss)
I0628 19:01:41.521747  7628 sgd_solver.cpp:137] Iteration 15700, lr = 0.0754687, m = 0.9
I0628 19:01:43.232728  7628 solver.cpp:349] Iteration 15800 (58.4484 iter/s, 1.71091s/100 iter), loss = 0.0627177
I0628 19:01:43.232750  7628 solver.cpp:371]     Train net output #0: loss = 0.0627182 (* 1 = 0.0627182 loss)
I0628 19:01:43.232754  7628 sgd_solver.cpp:137] Iteration 15800, lr = 0.0753125, m = 0.9
I0628 19:01:44.946244  7628 solver.cpp:349] Iteration 15900 (58.3628 iter/s, 1.71342s/100 iter), loss = 0.0412791
I0628 19:01:44.946266  7628 solver.cpp:371]     Train net output #0: loss = 0.0412796 (* 1 = 0.0412796 loss)
I0628 19:01:44.946271  7628 sgd_solver.cpp:137] Iteration 15900, lr = 0.0751562, m = 0.9
I0628 19:01:46.648152  7628 solver.cpp:545] Iteration 16000, Testing net (#0)
I0628 19:01:47.658442  7626 data_reader.cpp:262] Starting prefetch of epoch 16
I0628 19:01:47.678848  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.817
I0628 19:01:47.678859  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.982401
I0628 19:01:47.678865  7628 solver.cpp:630]     Test net output #2: loss = 0.70037 (* 1 = 0.70037 loss)
I0628 19:01:47.678880  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.0307s
I0628 19:01:47.696147  7628 solver.cpp:349] Iteration 16000 (36.3666 iter/s, 2.74978s/100 iter), loss = 0.0383414
I0628 19:01:47.696171  7628 solver.cpp:371]     Train net output #0: loss = 0.0383419 (* 1 = 0.0383419 loss)
I0628 19:01:47.696175  7628 sgd_solver.cpp:137] Iteration 16000, lr = 0.075, m = 0.9
I0628 19:01:49.405478  7628 solver.cpp:349] Iteration 16100 (58.5057 iter/s, 1.70923s/100 iter), loss = 0.242713
I0628 19:01:49.405498  7628 solver.cpp:371]     Train net output #0: loss = 0.242713 (* 1 = 0.242713 loss)
I0628 19:01:49.405501  7628 sgd_solver.cpp:137] Iteration 16100, lr = 0.0748438, m = 0.9
I0628 19:01:51.117408  7628 solver.cpp:349] Iteration 16200 (58.4168 iter/s, 1.71184s/100 iter), loss = 0.313398
I0628 19:01:51.117429  7628 solver.cpp:371]     Train net output #0: loss = 0.313398 (* 1 = 0.313398 loss)
I0628 19:01:51.117446  7628 sgd_solver.cpp:137] Iteration 16200, lr = 0.0746875, m = 0.9
I0628 19:01:52.832142  7628 solver.cpp:349] Iteration 16300 (58.3217 iter/s, 1.71463s/100 iter), loss = 0.0471741
I0628 19:01:52.832166  7628 solver.cpp:371]     Train net output #0: loss = 0.0471746 (* 1 = 0.0471746 loss)
I0628 19:01:52.832171  7628 sgd_solver.cpp:137] Iteration 16300, lr = 0.0745312, m = 0.9
I0628 19:01:54.555857  7628 solver.cpp:349] Iteration 16400 (58.0176 iter/s, 1.72362s/100 iter), loss = 0.0955996
I0628 19:01:54.555878  7628 solver.cpp:371]     Train net output #0: loss = 0.0956001 (* 1 = 0.0956001 loss)
I0628 19:01:54.555883  7628 sgd_solver.cpp:137] Iteration 16400, lr = 0.074375, m = 0.9
I0628 19:01:54.608268  7613 data_reader.cpp:262] Starting prefetch of epoch 21
I0628 19:01:56.267971  7628 solver.cpp:349] Iteration 16500 (58.4106 iter/s, 1.71202s/100 iter), loss = 0.233595
I0628 19:01:56.267992  7628 solver.cpp:371]     Train net output #0: loss = 0.233596 (* 1 = 0.233596 loss)
I0628 19:01:56.267997  7628 sgd_solver.cpp:137] Iteration 16500, lr = 0.0742188, m = 0.9
I0628 19:01:57.982388  7628 solver.cpp:349] Iteration 16600 (58.3321 iter/s, 1.71432s/100 iter), loss = 0.112702
I0628 19:01:57.982410  7628 solver.cpp:371]     Train net output #0: loss = 0.112702 (* 1 = 0.112702 loss)
I0628 19:01:57.982414  7628 sgd_solver.cpp:137] Iteration 16600, lr = 0.0740625, m = 0.9
I0628 19:01:59.698839  7628 solver.cpp:349] Iteration 16700 (58.2629 iter/s, 1.71636s/100 iter), loss = 0.264605
I0628 19:01:59.698860  7628 solver.cpp:371]     Train net output #0: loss = 0.264606 (* 1 = 0.264606 loss)
I0628 19:01:59.698865  7628 sgd_solver.cpp:137] Iteration 16700, lr = 0.0739063, m = 0.9
I0628 19:02:01.414146  7628 solver.cpp:349] Iteration 16800 (58.3018 iter/s, 1.71521s/100 iter), loss = 0.178389
I0628 19:02:01.414199  7628 solver.cpp:371]     Train net output #0: loss = 0.178389 (* 1 = 0.178389 loss)
I0628 19:02:01.414206  7628 sgd_solver.cpp:137] Iteration 16800, lr = 0.07375, m = 0.9
I0628 19:02:03.131058  7628 solver.cpp:349] Iteration 16900 (58.2485 iter/s, 1.71678s/100 iter), loss = 0.0772925
I0628 19:02:03.131081  7628 solver.cpp:371]     Train net output #0: loss = 0.0772931 (* 1 = 0.0772931 loss)
I0628 19:02:03.131088  7628 sgd_solver.cpp:137] Iteration 16900, lr = 0.0735938, m = 0.9
I0628 19:02:04.827167  7628 solver.cpp:545] Iteration 17000, Testing net (#0)
I0628 19:02:05.834820  7626 data_reader.cpp:262] Starting prefetch of epoch 17
I0628 19:02:05.856271  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8258
I0628 19:02:05.856283  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.99
I0628 19:02:05.856288  7628 solver.cpp:630]     Test net output #2: loss = 0.659182 (* 1 = 0.659182 loss)
I0628 19:02:05.856302  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.0291s
I0628 19:02:05.873535  7628 solver.cpp:349] Iteration 17000 (36.4651 iter/s, 2.74235s/100 iter), loss = 0.064513
I0628 19:02:05.873551  7628 solver.cpp:371]     Train net output #0: loss = 0.0645136 (* 1 = 0.0645136 loss)
I0628 19:02:05.873556  7628 sgd_solver.cpp:137] Iteration 17000, lr = 0.0734375, m = 0.9
I0628 19:02:07.590374  7628 solver.cpp:349] Iteration 17100 (58.2496 iter/s, 1.71675s/100 iter), loss = 0.0429606
I0628 19:02:07.590396  7628 solver.cpp:371]     Train net output #0: loss = 0.0429611 (* 1 = 0.0429611 loss)
I0628 19:02:07.590399  7628 sgd_solver.cpp:137] Iteration 17100, lr = 0.0732813, m = 0.9
I0628 19:02:09.028726  7613 data_reader.cpp:262] Starting prefetch of epoch 22
I0628 19:02:09.301760  7628 solver.cpp:349] Iteration 17200 (58.4353 iter/s, 1.71129s/100 iter), loss = 0.115812
I0628 19:02:09.301782  7628 solver.cpp:371]     Train net output #0: loss = 0.115812 (* 1 = 0.115812 loss)
I0628 19:02:09.301786  7628 sgd_solver.cpp:137] Iteration 17200, lr = 0.073125, m = 0.9
I0628 19:02:11.018302  7628 solver.cpp:349] Iteration 17300 (58.2598 iter/s, 1.71645s/100 iter), loss = 0.187114
I0628 19:02:11.018324  7628 solver.cpp:371]     Train net output #0: loss = 0.187115 (* 1 = 0.187115 loss)
I0628 19:02:11.018328  7628 sgd_solver.cpp:137] Iteration 17300, lr = 0.0729688, m = 0.9
I0628 19:02:12.731353  7628 solver.cpp:349] Iteration 17400 (58.3786 iter/s, 1.71296s/100 iter), loss = 0.343612
I0628 19:02:12.731375  7628 solver.cpp:371]     Train net output #0: loss = 0.343613 (* 1 = 0.343613 loss)
I0628 19:02:12.731380  7628 sgd_solver.cpp:137] Iteration 17400, lr = 0.0728125, m = 0.9
I0628 19:02:14.449165  7628 solver.cpp:349] Iteration 17500 (58.2169 iter/s, 1.71771s/100 iter), loss = 0.191918
I0628 19:02:14.449187  7628 solver.cpp:371]     Train net output #0: loss = 0.191918 (* 1 = 0.191918 loss)
I0628 19:02:14.449190  7628 sgd_solver.cpp:137] Iteration 17500, lr = 0.0726563, m = 0.9
I0628 19:02:16.168869  7628 solver.cpp:349] Iteration 17600 (58.1528 iter/s, 1.71961s/100 iter), loss = 0.0775687
I0628 19:02:16.168893  7628 solver.cpp:371]     Train net output #0: loss = 0.0775692 (* 1 = 0.0775692 loss)
I0628 19:02:16.168897  7628 sgd_solver.cpp:137] Iteration 17600, lr = 0.0725, m = 0.9
I0628 19:02:17.879652  7628 solver.cpp:349] Iteration 17700 (58.4561 iter/s, 1.71069s/100 iter), loss = 0.19436
I0628 19:02:17.879675  7628 solver.cpp:371]     Train net output #0: loss = 0.194361 (* 1 = 0.194361 loss)
I0628 19:02:17.879679  7628 sgd_solver.cpp:137] Iteration 17700, lr = 0.0723438, m = 0.9
I0628 19:02:19.593752  7628 solver.cpp:349] Iteration 17800 (58.3429 iter/s, 1.714s/100 iter), loss = 0.167825
I0628 19:02:19.593775  7628 solver.cpp:371]     Train net output #0: loss = 0.167825 (* 1 = 0.167825 loss)
I0628 19:02:19.593780  7628 sgd_solver.cpp:137] Iteration 17800, lr = 0.0721875, m = 0.9
I0628 19:02:21.304932  7628 solver.cpp:349] Iteration 17900 (58.4425 iter/s, 1.71108s/100 iter), loss = 0.108304
I0628 19:02:21.304955  7628 solver.cpp:371]     Train net output #0: loss = 0.108305 (* 1 = 0.108305 loss)
I0628 19:02:21.304980  7628 sgd_solver.cpp:137] Iteration 17900, lr = 0.0720313, m = 0.9
I0628 19:02:22.420881  7613 data_reader.cpp:262] Starting prefetch of epoch 23
I0628 19:02:23.004319  7628 solver.cpp:545] Iteration 18000, Testing net (#0)
I0628 19:02:24.011137  7626 data_reader.cpp:262] Starting prefetch of epoch 18
I0628 19:02:24.031456  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7576
I0628 19:02:24.031471  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9844
I0628 19:02:24.031476  7628 solver.cpp:630]     Test net output #2: loss = 1.01993 (* 1 = 1.01993 loss)
I0628 19:02:24.031489  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02714s
I0628 19:02:24.048673  7628 solver.cpp:349] Iteration 18000 (36.4486 iter/s, 2.74359s/100 iter), loss = 0.156118
I0628 19:02:24.048696  7628 solver.cpp:371]     Train net output #0: loss = 0.156119 (* 1 = 0.156119 loss)
I0628 19:02:24.048701  7628 sgd_solver.cpp:137] Iteration 18000, lr = 0.071875, m = 0.9
I0628 19:02:25.765646  7628 solver.cpp:349] Iteration 18100 (58.2454 iter/s, 1.71688s/100 iter), loss = 0.0753203
I0628 19:02:25.765676  7628 solver.cpp:371]     Train net output #0: loss = 0.0753209 (* 1 = 0.0753209 loss)
I0628 19:02:25.765682  7628 sgd_solver.cpp:137] Iteration 18100, lr = 0.0717188, m = 0.9
I0628 19:02:27.487658  7628 solver.cpp:349] Iteration 18200 (58.0753 iter/s, 1.7219s/100 iter), loss = 0.0712755
I0628 19:02:27.487679  7628 solver.cpp:371]     Train net output #0: loss = 0.071276 (* 1 = 0.071276 loss)
I0628 19:02:27.487684  7628 sgd_solver.cpp:137] Iteration 18200, lr = 0.0715625, m = 0.9
I0628 19:02:29.202008  7628 solver.cpp:349] Iteration 18300 (58.3344 iter/s, 1.71426s/100 iter), loss = 0.161428
I0628 19:02:29.202031  7628 solver.cpp:371]     Train net output #0: loss = 0.161428 (* 1 = 0.161428 loss)
I0628 19:02:29.202035  7628 sgd_solver.cpp:137] Iteration 18300, lr = 0.0714063, m = 0.9
I0628 19:02:30.920578  7628 solver.cpp:349] Iteration 18400 (58.1911 iter/s, 1.71848s/100 iter), loss = 0.0964538
I0628 19:02:30.920600  7628 solver.cpp:371]     Train net output #0: loss = 0.0964542 (* 1 = 0.0964542 loss)
I0628 19:02:30.920604  7628 sgd_solver.cpp:137] Iteration 18400, lr = 0.07125, m = 0.9
I0628 19:02:32.633864  7628 solver.cpp:349] Iteration 18500 (58.3706 iter/s, 1.71319s/100 iter), loss = 0.360065
I0628 19:02:32.633931  7628 solver.cpp:371]     Train net output #0: loss = 0.360066 (* 1 = 0.360066 loss)
I0628 19:02:32.633936  7628 sgd_solver.cpp:137] Iteration 18500, lr = 0.0710938, m = 0.9
I0628 19:02:34.344336  7628 solver.cpp:349] Iteration 18600 (58.4683 iter/s, 1.71033s/100 iter), loss = 0.1421
I0628 19:02:34.344357  7628 solver.cpp:371]     Train net output #0: loss = 0.1421 (* 1 = 0.1421 loss)
I0628 19:02:34.344360  7628 sgd_solver.cpp:137] Iteration 18600, lr = 0.0709375, m = 0.9
I0628 19:02:36.054982  7628 solver.cpp:349] Iteration 18700 (58.4606 iter/s, 1.71055s/100 iter), loss = 0.0430234
I0628 19:02:36.055003  7628 solver.cpp:371]     Train net output #0: loss = 0.0430238 (* 1 = 0.0430238 loss)
I0628 19:02:36.055007  7628 sgd_solver.cpp:137] Iteration 18700, lr = 0.0707813, m = 0.9
I0628 19:02:36.841756  7613 data_reader.cpp:262] Starting prefetch of epoch 24
I0628 19:02:37.768894  7628 solver.cpp:349] Iteration 18800 (58.3492 iter/s, 1.71382s/100 iter), loss = 0.079406
I0628 19:02:37.768918  7628 solver.cpp:371]     Train net output #0: loss = 0.0794064 (* 1 = 0.0794064 loss)
I0628 19:02:37.768921  7628 sgd_solver.cpp:137] Iteration 18800, lr = 0.070625, m = 0.9
I0628 19:02:39.487743  7628 solver.cpp:349] Iteration 18900 (58.1817 iter/s, 1.71875s/100 iter), loss = 0.246608
I0628 19:02:39.487766  7628 solver.cpp:371]     Train net output #0: loss = 0.246608 (* 1 = 0.246608 loss)
I0628 19:02:39.487771  7628 sgd_solver.cpp:137] Iteration 18900, lr = 0.0704687, m = 0.9
I0628 19:02:41.183037  7628 solver.cpp:545] Iteration 19000, Testing net (#0)
I0628 19:02:42.191124  7626 data_reader.cpp:262] Starting prefetch of epoch 19
I0628 19:02:42.212862  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7804
I0628 19:02:42.212875  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.984801
I0628 19:02:42.212880  7628 solver.cpp:630]     Test net output #2: loss = 0.845874 (* 1 = 0.845874 loss)
I0628 19:02:42.212894  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02982s
I0628 19:02:42.230347  7628 solver.cpp:349] Iteration 19000 (36.4634 iter/s, 2.74247s/100 iter), loss = 0.228494
I0628 19:02:42.230370  7628 solver.cpp:371]     Train net output #0: loss = 0.228495 (* 1 = 0.228495 loss)
I0628 19:02:42.230374  7628 sgd_solver.cpp:137] Iteration 19000, lr = 0.0703125, m = 0.9
I0628 19:02:43.947237  7628 solver.cpp:349] Iteration 19100 (58.2481 iter/s, 1.71679s/100 iter), loss = 0.137511
I0628 19:02:43.947259  7628 solver.cpp:371]     Train net output #0: loss = 0.137511 (* 1 = 0.137511 loss)
I0628 19:02:43.947264  7628 sgd_solver.cpp:137] Iteration 19100, lr = 0.0701563, m = 0.9
I0628 19:02:45.661747  7628 solver.cpp:349] Iteration 19200 (58.3289 iter/s, 1.71442s/100 iter), loss = 0.188803
I0628 19:02:45.661769  7628 solver.cpp:371]     Train net output #0: loss = 0.188804 (* 1 = 0.188804 loss)
I0628 19:02:45.661774  7628 sgd_solver.cpp:137] Iteration 19200, lr = 0.07, m = 0.9
I0628 19:02:47.378449  7628 solver.cpp:349] Iteration 19300 (58.2545 iter/s, 1.71661s/100 iter), loss = 0.0854008
I0628 19:02:47.378473  7628 solver.cpp:371]     Train net output #0: loss = 0.0854013 (* 1 = 0.0854013 loss)
I0628 19:02:47.378476  7628 sgd_solver.cpp:137] Iteration 19300, lr = 0.0698438, m = 0.9
I0628 19:02:49.094321  7628 solver.cpp:349] Iteration 19400 (58.2827 iter/s, 1.71578s/100 iter), loss = 0.155817
I0628 19:02:49.094343  7628 solver.cpp:371]     Train net output #0: loss = 0.155818 (* 1 = 0.155818 loss)
I0628 19:02:49.094347  7628 sgd_solver.cpp:137] Iteration 19400, lr = 0.0696875, m = 0.9
I0628 19:02:50.818792  7628 solver.cpp:349] Iteration 19500 (57.992 iter/s, 1.72438s/100 iter), loss = 0.0426153
I0628 19:02:50.818816  7628 solver.cpp:371]     Train net output #0: loss = 0.0426157 (* 1 = 0.0426157 loss)
I0628 19:02:50.818820  7628 sgd_solver.cpp:137] Iteration 19500, lr = 0.0695313, m = 0.9
I0628 19:02:51.298395  7613 data_reader.cpp:262] Starting prefetch of epoch 25
I0628 19:02:52.537441  7628 solver.cpp:349] Iteration 19600 (58.1885 iter/s, 1.71855s/100 iter), loss = 0.0243979
I0628 19:02:52.537463  7628 solver.cpp:371]     Train net output #0: loss = 0.0243983 (* 1 = 0.0243983 loss)
I0628 19:02:52.537482  7628 sgd_solver.cpp:137] Iteration 19600, lr = 0.069375, m = 0.9
I0628 19:02:54.253075  7628 solver.cpp:349] Iteration 19700 (58.2913 iter/s, 1.71552s/100 iter), loss = 0.123077
I0628 19:02:54.253098  7628 solver.cpp:371]     Train net output #0: loss = 0.123078 (* 1 = 0.123078 loss)
I0628 19:02:54.253101  7628 sgd_solver.cpp:137] Iteration 19700, lr = 0.0692187, m = 0.9
I0628 19:02:55.973013  7628 solver.cpp:349] Iteration 19800 (58.1448 iter/s, 1.71984s/100 iter), loss = 0.12663
I0628 19:02:55.973037  7628 solver.cpp:371]     Train net output #0: loss = 0.126631 (* 1 = 0.126631 loss)
I0628 19:02:55.973040  7628 sgd_solver.cpp:137] Iteration 19800, lr = 0.0690625, m = 0.9
I0628 19:02:57.690799  7628 solver.cpp:349] Iteration 19900 (58.2177 iter/s, 1.71769s/100 iter), loss = 0.139685
I0628 19:02:57.690821  7628 solver.cpp:371]     Train net output #0: loss = 0.139686 (* 1 = 0.139686 loss)
I0628 19:02:57.690825  7628 sgd_solver.cpp:137] Iteration 19900, lr = 0.0689062, m = 0.9
I0628 19:02:59.388952  7628 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_20000.caffemodel
I0628 19:02:59.396878  7628 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_20000.solverstate
I0628 19:02:59.400355  7628 solver.cpp:545] Iteration 20000, Testing net (#0)
I0628 19:03:00.407846  7626 data_reader.cpp:262] Starting prefetch of epoch 20
I0628 19:03:00.428213  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7706
I0628 19:03:00.428228  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9888
I0628 19:03:00.428236  7628 solver.cpp:630]     Test net output #2: loss = 0.876744 (* 1 = 0.876744 loss)
I0628 19:03:00.428253  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02786s
I0628 19:03:00.445734  7628 solver.cpp:349] Iteration 20000 (36.3002 iter/s, 2.7548s/100 iter), loss = 0.0415254
I0628 19:03:00.445760  7628 solver.cpp:371]     Train net output #0: loss = 0.041526 (* 1 = 0.041526 loss)
I0628 19:03:00.445766  7628 sgd_solver.cpp:137] Iteration 20000, lr = 0.06875, m = 0.9
I0628 19:03:02.161533  7628 solver.cpp:349] Iteration 20100 (58.2854 iter/s, 1.7157s/100 iter), loss = 0.163195
I0628 19:03:02.161559  7628 solver.cpp:371]     Train net output #0: loss = 0.163196 (* 1 = 0.163196 loss)
I0628 19:03:02.161566  7628 sgd_solver.cpp:137] Iteration 20100, lr = 0.0685938, m = 0.9
I0628 19:03:03.879040  7628 solver.cpp:349] Iteration 20200 (58.2274 iter/s, 1.7174s/100 iter), loss = 0.113287
I0628 19:03:03.880627  7628 solver.cpp:371]     Train net output #0: loss = 0.113288 (* 1 = 0.113288 loss)
I0628 19:03:03.880635  7628 sgd_solver.cpp:137] Iteration 20200, lr = 0.0684375, m = 0.9
I0628 19:03:05.596398  7628 solver.cpp:349] Iteration 20300 (58.2856 iter/s, 1.71569s/100 iter), loss = 0.0162844
I0628 19:03:05.596421  7628 solver.cpp:371]     Train net output #0: loss = 0.016285 (* 1 = 0.016285 loss)
I0628 19:03:05.596426  7628 sgd_solver.cpp:137] Iteration 20300, lr = 0.0682813, m = 0.9
I0628 19:03:05.751698  7613 data_reader.cpp:262] Starting prefetch of epoch 26
I0628 19:03:07.314674  7628 solver.cpp:349] Iteration 20400 (58.2011 iter/s, 1.71818s/100 iter), loss = 0.0662627
I0628 19:03:07.314695  7628 solver.cpp:371]     Train net output #0: loss = 0.0662633 (* 1 = 0.0662633 loss)
I0628 19:03:07.314700  7628 sgd_solver.cpp:137] Iteration 20400, lr = 0.068125, m = 0.9
I0628 19:03:09.033452  7628 solver.cpp:349] Iteration 20500 (58.1841 iter/s, 1.71868s/100 iter), loss = 0.0271667
I0628 19:03:09.033480  7628 solver.cpp:371]     Train net output #0: loss = 0.0271672 (* 1 = 0.0271672 loss)
I0628 19:03:09.033488  7628 sgd_solver.cpp:137] Iteration 20500, lr = 0.0679687, m = 0.9
I0628 19:03:10.752234  7628 solver.cpp:349] Iteration 20600 (58.1844 iter/s, 1.71867s/100 iter), loss = 0.257284
I0628 19:03:10.752257  7628 solver.cpp:371]     Train net output #0: loss = 0.257284 (* 1 = 0.257284 loss)
I0628 19:03:10.752261  7628 sgd_solver.cpp:137] Iteration 20600, lr = 0.0678125, m = 0.9
I0628 19:03:12.471570  7628 solver.cpp:349] Iteration 20700 (58.1652 iter/s, 1.71924s/100 iter), loss = 0.084298
I0628 19:03:12.471595  7628 solver.cpp:371]     Train net output #0: loss = 0.0842984 (* 1 = 0.0842984 loss)
I0628 19:03:12.471601  7628 sgd_solver.cpp:137] Iteration 20700, lr = 0.0676562, m = 0.9
I0628 19:03:14.185050  7628 solver.cpp:349] Iteration 20800 (58.3642 iter/s, 1.71338s/100 iter), loss = 0.106801
I0628 19:03:14.185071  7628 solver.cpp:371]     Train net output #0: loss = 0.106801 (* 1 = 0.106801 loss)
I0628 19:03:14.185075  7628 sgd_solver.cpp:137] Iteration 20800, lr = 0.0675, m = 0.9
I0628 19:03:15.901823  7628 solver.cpp:349] Iteration 20900 (58.252 iter/s, 1.71668s/100 iter), loss = 0.0654122
I0628 19:03:15.901849  7628 solver.cpp:371]     Train net output #0: loss = 0.0654127 (* 1 = 0.0654127 loss)
I0628 19:03:15.901852  7628 sgd_solver.cpp:137] Iteration 20900, lr = 0.0673437, m = 0.9
I0628 19:03:17.609927  7628 solver.cpp:545] Iteration 21000, Testing net (#0)
I0628 19:03:18.617839  7626 data_reader.cpp:262] Starting prefetch of epoch 21
I0628 19:03:18.640863  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7992
I0628 19:03:18.640877  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.984601
I0628 19:03:18.640882  7628 solver.cpp:630]     Test net output #2: loss = 0.889261 (* 1 = 0.889261 loss)
I0628 19:03:18.640897  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03094s
I0628 19:03:18.658236  7628 solver.cpp:349] Iteration 21000 (36.2808 iter/s, 2.75628s/100 iter), loss = 0.135288
I0628 19:03:18.658259  7628 solver.cpp:371]     Train net output #0: loss = 0.135288 (* 1 = 0.135288 loss)
I0628 19:03:18.658264  7628 sgd_solver.cpp:137] Iteration 21000, lr = 0.0671875, m = 0.9
I0628 19:03:20.202744  7613 data_reader.cpp:262] Starting prefetch of epoch 27
I0628 19:03:20.374210  7628 solver.cpp:349] Iteration 21100 (58.2792 iter/s, 1.71588s/100 iter), loss = 0.0663834
I0628 19:03:20.374233  7628 solver.cpp:371]     Train net output #0: loss = 0.0663839 (* 1 = 0.0663839 loss)
I0628 19:03:20.374236  7628 sgd_solver.cpp:137] Iteration 21100, lr = 0.0670313, m = 0.9
I0628 19:03:22.101454  7628 solver.cpp:349] Iteration 21200 (57.8989 iter/s, 1.72715s/100 iter), loss = 0.159229
I0628 19:03:22.101476  7628 solver.cpp:371]     Train net output #0: loss = 0.15923 (* 1 = 0.15923 loss)
I0628 19:03:22.101480  7628 sgd_solver.cpp:137] Iteration 21200, lr = 0.066875, m = 0.9
I0628 19:03:23.818737  7628 solver.cpp:349] Iteration 21300 (58.2348 iter/s, 1.71719s/100 iter), loss = 0.224368
I0628 19:03:23.818758  7628 solver.cpp:371]     Train net output #0: loss = 0.224369 (* 1 = 0.224369 loss)
I0628 19:03:23.818778  7628 sgd_solver.cpp:137] Iteration 21300, lr = 0.0667187, m = 0.9
I0628 19:03:25.541247  7628 solver.cpp:349] Iteration 21400 (58.0586 iter/s, 1.7224s/100 iter), loss = 0.0988281
I0628 19:03:25.541270  7628 solver.cpp:371]     Train net output #0: loss = 0.0988286 (* 1 = 0.0988286 loss)
I0628 19:03:25.541275  7628 sgd_solver.cpp:137] Iteration 21400, lr = 0.0665625, m = 0.9
I0628 19:03:27.258735  7628 solver.cpp:349] Iteration 21500 (58.2278 iter/s, 1.71739s/100 iter), loss = 0.058926
I0628 19:03:27.258759  7628 solver.cpp:371]     Train net output #0: loss = 0.0589265 (* 1 = 0.0589265 loss)
I0628 19:03:27.258764  7628 sgd_solver.cpp:137] Iteration 21500, lr = 0.0664062, m = 0.9
I0628 19:03:28.976531  7628 solver.cpp:349] Iteration 21600 (58.2174 iter/s, 1.7177s/100 iter), loss = 0.12357
I0628 19:03:28.976553  7628 solver.cpp:371]     Train net output #0: loss = 0.12357 (* 1 = 0.12357 loss)
I0628 19:03:28.976559  7628 sgd_solver.cpp:137] Iteration 21600, lr = 0.06625, m = 0.9
I0628 19:03:30.693145  7628 solver.cpp:349] Iteration 21700 (58.2576 iter/s, 1.71652s/100 iter), loss = 0.105311
I0628 19:03:30.693167  7628 solver.cpp:371]     Train net output #0: loss = 0.105311 (* 1 = 0.105311 loss)
I0628 19:03:30.693172  7628 sgd_solver.cpp:137] Iteration 21700, lr = 0.0660938, m = 0.9
I0628 19:03:32.413148  7628 solver.cpp:349] Iteration 21800 (58.1427 iter/s, 1.71991s/100 iter), loss = 0.0887567
I0628 19:03:32.413172  7628 solver.cpp:371]     Train net output #0: loss = 0.0887572 (* 1 = 0.0887572 loss)
I0628 19:03:32.413177  7628 sgd_solver.cpp:137] Iteration 21800, lr = 0.0659375, m = 0.9
I0628 19:03:33.634526  7613 data_reader.cpp:262] Starting prefetch of epoch 28
I0628 19:03:34.131238  7628 solver.cpp:349] Iteration 21900 (58.2075 iter/s, 1.71799s/100 iter), loss = 0.0658916
I0628 19:03:34.131321  7628 solver.cpp:371]     Train net output #0: loss = 0.0658921 (* 1 = 0.0658921 loss)
I0628 19:03:34.131330  7628 sgd_solver.cpp:137] Iteration 21900, lr = 0.0657813, m = 0.9
I0628 19:03:35.835413  7628 solver.cpp:545] Iteration 22000, Testing net (#0)
I0628 19:03:36.845732  7626 data_reader.cpp:262] Starting prefetch of epoch 22
I0628 19:03:36.866143  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.842
I0628 19:03:36.866155  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9918
I0628 19:03:36.866160  7628 solver.cpp:630]     Test net output #2: loss = 0.557517 (* 1 = 0.557517 loss)
I0628 19:03:36.866173  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03073s
I0628 19:03:36.883497  7628 solver.cpp:349] Iteration 22000 (36.3364 iter/s, 2.75206s/100 iter), loss = 0.161584
I0628 19:03:36.883519  7628 solver.cpp:371]     Train net output #0: loss = 0.161585 (* 1 = 0.161585 loss)
I0628 19:03:36.883523  7628 sgd_solver.cpp:137] Iteration 22000, lr = 0.065625, m = 0.9
I0628 19:03:38.596156  7628 solver.cpp:349] Iteration 22100 (58.3919 iter/s, 1.71257s/100 iter), loss = 0.0650359
I0628 19:03:38.596179  7628 solver.cpp:371]     Train net output #0: loss = 0.0650365 (* 1 = 0.0650365 loss)
I0628 19:03:38.596182  7628 sgd_solver.cpp:137] Iteration 22100, lr = 0.0654688, m = 0.9
I0628 19:03:40.312649  7628 solver.cpp:349] Iteration 22200 (58.2615 iter/s, 1.7164s/100 iter), loss = 0.00898014
I0628 19:03:40.312671  7628 solver.cpp:371]     Train net output #0: loss = 0.0089807 (* 1 = 0.0089807 loss)
I0628 19:03:40.312675  7628 sgd_solver.cpp:137] Iteration 22200, lr = 0.0653125, m = 0.9
I0628 19:03:42.029618  7628 solver.cpp:349] Iteration 22300 (58.2454 iter/s, 1.71687s/100 iter), loss = 0.0655617
I0628 19:03:42.029639  7628 solver.cpp:371]     Train net output #0: loss = 0.0655624 (* 1 = 0.0655624 loss)
I0628 19:03:42.029644  7628 sgd_solver.cpp:137] Iteration 22300, lr = 0.0651563, m = 0.9
I0628 19:03:43.750936  7628 solver.cpp:349] Iteration 22400 (58.0982 iter/s, 1.72122s/100 iter), loss = 0.0694209
I0628 19:03:43.750958  7628 solver.cpp:371]     Train net output #0: loss = 0.0694215 (* 1 = 0.0694215 loss)
I0628 19:03:43.750962  7628 sgd_solver.cpp:137] Iteration 22400, lr = 0.065, m = 0.9
I0628 19:03:45.465617  7628 solver.cpp:349] Iteration 22500 (58.3231 iter/s, 1.71459s/100 iter), loss = 0.034521
I0628 19:03:45.465639  7628 solver.cpp:371]     Train net output #0: loss = 0.0345216 (* 1 = 0.0345216 loss)
I0628 19:03:45.465643  7628 sgd_solver.cpp:137] Iteration 22500, lr = 0.0648438, m = 0.9
I0628 19:03:47.183300  7628 solver.cpp:349] Iteration 22600 (58.2212 iter/s, 1.71759s/100 iter), loss = 0.174943
I0628 19:03:47.183322  7628 solver.cpp:371]     Train net output #0: loss = 0.174943 (* 1 = 0.174943 loss)
I0628 19:03:47.183326  7628 sgd_solver.cpp:137] Iteration 22600, lr = 0.0646875, m = 0.9
I0628 19:03:48.094523  7613 data_reader.cpp:262] Starting prefetch of epoch 29
I0628 19:03:48.900153  7628 solver.cpp:349] Iteration 22700 (58.2493 iter/s, 1.71676s/100 iter), loss = 0.0680287
I0628 19:03:48.900174  7628 solver.cpp:371]     Train net output #0: loss = 0.0680293 (* 1 = 0.0680293 loss)
I0628 19:03:48.900178  7628 sgd_solver.cpp:137] Iteration 22700, lr = 0.0645313, m = 0.9
I0628 19:03:50.617058  7628 solver.cpp:349] Iteration 22800 (58.2476 iter/s, 1.71681s/100 iter), loss = 0.0437461
I0628 19:03:50.617079  7628 solver.cpp:371]     Train net output #0: loss = 0.0437467 (* 1 = 0.0437467 loss)
I0628 19:03:50.617082  7628 sgd_solver.cpp:137] Iteration 22800, lr = 0.064375, m = 0.9
I0628 19:03:52.335393  7628 solver.cpp:349] Iteration 22900 (58.199 iter/s, 1.71824s/100 iter), loss = 0.0141577
I0628 19:03:52.335415  7628 solver.cpp:371]     Train net output #0: loss = 0.0141583 (* 1 = 0.0141583 loss)
I0628 19:03:52.335419  7628 sgd_solver.cpp:137] Iteration 22900, lr = 0.0642188, m = 0.9
I0628 19:03:54.036454  7628 solver.cpp:545] Iteration 23000, Testing net (#0)
I0628 19:03:55.043154  7626 data_reader.cpp:262] Starting prefetch of epoch 23
I0628 19:03:55.065836  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.798399
I0628 19:03:55.065865  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9886
I0628 19:03:55.065871  7628 solver.cpp:630]     Test net output #2: loss = 0.731195 (* 1 = 0.731195 loss)
I0628 19:03:55.065886  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.0294s
I0628 19:03:55.083269  7628 solver.cpp:349] Iteration 23000 (36.3934 iter/s, 2.74775s/100 iter), loss = 0.20092
I0628 19:03:55.083295  7628 solver.cpp:371]     Train net output #0: loss = 0.20092 (* 1 = 0.20092 loss)
I0628 19:03:55.083300  7628 sgd_solver.cpp:137] Iteration 23000, lr = 0.0640625, m = 0.9
I0628 19:03:56.802773  7628 solver.cpp:349] Iteration 23100 (58.1596 iter/s, 1.71941s/100 iter), loss = 0.179231
I0628 19:03:56.802798  7628 solver.cpp:371]     Train net output #0: loss = 0.179231 (* 1 = 0.179231 loss)
I0628 19:03:56.802801  7628 sgd_solver.cpp:137] Iteration 23100, lr = 0.0639063, m = 0.9
I0628 19:03:58.519323  7628 solver.cpp:349] Iteration 23200 (58.2597 iter/s, 1.71645s/100 iter), loss = 0.105
I0628 19:03:58.519345  7628 solver.cpp:371]     Train net output #0: loss = 0.105001 (* 1 = 0.105001 loss)
I0628 19:03:58.519348  7628 sgd_solver.cpp:137] Iteration 23200, lr = 0.06375, m = 0.9
I0628 19:04:00.240334  7628 solver.cpp:349] Iteration 23300 (58.1086 iter/s, 1.72092s/100 iter), loss = 0.114955
I0628 19:04:00.240356  7628 solver.cpp:371]     Train net output #0: loss = 0.114955 (* 1 = 0.114955 loss)
I0628 19:04:00.240360  7628 sgd_solver.cpp:137] Iteration 23300, lr = 0.0635938, m = 0.9
I0628 19:04:01.956389  7628 solver.cpp:349] Iteration 23400 (58.2764 iter/s, 1.71596s/100 iter), loss = 0.0340129
I0628 19:04:01.956411  7628 solver.cpp:371]     Train net output #0: loss = 0.0340133 (* 1 = 0.0340133 loss)
I0628 19:04:01.956415  7628 sgd_solver.cpp:137] Iteration 23400, lr = 0.0634375, m = 0.9
I0628 19:04:02.541893  7613 data_reader.cpp:262] Starting prefetch of epoch 30
I0628 19:04:03.691287  7628 solver.cpp:349] Iteration 23500 (57.6435 iter/s, 1.7348s/100 iter), loss = 0.124718
I0628 19:04:03.691310  7628 solver.cpp:371]     Train net output #0: loss = 0.124718 (* 1 = 0.124718 loss)
I0628 19:04:03.691314  7628 sgd_solver.cpp:137] Iteration 23500, lr = 0.0632813, m = 0.9
I0628 19:04:05.407145  7628 solver.cpp:349] Iteration 23600 (58.2831 iter/s, 1.71576s/100 iter), loss = 0.117434
I0628 19:04:05.407208  7628 solver.cpp:371]     Train net output #0: loss = 0.117434 (* 1 = 0.117434 loss)
I0628 19:04:05.407213  7628 sgd_solver.cpp:137] Iteration 23600, lr = 0.063125, m = 0.9
I0628 19:04:07.128844  7628 solver.cpp:349] Iteration 23700 (58.0868 iter/s, 1.72156s/100 iter), loss = 0.055336
I0628 19:04:07.128866  7628 solver.cpp:371]     Train net output #0: loss = 0.0553364 (* 1 = 0.0553364 loss)
I0628 19:04:07.128870  7628 sgd_solver.cpp:137] Iteration 23700, lr = 0.0629688, m = 0.9
I0628 19:04:08.843495  7628 solver.cpp:349] Iteration 23800 (58.3242 iter/s, 1.71455s/100 iter), loss = 0.065086
I0628 19:04:08.843518  7628 solver.cpp:371]     Train net output #0: loss = 0.0650865 (* 1 = 0.0650865 loss)
I0628 19:04:08.843521  7628 sgd_solver.cpp:137] Iteration 23800, lr = 0.0628125, m = 0.9
I0628 19:04:10.557212  7628 solver.cpp:349] Iteration 23900 (58.3559 iter/s, 1.71362s/100 iter), loss = 0.109873
I0628 19:04:10.557232  7628 solver.cpp:371]     Train net output #0: loss = 0.109874 (* 1 = 0.109874 loss)
I0628 19:04:10.557236  7628 sgd_solver.cpp:137] Iteration 23900, lr = 0.0626562, m = 0.9
I0628 19:04:12.254539  7628 solver.cpp:545] Iteration 24000, Testing net (#0)
I0628 19:04:13.260766  7626 data_reader.cpp:262] Starting prefetch of epoch 24
I0628 19:04:13.284538  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8488
I0628 19:04:13.284551  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9908
I0628 19:04:13.284556  7628 solver.cpp:630]     Test net output #2: loss = 0.566731 (* 1 = 0.566731 loss)
I0628 19:04:13.284569  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03s
I0628 19:04:13.301950  7628 solver.cpp:349] Iteration 24000 (36.435 iter/s, 2.74461s/100 iter), loss = 0.0357238
I0628 19:04:13.301973  7628 solver.cpp:371]     Train net output #0: loss = 0.0357243 (* 1 = 0.0357243 loss)
I0628 19:04:13.301976  7628 sgd_solver.cpp:137] Iteration 24000, lr = 0.0625, m = 0.9
I0628 19:04:15.019606  7628 solver.cpp:349] Iteration 24100 (58.2221 iter/s, 1.71756s/100 iter), loss = 0.12955
I0628 19:04:15.019629  7628 solver.cpp:371]     Train net output #0: loss = 0.129551 (* 1 = 0.129551 loss)
I0628 19:04:15.019635  7628 sgd_solver.cpp:137] Iteration 24100, lr = 0.0623438, m = 0.9
I0628 19:04:16.745823  7628 solver.cpp:349] Iteration 24200 (57.9334 iter/s, 1.72612s/100 iter), loss = 0.129574
I0628 19:04:16.745848  7628 solver.cpp:371]     Train net output #0: loss = 0.129575 (* 1 = 0.129575 loss)
I0628 19:04:16.745852  7628 sgd_solver.cpp:137] Iteration 24200, lr = 0.0621875, m = 0.9
I0628 19:04:17.004359  7613 data_reader.cpp:262] Starting prefetch of epoch 31
I0628 19:04:18.466428  7628 solver.cpp:349] Iteration 24300 (58.1225 iter/s, 1.72051s/100 iter), loss = 0.0603604
I0628 19:04:18.466451  7628 solver.cpp:371]     Train net output #0: loss = 0.0603609 (* 1 = 0.0603609 loss)
I0628 19:04:18.466457  7628 sgd_solver.cpp:137] Iteration 24300, lr = 0.0620313, m = 0.9
I0628 19:04:20.184626  7628 solver.cpp:349] Iteration 24400 (58.2038 iter/s, 1.7181s/100 iter), loss = 0.0671634
I0628 19:04:20.184648  7628 solver.cpp:371]     Train net output #0: loss = 0.0671639 (* 1 = 0.0671639 loss)
I0628 19:04:20.184653  7628 sgd_solver.cpp:137] Iteration 24400, lr = 0.061875, m = 0.9
I0628 19:04:21.901255  7628 solver.cpp:349] Iteration 24500 (58.2569 iter/s, 1.71654s/100 iter), loss = 0.108937
I0628 19:04:21.901279  7628 solver.cpp:371]     Train net output #0: loss = 0.108937 (* 1 = 0.108937 loss)
I0628 19:04:21.901284  7628 sgd_solver.cpp:137] Iteration 24500, lr = 0.0617188, m = 0.9
I0628 19:04:23.626744  7628 solver.cpp:349] Iteration 24600 (57.958 iter/s, 1.72539s/100 iter), loss = 0.149503
I0628 19:04:23.626767  7628 solver.cpp:371]     Train net output #0: loss = 0.149504 (* 1 = 0.149504 loss)
I0628 19:04:23.626772  7628 sgd_solver.cpp:137] Iteration 24600, lr = 0.0615625, m = 0.9
I0628 19:04:25.343310  7628 solver.cpp:349] Iteration 24700 (58.2592 iter/s, 1.71647s/100 iter), loss = 0.0177554
I0628 19:04:25.343333  7628 solver.cpp:371]     Train net output #0: loss = 0.0177559 (* 1 = 0.0177559 loss)
I0628 19:04:25.343355  7628 sgd_solver.cpp:137] Iteration 24700, lr = 0.0614063, m = 0.9
I0628 19:04:27.064278  7628 solver.cpp:349] Iteration 24800 (58.1111 iter/s, 1.72084s/100 iter), loss = 0.0426147
I0628 19:04:27.064301  7628 solver.cpp:371]     Train net output #0: loss = 0.0426152 (* 1 = 0.0426152 loss)
I0628 19:04:27.064307  7628 sgd_solver.cpp:137] Iteration 24800, lr = 0.06125, m = 0.9
I0628 19:04:28.783128  7628 solver.cpp:349] Iteration 24900 (58.1818 iter/s, 1.71875s/100 iter), loss = 0.023172
I0628 19:04:28.783152  7628 solver.cpp:371]     Train net output #0: loss = 0.0231725 (* 1 = 0.0231725 loss)
I0628 19:04:28.783157  7628 sgd_solver.cpp:137] Iteration 24900, lr = 0.0610937, m = 0.9
I0628 19:04:30.432338  7613 data_reader.cpp:262] Starting prefetch of epoch 32
I0628 19:04:30.483827  7628 solver.cpp:545] Iteration 25000, Testing net (#0)
I0628 19:04:31.490572  7626 data_reader.cpp:262] Starting prefetch of epoch 25
I0628 19:04:31.512877  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8398
I0628 19:04:31.512890  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9906
I0628 19:04:31.512895  7628 solver.cpp:630]     Test net output #2: loss = 0.611064 (* 1 = 0.611064 loss)
I0628 19:04:31.512909  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02905s
I0628 19:04:31.530408  7628 solver.cpp:349] Iteration 25000 (36.4014 iter/s, 2.74715s/100 iter), loss = 0.023916
I0628 19:04:31.530434  7628 solver.cpp:371]     Train net output #0: loss = 0.0239164 (* 1 = 0.0239164 loss)
I0628 19:04:31.530438  7628 sgd_solver.cpp:137] Iteration 25000, lr = 0.0609375, m = 0.9
I0628 19:04:33.248417  7628 solver.cpp:349] Iteration 25100 (58.2103 iter/s, 1.71791s/100 iter), loss = 0.021974
I0628 19:04:33.248440  7628 solver.cpp:371]     Train net output #0: loss = 0.0219744 (* 1 = 0.0219744 loss)
I0628 19:04:33.248445  7628 sgd_solver.cpp:137] Iteration 25100, lr = 0.0607813, m = 0.9
I0628 19:04:34.962828  7628 solver.cpp:349] Iteration 25200 (58.3323 iter/s, 1.71432s/100 iter), loss = 0.094193
I0628 19:04:34.962851  7628 solver.cpp:371]     Train net output #0: loss = 0.0941935 (* 1 = 0.0941935 loss)
I0628 19:04:34.962854  7628 sgd_solver.cpp:137] Iteration 25200, lr = 0.060625, m = 0.9
I0628 19:04:36.679065  7628 solver.cpp:349] Iteration 25300 (58.2702 iter/s, 1.71614s/100 iter), loss = 0.128046
I0628 19:04:36.679147  7628 solver.cpp:371]     Train net output #0: loss = 0.128046 (* 1 = 0.128046 loss)
I0628 19:04:36.679153  7628 sgd_solver.cpp:137] Iteration 25300, lr = 0.0604688, m = 0.9
I0628 19:04:38.394839  7628 solver.cpp:349] Iteration 25400 (58.2881 iter/s, 1.71561s/100 iter), loss = 0.0498717
I0628 19:04:38.394861  7628 solver.cpp:371]     Train net output #0: loss = 0.0498721 (* 1 = 0.0498721 loss)
I0628 19:04:38.394866  7628 sgd_solver.cpp:137] Iteration 25400, lr = 0.0603125, m = 0.9
I0628 19:04:40.113113  7628 solver.cpp:349] Iteration 25500 (58.2012 iter/s, 1.71818s/100 iter), loss = 0.0575084
I0628 19:04:40.113137  7628 solver.cpp:371]     Train net output #0: loss = 0.0575088 (* 1 = 0.0575088 loss)
I0628 19:04:40.113140  7628 sgd_solver.cpp:137] Iteration 25500, lr = 0.0601563, m = 0.9
I0628 19:04:41.831223  7628 solver.cpp:349] Iteration 25600 (58.2067 iter/s, 1.71801s/100 iter), loss = 0.00772723
I0628 19:04:41.831244  7628 solver.cpp:371]     Train net output #0: loss = 0.00772761 (* 1 = 0.00772761 loss)
I0628 19:04:41.831249  7628 sgd_solver.cpp:137] Iteration 25600, lr = 0.06, m = 0.9
I0628 19:04:43.546838  7628 solver.cpp:349] Iteration 25700 (58.2914 iter/s, 1.71552s/100 iter), loss = 0.0618837
I0628 19:04:43.546860  7628 solver.cpp:371]     Train net output #0: loss = 0.0618841 (* 1 = 0.0618841 loss)
I0628 19:04:43.546865  7628 sgd_solver.cpp:137] Iteration 25700, lr = 0.0598437, m = 0.9
I0628 19:04:44.890121  7613 data_reader.cpp:262] Starting prefetch of epoch 33
I0628 19:04:45.267797  7628 solver.cpp:349] Iteration 25800 (58.1104 iter/s, 1.72086s/100 iter), loss = 0.00967285
I0628 19:04:45.267822  7628 solver.cpp:371]     Train net output #0: loss = 0.0096733 (* 1 = 0.0096733 loss)
I0628 19:04:45.267825  7628 sgd_solver.cpp:137] Iteration 25800, lr = 0.0596875, m = 0.9
I0628 19:04:46.986641  7628 solver.cpp:349] Iteration 25900 (58.1819 iter/s, 1.71875s/100 iter), loss = 0.0729209
I0628 19:04:46.986665  7628 solver.cpp:371]     Train net output #0: loss = 0.0729214 (* 1 = 0.0729214 loss)
I0628 19:04:46.986670  7628 sgd_solver.cpp:137] Iteration 25900, lr = 0.0595312, m = 0.9
I0628 19:04:48.684849  7628 solver.cpp:545] Iteration 26000, Testing net (#0)
I0628 19:04:49.693657  7626 data_reader.cpp:262] Starting prefetch of epoch 26
I0628 19:04:49.714624  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7904
I0628 19:04:49.714635  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9882
I0628 19:04:49.714640  7628 solver.cpp:630]     Test net output #2: loss = 0.735148 (* 1 = 0.735148 loss)
I0628 19:04:49.714654  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02977s
I0628 19:04:49.731997  7628 solver.cpp:349] Iteration 26000 (36.4269 iter/s, 2.74523s/100 iter), loss = 0.0748499
I0628 19:04:49.732018  7628 solver.cpp:371]     Train net output #0: loss = 0.0748503 (* 1 = 0.0748503 loss)
I0628 19:04:49.732023  7628 sgd_solver.cpp:137] Iteration 26000, lr = 0.059375, m = 0.9
I0628 19:04:51.454314  7628 solver.cpp:349] Iteration 26100 (58.0645 iter/s, 1.72222s/100 iter), loss = 0.181765
I0628 19:04:51.454336  7628 solver.cpp:371]     Train net output #0: loss = 0.181766 (* 1 = 0.181766 loss)
I0628 19:04:51.454340  7628 sgd_solver.cpp:137] Iteration 26100, lr = 0.0592188, m = 0.9
I0628 19:04:53.168582  7628 solver.cpp:349] Iteration 26200 (58.3372 iter/s, 1.71417s/100 iter), loss = 0.15598
I0628 19:04:53.168604  7628 solver.cpp:371]     Train net output #0: loss = 0.155981 (* 1 = 0.155981 loss)
I0628 19:04:53.168608  7628 sgd_solver.cpp:137] Iteration 26200, lr = 0.0590625, m = 0.9
I0628 19:04:54.886709  7628 solver.cpp:349] Iteration 26300 (58.2061 iter/s, 1.71803s/100 iter), loss = 0.0386525
I0628 19:04:54.886731  7628 solver.cpp:371]     Train net output #0: loss = 0.0386529 (* 1 = 0.0386529 loss)
I0628 19:04:54.886735  7628 sgd_solver.cpp:137] Iteration 26300, lr = 0.0589063, m = 0.9
I0628 19:04:56.602882  7628 solver.cpp:349] Iteration 26400 (58.2724 iter/s, 1.71608s/100 iter), loss = 0.0649482
I0628 19:04:56.602902  7628 solver.cpp:371]     Train net output #0: loss = 0.0649486 (* 1 = 0.0649486 loss)
I0628 19:04:56.602921  7628 sgd_solver.cpp:137] Iteration 26400, lr = 0.05875, m = 0.9
I0628 19:04:58.324997  7628 solver.cpp:349] Iteration 26500 (58.0718 iter/s, 1.72201s/100 iter), loss = 0.136356
I0628 19:04:58.325021  7628 solver.cpp:371]     Train net output #0: loss = 0.136356 (* 1 = 0.136356 loss)
I0628 19:04:58.325026  7628 sgd_solver.cpp:137] Iteration 26500, lr = 0.0585938, m = 0.9
I0628 19:04:59.336755  7613 data_reader.cpp:262] Starting prefetch of epoch 34
I0628 19:05:00.041446  7628 solver.cpp:349] Iteration 26600 (58.2632 iter/s, 1.71635s/100 iter), loss = 0.0637655
I0628 19:05:00.041467  7628 solver.cpp:371]     Train net output #0: loss = 0.063766 (* 1 = 0.063766 loss)
I0628 19:05:00.041471  7628 sgd_solver.cpp:137] Iteration 26600, lr = 0.0584375, m = 0.9
I0628 19:05:01.756268  7628 solver.cpp:349] Iteration 26700 (58.3183 iter/s, 1.71473s/100 iter), loss = 0.264775
I0628 19:05:01.756289  7628 solver.cpp:371]     Train net output #0: loss = 0.264776 (* 1 = 0.264776 loss)
I0628 19:05:01.756294  7628 sgd_solver.cpp:137] Iteration 26700, lr = 0.0582813, m = 0.9
I0628 19:05:03.476562  7628 solver.cpp:349] Iteration 26800 (58.1328 iter/s, 1.7202s/100 iter), loss = 0.0658696
I0628 19:05:03.476584  7628 solver.cpp:371]     Train net output #0: loss = 0.0658701 (* 1 = 0.0658701 loss)
I0628 19:05:03.476588  7628 sgd_solver.cpp:137] Iteration 26800, lr = 0.058125, m = 0.9
I0628 19:05:05.196439  7628 solver.cpp:349] Iteration 26900 (58.1469 iter/s, 1.71978s/100 iter), loss = 0.09746
I0628 19:05:05.196461  7628 solver.cpp:371]     Train net output #0: loss = 0.0974605 (* 1 = 0.0974605 loss)
I0628 19:05:05.196465  7628 sgd_solver.cpp:137] Iteration 26900, lr = 0.0579687, m = 0.9
I0628 19:05:06.900190  7628 solver.cpp:545] Iteration 27000, Testing net (#0)
I0628 19:05:07.908499  7626 data_reader.cpp:262] Starting prefetch of epoch 27
I0628 19:05:07.928814  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8248
I0628 19:05:07.928829  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9942
I0628 19:05:07.928834  7628 solver.cpp:630]     Test net output #2: loss = 0.718437 (* 1 = 0.718437 loss)
I0628 19:05:07.928848  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02863s
I0628 19:05:07.946106  7628 solver.cpp:349] Iteration 27000 (36.3697 iter/s, 2.74954s/100 iter), loss = 0.034943
I0628 19:05:07.946130  7628 solver.cpp:371]     Train net output #0: loss = 0.0349435 (* 1 = 0.0349435 loss)
I0628 19:05:07.946133  7628 sgd_solver.cpp:137] Iteration 27000, lr = 0.0578125, m = 0.9
I0628 19:05:09.668051  7628 solver.cpp:349] Iteration 27100 (58.0771 iter/s, 1.72185s/100 iter), loss = 0.105791
I0628 19:05:09.668072  7628 solver.cpp:371]     Train net output #0: loss = 0.105791 (* 1 = 0.105791 loss)
I0628 19:05:09.668076  7628 sgd_solver.cpp:137] Iteration 27100, lr = 0.0576563, m = 0.9
I0628 19:05:11.387207  7628 solver.cpp:349] Iteration 27200 (58.1713 iter/s, 1.71906s/100 iter), loss = 0.0212828
I0628 19:05:11.387229  7628 solver.cpp:371]     Train net output #0: loss = 0.0212833 (* 1 = 0.0212833 loss)
I0628 19:05:11.387233  7628 sgd_solver.cpp:137] Iteration 27200, lr = 0.0575, m = 0.9
I0628 19:05:13.101455  7628 solver.cpp:349] Iteration 27300 (58.3378 iter/s, 1.71415s/100 iter), loss = 0.30651
I0628 19:05:13.101477  7628 solver.cpp:371]     Train net output #0: loss = 0.30651 (* 1 = 0.30651 loss)
I0628 19:05:13.101481  7628 sgd_solver.cpp:137] Iteration 27300, lr = 0.0573438, m = 0.9
I0628 19:05:13.786952  7613 data_reader.cpp:262] Starting prefetch of epoch 35
I0628 19:05:14.817136  7628 solver.cpp:349] Iteration 27400 (58.2892 iter/s, 1.71559s/100 iter), loss = 0.125281
I0628 19:05:14.817157  7628 solver.cpp:371]     Train net output #0: loss = 0.125282 (* 1 = 0.125282 loss)
I0628 19:05:14.817162  7628 sgd_solver.cpp:137] Iteration 27400, lr = 0.0571875, m = 0.9
I0628 19:05:16.541220  7628 solver.cpp:349] Iteration 27500 (58.0049 iter/s, 1.72399s/100 iter), loss = 0.0593361
I0628 19:05:16.541247  7628 solver.cpp:371]     Train net output #0: loss = 0.0593366 (* 1 = 0.0593366 loss)
I0628 19:05:16.541254  7628 sgd_solver.cpp:137] Iteration 27500, lr = 0.0570313, m = 0.9
I0628 19:05:18.261922  7628 solver.cpp:349] Iteration 27600 (58.1193 iter/s, 1.7206s/100 iter), loss = 0.0993841
I0628 19:05:18.261946  7628 solver.cpp:371]     Train net output #0: loss = 0.0993847 (* 1 = 0.0993847 loss)
I0628 19:05:18.261950  7628 sgd_solver.cpp:137] Iteration 27600, lr = 0.056875, m = 0.9
I0628 19:05:19.980873  7628 solver.cpp:349] Iteration 27700 (58.1783 iter/s, 1.71885s/100 iter), loss = 0.0192212
I0628 19:05:19.980893  7628 solver.cpp:371]     Train net output #0: loss = 0.0192218 (* 1 = 0.0192218 loss)
I0628 19:05:19.980897  7628 sgd_solver.cpp:137] Iteration 27700, lr = 0.0567187, m = 0.9
I0628 19:05:21.695430  7628 solver.cpp:349] Iteration 27800 (58.3273 iter/s, 1.71446s/100 iter), loss = 0.0273402
I0628 19:05:21.695451  7628 solver.cpp:371]     Train net output #0: loss = 0.0273407 (* 1 = 0.0273407 loss)
I0628 19:05:21.695454  7628 sgd_solver.cpp:137] Iteration 27800, lr = 0.0565625, m = 0.9
I0628 19:05:23.412189  7628 solver.cpp:349] Iteration 27900 (58.2525 iter/s, 1.71666s/100 iter), loss = 0.113072
I0628 19:05:23.412211  7628 solver.cpp:371]     Train net output #0: loss = 0.113073 (* 1 = 0.113073 loss)
I0628 19:05:23.412215  7628 sgd_solver.cpp:137] Iteration 27900, lr = 0.0564062, m = 0.9
I0628 19:05:25.107760  7628 solver.cpp:545] Iteration 28000, Testing net (#0)
I0628 19:05:26.114333  7626 data_reader.cpp:262] Starting prefetch of epoch 28
I0628 19:05:26.137614  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8264
I0628 19:05:26.137627  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.989401
I0628 19:05:26.137632  7628 solver.cpp:630]     Test net output #2: loss = 0.736059 (* 1 = 0.736059 loss)
I0628 19:05:26.137647  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02985s
I0628 19:05:26.154966  7628 solver.cpp:349] Iteration 28000 (36.4611 iter/s, 2.74265s/100 iter), loss = 0.0286982
I0628 19:05:26.154989  7628 solver.cpp:371]     Train net output #0: loss = 0.0286987 (* 1 = 0.0286987 loss)
I0628 19:05:26.154994  7628 sgd_solver.cpp:137] Iteration 28000, lr = 0.05625, m = 0.9
I0628 19:05:27.873250  7628 solver.cpp:349] Iteration 28100 (58.2008 iter/s, 1.71819s/100 iter), loss = 0.082039
I0628 19:05:27.873270  7628 solver.cpp:371]     Train net output #0: loss = 0.0820395 (* 1 = 0.0820395 loss)
I0628 19:05:27.873273  7628 sgd_solver.cpp:137] Iteration 28100, lr = 0.0560938, m = 0.9
I0628 19:05:28.233837  7613 data_reader.cpp:262] Starting prefetch of epoch 36
I0628 19:05:29.587970  7628 solver.cpp:349] Iteration 28200 (58.3217 iter/s, 1.71463s/100 iter), loss = 0.0510117
I0628 19:05:29.587991  7628 solver.cpp:371]     Train net output #0: loss = 0.0510122 (* 1 = 0.0510122 loss)
I0628 19:05:29.587996  7628 sgd_solver.cpp:137] Iteration 28200, lr = 0.0559375, m = 0.9
I0628 19:05:31.307284  7628 solver.cpp:349] Iteration 28300 (58.1659 iter/s, 1.71922s/100 iter), loss = 0.00825809
I0628 19:05:31.307307  7628 solver.cpp:371]     Train net output #0: loss = 0.00825857 (* 1 = 0.00825857 loss)
I0628 19:05:31.307310  7628 sgd_solver.cpp:137] Iteration 28300, lr = 0.0557813, m = 0.9
I0628 19:05:33.031756  7628 solver.cpp:349] Iteration 28400 (57.992 iter/s, 1.72438s/100 iter), loss = 0.138117
I0628 19:05:33.031776  7628 solver.cpp:371]     Train net output #0: loss = 0.138118 (* 1 = 0.138118 loss)
I0628 19:05:33.031780  7628 sgd_solver.cpp:137] Iteration 28400, lr = 0.055625, m = 0.9
I0628 19:05:34.752979  7628 solver.cpp:349] Iteration 28500 (58.1014 iter/s, 1.72113s/100 iter), loss = 0.038144
I0628 19:05:34.753000  7628 solver.cpp:371]     Train net output #0: loss = 0.0381445 (* 1 = 0.0381445 loss)
I0628 19:05:34.753005  7628 sgd_solver.cpp:137] Iteration 28500, lr = 0.0554687, m = 0.9
I0628 19:05:36.469840  7628 solver.cpp:349] Iteration 28600 (58.249 iter/s, 1.71677s/100 iter), loss = 0.123572
I0628 19:05:36.469862  7628 solver.cpp:371]     Train net output #0: loss = 0.123573 (* 1 = 0.123573 loss)
I0628 19:05:36.469866  7628 sgd_solver.cpp:137] Iteration 28600, lr = 0.0553125, m = 0.9
I0628 19:05:38.186184  7628 solver.cpp:349] Iteration 28700 (58.2666 iter/s, 1.71625s/100 iter), loss = 0.123993
I0628 19:05:38.186254  7628 solver.cpp:371]     Train net output #0: loss = 0.123994 (* 1 = 0.123994 loss)
I0628 19:05:38.186261  7628 sgd_solver.cpp:137] Iteration 28700, lr = 0.0551562, m = 0.9
I0628 19:05:39.900254  7628 solver.cpp:349] Iteration 28800 (58.3458 iter/s, 1.71392s/100 iter), loss = 0.0142047
I0628 19:05:39.900276  7628 solver.cpp:371]     Train net output #0: loss = 0.0142052 (* 1 = 0.0142052 loss)
I0628 19:05:39.900280  7628 sgd_solver.cpp:137] Iteration 28800, lr = 0.055, m = 0.9
I0628 19:05:41.614596  7628 solver.cpp:349] Iteration 28900 (58.3346 iter/s, 1.71425s/100 iter), loss = 0.0465973
I0628 19:05:41.614619  7628 solver.cpp:371]     Train net output #0: loss = 0.0465978 (* 1 = 0.0465978 loss)
I0628 19:05:41.614622  7628 sgd_solver.cpp:137] Iteration 28900, lr = 0.0548437, m = 0.9
I0628 19:05:41.669292  7613 data_reader.cpp:262] Starting prefetch of epoch 37
I0628 19:05:43.323330  7628 solver.cpp:545] Iteration 29000, Testing net (#0)
I0628 19:05:44.329296  7626 data_reader.cpp:262] Starting prefetch of epoch 29
I0628 19:05:44.351356  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8282
I0628 19:05:44.351369  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9864
I0628 19:05:44.351375  7628 solver.cpp:630]     Test net output #2: loss = 0.80059 (* 1 = 0.80059 loss)
I0628 19:05:44.351390  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02803s
I0628 19:05:44.368608  7628 solver.cpp:349] Iteration 29000 (36.3123 iter/s, 2.75389s/100 iter), loss = 0.171577
I0628 19:05:44.368630  7628 solver.cpp:371]     Train net output #0: loss = 0.171577 (* 1 = 0.171577 loss)
I0628 19:05:44.368634  7628 sgd_solver.cpp:137] Iteration 29000, lr = 0.0546875, m = 0.9
I0628 19:05:46.086408  7628 solver.cpp:349] Iteration 29100 (58.2172 iter/s, 1.7177s/100 iter), loss = 0.226602
I0628 19:05:46.086431  7628 solver.cpp:371]     Train net output #0: loss = 0.226603 (* 1 = 0.226603 loss)
I0628 19:05:46.086434  7628 sgd_solver.cpp:137] Iteration 29100, lr = 0.0545313, m = 0.9
I0628 19:05:47.803170  7628 solver.cpp:349] Iteration 29200 (58.2524 iter/s, 1.71667s/100 iter), loss = 0.0983857
I0628 19:05:47.803192  7628 solver.cpp:371]     Train net output #0: loss = 0.0983862 (* 1 = 0.0983862 loss)
I0628 19:05:47.803196  7628 sgd_solver.cpp:137] Iteration 29200, lr = 0.054375, m = 0.9
I0628 19:05:49.515540  7628 solver.cpp:349] Iteration 29300 (58.4018 iter/s, 1.71228s/100 iter), loss = 0.0454113
I0628 19:05:49.515563  7628 solver.cpp:371]     Train net output #0: loss = 0.0454118 (* 1 = 0.0454118 loss)
I0628 19:05:49.515568  7628 sgd_solver.cpp:137] Iteration 29300, lr = 0.0542188, m = 0.9
I0628 19:05:51.230937  7628 solver.cpp:349] Iteration 29400 (58.2989 iter/s, 1.7153s/100 iter), loss = 0.0166961
I0628 19:05:51.230958  7628 solver.cpp:371]     Train net output #0: loss = 0.0166966 (* 1 = 0.0166966 loss)
I0628 19:05:51.230962  7628 sgd_solver.cpp:137] Iteration 29400, lr = 0.0540625, m = 0.9
I0628 19:05:52.944142  7628 solver.cpp:349] Iteration 29500 (58.3733 iter/s, 1.71311s/100 iter), loss = 0.0827445
I0628 19:05:52.944164  7628 solver.cpp:371]     Train net output #0: loss = 0.082745 (* 1 = 0.082745 loss)
I0628 19:05:52.944167  7628 sgd_solver.cpp:137] Iteration 29500, lr = 0.0539063, m = 0.9
I0628 19:05:54.659499  7628 solver.cpp:349] Iteration 29600 (58.3 iter/s, 1.71526s/100 iter), loss = 0.0149628
I0628 19:05:54.659518  7628 solver.cpp:371]     Train net output #0: loss = 0.0149633 (* 1 = 0.0149633 loss)
I0628 19:05:54.659523  7628 sgd_solver.cpp:137] Iteration 29600, lr = 0.05375, m = 0.9
I0628 19:05:56.099551  7613 data_reader.cpp:262] Starting prefetch of epoch 38
I0628 19:05:56.374198  7628 solver.cpp:349] Iteration 29700 (58.3224 iter/s, 1.71461s/100 iter), loss = 0.0162122
I0628 19:05:56.374220  7628 solver.cpp:371]     Train net output #0: loss = 0.0162126 (* 1 = 0.0162126 loss)
I0628 19:05:56.374225  7628 sgd_solver.cpp:137] Iteration 29700, lr = 0.0535938, m = 0.9
I0628 19:05:58.091725  7628 solver.cpp:349] Iteration 29800 (58.2265 iter/s, 1.71743s/100 iter), loss = 0.15719
I0628 19:05:58.091747  7628 solver.cpp:371]     Train net output #0: loss = 0.157191 (* 1 = 0.157191 loss)
I0628 19:05:58.091766  7628 sgd_solver.cpp:137] Iteration 29800, lr = 0.0534375, m = 0.9
I0628 19:05:59.805541  7628 solver.cpp:349] Iteration 29900 (58.3532 iter/s, 1.7137s/100 iter), loss = 0.120593
I0628 19:05:59.805567  7628 solver.cpp:371]     Train net output #0: loss = 0.120593 (* 1 = 0.120593 loss)
I0628 19:05:59.805572  7628 sgd_solver.cpp:137] Iteration 29900, lr = 0.0532812, m = 0.9
I0628 19:06:01.502374  7628 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_30000.caffemodel
I0628 19:06:01.510514  7628 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_30000.solverstate
I0628 19:06:01.514026  7628 solver.cpp:545] Iteration 30000, Testing net (#0)
I0628 19:06:02.520715  7626 data_reader.cpp:262] Starting prefetch of epoch 30
I0628 19:06:02.540995  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.849
I0628 19:06:02.541007  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.989
I0628 19:06:02.541010  7628 solver.cpp:630]     Test net output #2: loss = 0.603788 (* 1 = 0.603788 loss)
I0628 19:06:02.541024  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02696s
I0628 19:06:02.558336  7628 solver.cpp:349] Iteration 30000 (36.3285 iter/s, 2.75266s/100 iter), loss = 0.0241424
I0628 19:06:02.558359  7628 solver.cpp:371]     Train net output #0: loss = 0.0241429 (* 1 = 0.0241429 loss)
I0628 19:06:02.558363  7628 sgd_solver.cpp:137] Iteration 30000, lr = 0.053125, m = 0.9
I0628 19:06:04.275207  7628 solver.cpp:349] Iteration 30100 (58.2488 iter/s, 1.71677s/100 iter), loss = 0.20854
I0628 19:06:04.275228  7628 solver.cpp:371]     Train net output #0: loss = 0.208541 (* 1 = 0.208541 loss)
I0628 19:06:04.275233  7628 sgd_solver.cpp:137] Iteration 30100, lr = 0.0529688, m = 0.9
I0628 19:06:05.990310  7628 solver.cpp:349] Iteration 30200 (58.3088 iter/s, 1.71501s/100 iter), loss = 0.18254
I0628 19:06:05.990331  7628 solver.cpp:371]     Train net output #0: loss = 0.18254 (* 1 = 0.18254 loss)
I0628 19:06:05.990335  7628 sgd_solver.cpp:137] Iteration 30200, lr = 0.0528125, m = 0.9
I0628 19:06:07.707015  7628 solver.cpp:349] Iteration 30300 (58.2544 iter/s, 1.71661s/100 iter), loss = 0.134757
I0628 19:06:07.707039  7628 solver.cpp:371]     Train net output #0: loss = 0.134757 (* 1 = 0.134757 loss)
I0628 19:06:07.707046  7628 sgd_solver.cpp:137] Iteration 30300, lr = 0.0526563, m = 0.9
I0628 19:06:09.426231  7628 solver.cpp:349] Iteration 30400 (58.1695 iter/s, 1.71911s/100 iter), loss = 0.113125
I0628 19:06:09.426301  7628 solver.cpp:371]     Train net output #0: loss = 0.113126 (* 1 = 0.113126 loss)
I0628 19:06:09.426306  7628 sgd_solver.cpp:137] Iteration 30400, lr = 0.0525, m = 0.9
I0628 19:06:10.544381  7613 data_reader.cpp:262] Starting prefetch of epoch 39
I0628 19:06:11.144446  7628 solver.cpp:349] Iteration 30500 (58.2049 iter/s, 1.71807s/100 iter), loss = 0.0324748
I0628 19:06:11.144470  7628 solver.cpp:371]     Train net output #0: loss = 0.0324752 (* 1 = 0.0324752 loss)
I0628 19:06:11.144474  7628 sgd_solver.cpp:137] Iteration 30500, lr = 0.0523438, m = 0.9
I0628 19:06:12.860878  7628 solver.cpp:349] Iteration 30600 (58.2637 iter/s, 1.71634s/100 iter), loss = 0.0132616
I0628 19:06:12.860899  7628 solver.cpp:371]     Train net output #0: loss = 0.013262 (* 1 = 0.013262 loss)
I0628 19:06:12.860903  7628 sgd_solver.cpp:137] Iteration 30600, lr = 0.0521875, m = 0.9
I0628 19:06:14.574206  7628 solver.cpp:349] Iteration 30700 (58.3691 iter/s, 1.71324s/100 iter), loss = 0.0107287
I0628 19:06:14.574229  7628 solver.cpp:371]     Train net output #0: loss = 0.010729 (* 1 = 0.010729 loss)
I0628 19:06:14.574234  7628 sgd_solver.cpp:137] Iteration 30700, lr = 0.0520312, m = 0.9
I0628 19:06:16.287752  7628 solver.cpp:349] Iteration 30800 (58.3618 iter/s, 1.71345s/100 iter), loss = 0.230793
I0628 19:06:16.287775  7628 solver.cpp:371]     Train net output #0: loss = 0.230793 (* 1 = 0.230793 loss)
I0628 19:06:16.287778  7628 sgd_solver.cpp:137] Iteration 30800, lr = 0.051875, m = 0.9
I0628 19:06:18.004813  7628 solver.cpp:349] Iteration 30900 (58.2423 iter/s, 1.71696s/100 iter), loss = 0.0709451
I0628 19:06:18.004835  7628 solver.cpp:371]     Train net output #0: loss = 0.0709454 (* 1 = 0.0709454 loss)
I0628 19:06:18.004839  7628 sgd_solver.cpp:137] Iteration 30900, lr = 0.0517187, m = 0.9
I0628 19:06:19.703270  7628 solver.cpp:545] Iteration 31000, Testing net (#0)
I0628 19:06:20.711752  7626 data_reader.cpp:262] Starting prefetch of epoch 31
I0628 19:06:20.736156  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.7916
I0628 19:06:20.736176  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9894
I0628 19:06:20.736181  7628 solver.cpp:630]     Test net output #2: loss = 0.856665 (* 1 = 0.856665 loss)
I0628 19:06:20.736196  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03289s
I0628 19:06:20.753470  7628 solver.cpp:349] Iteration 31000 (36.3831 iter/s, 2.74853s/100 iter), loss = 0.304491
I0628 19:06:20.753494  7628 solver.cpp:371]     Train net output #0: loss = 0.304491 (* 1 = 0.304491 loss)
I0628 19:06:20.753497  7628 sgd_solver.cpp:137] Iteration 31000, lr = 0.0515625, m = 0.9
I0628 19:06:22.470469  7628 solver.cpp:349] Iteration 31100 (58.2444 iter/s, 1.7169s/100 iter), loss = 0.121171
I0628 19:06:22.470489  7628 solver.cpp:371]     Train net output #0: loss = 0.121171 (* 1 = 0.121171 loss)
I0628 19:06:22.470494  7628 sgd_solver.cpp:137] Iteration 31100, lr = 0.0514063, m = 0.9
I0628 19:06:24.194416  7628 solver.cpp:349] Iteration 31200 (58.0095 iter/s, 1.72386s/100 iter), loss = 0.062766
I0628 19:06:24.194440  7628 solver.cpp:371]     Train net output #0: loss = 0.0627663 (* 1 = 0.0627663 loss)
I0628 19:06:24.194444  7628 sgd_solver.cpp:137] Iteration 31200, lr = 0.05125, m = 0.9
I0628 19:06:24.984730  7613 data_reader.cpp:262] Starting prefetch of epoch 40
I0628 19:06:25.917922  7628 solver.cpp:349] Iteration 31300 (58.0245 iter/s, 1.72341s/100 iter), loss = 0.110448
I0628 19:06:25.917943  7628 solver.cpp:371]     Train net output #0: loss = 0.110448 (* 1 = 0.110448 loss)
I0628 19:06:25.917948  7628 sgd_solver.cpp:137] Iteration 31300, lr = 0.0510938, m = 0.9
I0628 19:06:27.637514  7628 solver.cpp:349] Iteration 31400 (58.1565 iter/s, 1.7195s/100 iter), loss = 0.0764309
I0628 19:06:27.637537  7628 solver.cpp:371]     Train net output #0: loss = 0.0764313 (* 1 = 0.0764313 loss)
I0628 19:06:27.637540  7628 sgd_solver.cpp:137] Iteration 31400, lr = 0.0509375, m = 0.9
I0628 19:06:29.357252  7628 solver.cpp:349] Iteration 31500 (58.1516 iter/s, 1.71964s/100 iter), loss = 0.0140733
I0628 19:06:29.357275  7628 solver.cpp:371]     Train net output #0: loss = 0.0140736 (* 1 = 0.0140736 loss)
I0628 19:06:29.357292  7628 sgd_solver.cpp:137] Iteration 31500, lr = 0.0507812, m = 0.9
I0628 19:06:31.073936  7628 solver.cpp:349] Iteration 31600 (58.2556 iter/s, 1.71657s/100 iter), loss = 0.0125675
I0628 19:06:31.073959  7628 solver.cpp:371]     Train net output #0: loss = 0.0125678 (* 1 = 0.0125678 loss)
I0628 19:06:31.073964  7628 sgd_solver.cpp:137] Iteration 31600, lr = 0.050625, m = 0.9
I0628 19:06:32.788538  7628 solver.cpp:349] Iteration 31700 (58.3259 iter/s, 1.7145s/100 iter), loss = 0.0115482
I0628 19:06:32.788560  7628 solver.cpp:371]     Train net output #0: loss = 0.0115486 (* 1 = 0.0115486 loss)
I0628 19:06:32.788566  7628 sgd_solver.cpp:137] Iteration 31700, lr = 0.0504688, m = 0.9
I0628 19:06:34.507336  7628 solver.cpp:349] Iteration 31800 (58.1835 iter/s, 1.7187s/100 iter), loss = 0.0473853
I0628 19:06:34.507361  7628 solver.cpp:371]     Train net output #0: loss = 0.0473857 (* 1 = 0.0473857 loss)
I0628 19:06:34.507366  7628 sgd_solver.cpp:137] Iteration 31800, lr = 0.0503125, m = 0.9
I0628 19:06:36.223901  7628 solver.cpp:349] Iteration 31900 (58.2593 iter/s, 1.71646s/100 iter), loss = 0.0374394
I0628 19:06:36.223924  7628 solver.cpp:371]     Train net output #0: loss = 0.0374398 (* 1 = 0.0374398 loss)
I0628 19:06:36.223929  7628 sgd_solver.cpp:137] Iteration 31900, lr = 0.0501562, m = 0.9
I0628 19:06:37.928499  7628 solver.cpp:545] Iteration 32000, Testing net (#0)
I0628 19:06:38.933234  7626 data_reader.cpp:262] Starting prefetch of epoch 32
I0628 19:06:38.957514  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8364
I0628 19:06:38.957526  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9906
I0628 19:06:38.957532  7628 solver.cpp:630]     Test net output #2: loss = 0.636487 (* 1 = 0.636487 loss)
I0628 19:06:38.957554  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02902s
I0628 19:06:38.974835  7628 solver.cpp:349] Iteration 32000 (36.353 iter/s, 2.7508s/100 iter), loss = 0.0228742
I0628 19:06:38.974858  7628 solver.cpp:371]     Train net output #0: loss = 0.0228745 (* 1 = 0.0228745 loss)
I0628 19:06:38.974861  7628 sgd_solver.cpp:137] Iteration 32000, lr = 0.05, m = 0.9
I0628 19:06:39.454885  7613 data_reader.cpp:262] Starting prefetch of epoch 41
I0628 19:06:40.693675  7628 solver.cpp:349] Iteration 32100 (58.1821 iter/s, 1.71874s/100 iter), loss = 0.00624635
I0628 19:06:40.693698  7628 solver.cpp:371]     Train net output #0: loss = 0.00624673 (* 1 = 0.00624673 loss)
I0628 19:06:40.693704  7628 sgd_solver.cpp:137] Iteration 32100, lr = 0.0498438, m = 0.9
I0628 19:06:42.409294  7628 solver.cpp:349] Iteration 32200 (58.2914 iter/s, 1.71552s/100 iter), loss = 0.00632464
I0628 19:06:42.409317  7628 solver.cpp:371]     Train net output #0: loss = 0.00632502 (* 1 = 0.00632502 loss)
I0628 19:06:42.409323  7628 sgd_solver.cpp:137] Iteration 32200, lr = 0.0496875, m = 0.9
I0628 19:06:44.127518  7628 solver.cpp:349] Iteration 32300 (58.203 iter/s, 1.71812s/100 iter), loss = 0.205381
I0628 19:06:44.127542  7628 solver.cpp:371]     Train net output #0: loss = 0.205381 (* 1 = 0.205381 loss)
I0628 19:06:44.127547  7628 sgd_solver.cpp:137] Iteration 32300, lr = 0.0495313, m = 0.9
I0628 19:06:45.841627  7628 solver.cpp:349] Iteration 32400 (58.3427 iter/s, 1.71401s/100 iter), loss = 0.0250125
I0628 19:06:45.841650  7628 solver.cpp:371]     Train net output #0: loss = 0.0250129 (* 1 = 0.0250129 loss)
I0628 19:06:45.841653  7628 sgd_solver.cpp:137] Iteration 32400, lr = 0.049375, m = 0.9
I0628 19:06:47.560745  7628 solver.cpp:349] Iteration 32500 (58.1725 iter/s, 1.71902s/100 iter), loss = 0.00547277
I0628 19:06:47.560768  7628 solver.cpp:371]     Train net output #0: loss = 0.00547318 (* 1 = 0.00547318 loss)
I0628 19:06:47.560772  7628 sgd_solver.cpp:137] Iteration 32500, lr = 0.0492188, m = 0.9
I0628 19:06:49.281523  7628 solver.cpp:349] Iteration 32600 (58.1165 iter/s, 1.72068s/100 iter), loss = 0.230489
I0628 19:06:49.281545  7628 solver.cpp:371]     Train net output #0: loss = 0.230489 (* 1 = 0.230489 loss)
I0628 19:06:49.281553  7628 sgd_solver.cpp:137] Iteration 32600, lr = 0.0490625, m = 0.9
I0628 19:06:50.998386  7628 solver.cpp:349] Iteration 32700 (58.2491 iter/s, 1.71676s/100 iter), loss = 0.0659411
I0628 19:06:50.998409  7628 solver.cpp:371]     Train net output #0: loss = 0.0659415 (* 1 = 0.0659415 loss)
I0628 19:06:50.998412  7628 sgd_solver.cpp:137] Iteration 32700, lr = 0.0489062, m = 0.9
I0628 19:06:52.713414  7628 solver.cpp:349] Iteration 32800 (58.3113 iter/s, 1.71493s/100 iter), loss = 0.0237529
I0628 19:06:52.713434  7628 solver.cpp:371]     Train net output #0: loss = 0.0237533 (* 1 = 0.0237533 loss)
I0628 19:06:52.713438  7628 sgd_solver.cpp:137] Iteration 32800, lr = 0.04875, m = 0.9
I0628 19:06:52.868574  7613 data_reader.cpp:262] Starting prefetch of epoch 42
I0628 19:06:54.427783  7628 solver.cpp:349] Iteration 32900 (58.3337 iter/s, 1.71427s/100 iter), loss = 0.0654004
I0628 19:06:54.427803  7628 solver.cpp:371]     Train net output #0: loss = 0.0654008 (* 1 = 0.0654008 loss)
I0628 19:06:54.427809  7628 sgd_solver.cpp:137] Iteration 32900, lr = 0.0485937, m = 0.9
I0628 19:06:56.136521  7628 solver.cpp:545] Iteration 33000, Testing net (#0)
I0628 19:06:57.143769  7626 data_reader.cpp:262] Starting prefetch of epoch 33
I0628 19:06:57.166792  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8522
I0628 19:06:57.166805  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9924
I0628 19:06:57.166811  7628 solver.cpp:630]     Test net output #2: loss = 0.561435 (* 1 = 0.561435 loss)
I0628 19:06:57.166826  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03027s
I0628 19:06:57.184093  7628 solver.cpp:349] Iteration 33000 (36.2821 iter/s, 2.75618s/100 iter), loss = 0.00449223
I0628 19:06:57.184118  7628 solver.cpp:371]     Train net output #0: loss = 0.0044926 (* 1 = 0.0044926 loss)
I0628 19:06:57.184123  7628 sgd_solver.cpp:137] Iteration 33000, lr = 0.0484375, m = 0.9
I0628 19:06:58.897359  7628 solver.cpp:349] Iteration 33100 (58.3714 iter/s, 1.71317s/100 iter), loss = 0.0134461
I0628 19:06:58.897382  7628 solver.cpp:371]     Train net output #0: loss = 0.0134464 (* 1 = 0.0134464 loss)
I0628 19:06:58.897385  7628 sgd_solver.cpp:137] Iteration 33100, lr = 0.0482813, m = 0.9
I0628 19:07:00.618787  7628 solver.cpp:349] Iteration 33200 (58.0946 iter/s, 1.72133s/100 iter), loss = 0.0356514
I0628 19:07:00.618824  7628 solver.cpp:371]     Train net output #0: loss = 0.0356518 (* 1 = 0.0356518 loss)
I0628 19:07:00.618830  7628 sgd_solver.cpp:137] Iteration 33200, lr = 0.048125, m = 0.9
I0628 19:07:02.337651  7628 solver.cpp:349] Iteration 33300 (58.1819 iter/s, 1.71875s/100 iter), loss = 0.0281387
I0628 19:07:02.337673  7628 solver.cpp:371]     Train net output #0: loss = 0.0281391 (* 1 = 0.0281391 loss)
I0628 19:07:02.337677  7628 sgd_solver.cpp:137] Iteration 33300, lr = 0.0479688, m = 0.9
I0628 19:07:04.053452  7628 solver.cpp:349] Iteration 33400 (58.2851 iter/s, 1.71571s/100 iter), loss = 0.0164205
I0628 19:07:04.053473  7628 solver.cpp:371]     Train net output #0: loss = 0.0164208 (* 1 = 0.0164208 loss)
I0628 19:07:04.053479  7628 sgd_solver.cpp:137] Iteration 33400, lr = 0.0478125, m = 0.9
I0628 19:07:05.771042  7628 solver.cpp:349] Iteration 33500 (58.2244 iter/s, 1.71749s/100 iter), loss = 0.0269668
I0628 19:07:05.771064  7628 solver.cpp:371]     Train net output #0: loss = 0.026967 (* 1 = 0.026967 loss)
I0628 19:07:05.771067  7628 sgd_solver.cpp:137] Iteration 33500, lr = 0.0476562, m = 0.9
I0628 19:07:07.313874  7613 data_reader.cpp:262] Starting prefetch of epoch 43
I0628 19:07:07.485524  7628 solver.cpp:349] Iteration 33600 (58.3299 iter/s, 1.71439s/100 iter), loss = 0.0663488
I0628 19:07:07.485553  7628 solver.cpp:371]     Train net output #0: loss = 0.0663491 (* 1 = 0.0663491 loss)
I0628 19:07:07.485559  7628 sgd_solver.cpp:137] Iteration 33600, lr = 0.0475, m = 0.9
I0628 19:07:09.202986  7628 solver.cpp:349] Iteration 33700 (58.2291 iter/s, 1.71736s/100 iter), loss = 0.0171326
I0628 19:07:09.203008  7628 solver.cpp:371]     Train net output #0: loss = 0.0171329 (* 1 = 0.0171329 loss)
I0628 19:07:09.203012  7628 sgd_solver.cpp:137] Iteration 33700, lr = 0.0473437, m = 0.9
I0628 19:07:10.918723  7628 solver.cpp:349] Iteration 33800 (58.2872 iter/s, 1.71564s/100 iter), loss = 0.100063
I0628 19:07:10.918793  7628 solver.cpp:371]     Train net output #0: loss = 0.100063 (* 1 = 0.100063 loss)
I0628 19:07:10.918799  7628 sgd_solver.cpp:137] Iteration 33800, lr = 0.0471875, m = 0.9
I0628 19:07:12.639168  7628 solver.cpp:349] Iteration 33900 (58.1294 iter/s, 1.7203s/100 iter), loss = 0.0367861
I0628 19:07:12.639191  7628 solver.cpp:371]     Train net output #0: loss = 0.0367865 (* 1 = 0.0367865 loss)
I0628 19:07:12.639195  7628 sgd_solver.cpp:137] Iteration 33900, lr = 0.0470312, m = 0.9
I0628 19:07:14.337692  7628 solver.cpp:545] Iteration 34000, Testing net (#0)
I0628 19:07:15.345922  7626 data_reader.cpp:262] Starting prefetch of epoch 34
I0628 19:07:15.366288  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.848
I0628 19:07:15.366302  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9926
I0628 19:07:15.366307  7628 solver.cpp:630]     Test net output #2: loss = 0.559289 (* 1 = 0.559289 loss)
I0628 19:07:15.366319  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.0286s
I0628 19:07:15.383575  7628 solver.cpp:349] Iteration 34000 (36.4395 iter/s, 2.74428s/100 iter), loss = 0.0220245
I0628 19:07:15.383599  7628 solver.cpp:371]     Train net output #0: loss = 0.0220248 (* 1 = 0.0220248 loss)
I0628 19:07:15.383602  7628 sgd_solver.cpp:137] Iteration 34000, lr = 0.046875, m = 0.9
I0628 19:07:17.102461  7628 solver.cpp:349] Iteration 34100 (58.1804 iter/s, 1.71879s/100 iter), loss = 0.0262743
I0628 19:07:17.102483  7628 solver.cpp:371]     Train net output #0: loss = 0.0262746 (* 1 = 0.0262746 loss)
I0628 19:07:17.102486  7628 sgd_solver.cpp:137] Iteration 34100, lr = 0.0467188, m = 0.9
I0628 19:07:18.824784  7628 solver.cpp:349] Iteration 34200 (58.0644 iter/s, 1.72223s/100 iter), loss = 0.120657
I0628 19:07:18.824805  7628 solver.cpp:371]     Train net output #0: loss = 0.120657 (* 1 = 0.120657 loss)
I0628 19:07:18.824808  7628 sgd_solver.cpp:137] Iteration 34200, lr = 0.0465625, m = 0.9
I0628 19:07:20.542635  7628 solver.cpp:349] Iteration 34300 (58.2154 iter/s, 1.71776s/100 iter), loss = 0.0190968
I0628 19:07:20.542659  7628 solver.cpp:371]     Train net output #0: loss = 0.0190971 (* 1 = 0.0190971 loss)
I0628 19:07:20.542662  7628 sgd_solver.cpp:137] Iteration 34300, lr = 0.0464063, m = 0.9
I0628 19:07:21.759445  7613 data_reader.cpp:262] Starting prefetch of epoch 44
I0628 19:07:22.256207  7628 solver.cpp:349] Iteration 34400 (58.3609 iter/s, 1.71348s/100 iter), loss = 0.196539
I0628 19:07:22.256229  7628 solver.cpp:371]     Train net output #0: loss = 0.19654 (* 1 = 0.19654 loss)
I0628 19:07:22.256233  7628 sgd_solver.cpp:137] Iteration 34400, lr = 0.04625, m = 0.9
I0628 19:07:23.974836  7628 solver.cpp:349] Iteration 34500 (58.1892 iter/s, 1.71853s/100 iter), loss = 0.0528431
I0628 19:07:23.974859  7628 solver.cpp:371]     Train net output #0: loss = 0.0528435 (* 1 = 0.0528435 loss)
I0628 19:07:23.974864  7628 sgd_solver.cpp:137] Iteration 34500, lr = 0.0460938, m = 0.9
I0628 19:07:25.696321  7628 solver.cpp:349] Iteration 34600 (58.0927 iter/s, 1.72139s/100 iter), loss = 0.0482805
I0628 19:07:25.696343  7628 solver.cpp:371]     Train net output #0: loss = 0.0482809 (* 1 = 0.0482809 loss)
I0628 19:07:25.696348  7628 sgd_solver.cpp:137] Iteration 34600, lr = 0.0459375, m = 0.9
I0628 19:07:27.412932  7628 solver.cpp:349] Iteration 34700 (58.2576 iter/s, 1.71651s/100 iter), loss = 0.127694
I0628 19:07:27.412955  7628 solver.cpp:371]     Train net output #0: loss = 0.127694 (* 1 = 0.127694 loss)
I0628 19:07:27.412958  7628 sgd_solver.cpp:137] Iteration 34700, lr = 0.0457813, m = 0.9
I0628 19:07:29.144345  7628 solver.cpp:349] Iteration 34800 (57.7595 iter/s, 1.73132s/100 iter), loss = 0.0281769
I0628 19:07:29.144366  7628 solver.cpp:371]     Train net output #0: loss = 0.0281772 (* 1 = 0.0281772 loss)
I0628 19:07:29.144369  7628 sgd_solver.cpp:137] Iteration 34800, lr = 0.045625, m = 0.9
I0628 19:07:30.859596  7628 solver.cpp:349] Iteration 34900 (58.3037 iter/s, 1.71516s/100 iter), loss = 0.0892715
I0628 19:07:30.859619  7628 solver.cpp:371]     Train net output #0: loss = 0.0892718 (* 1 = 0.0892718 loss)
I0628 19:07:30.859638  7628 sgd_solver.cpp:137] Iteration 34900, lr = 0.0454687, m = 0.9
I0628 19:07:32.560883  7628 solver.cpp:545] Iteration 35000, Testing net (#0)
I0628 19:07:33.567853  7626 data_reader.cpp:262] Starting prefetch of epoch 35
I0628 19:07:33.589843  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8538
I0628 19:07:33.589855  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9936
I0628 19:07:33.589860  7628 solver.cpp:630]     Test net output #2: loss = 0.536602 (* 1 = 0.536602 loss)
I0628 19:07:33.589874  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02896s
I0628 19:07:33.607177  7628 solver.cpp:349] Iteration 35000 (36.3978 iter/s, 2.74742s/100 iter), loss = 0.0187084
I0628 19:07:33.607200  7628 solver.cpp:371]     Train net output #0: loss = 0.0187087 (* 1 = 0.0187087 loss)
I0628 19:07:33.607203  7628 sgd_solver.cpp:137] Iteration 35000, lr = 0.0453125, m = 0.9
I0628 19:07:35.322659  7628 solver.cpp:349] Iteration 35100 (58.2959 iter/s, 1.71539s/100 iter), loss = 0.110716
I0628 19:07:35.322682  7628 solver.cpp:371]     Train net output #0: loss = 0.110716 (* 1 = 0.110716 loss)
I0628 19:07:35.322688  7628 sgd_solver.cpp:137] Iteration 35100, lr = 0.0451563, m = 0.9
I0628 19:07:36.231232  7613 data_reader.cpp:262] Starting prefetch of epoch 45
I0628 19:07:37.036922  7628 solver.cpp:349] Iteration 35200 (58.3375 iter/s, 1.71416s/100 iter), loss = 0.0618912
I0628 19:07:37.036944  7628 solver.cpp:371]     Train net output #0: loss = 0.0618915 (* 1 = 0.0618915 loss)
I0628 19:07:37.036950  7628 sgd_solver.cpp:137] Iteration 35200, lr = 0.045, m = 0.9
I0628 19:07:38.753038  7628 solver.cpp:349] Iteration 35300 (58.2745 iter/s, 1.71602s/100 iter), loss = 0.0607481
I0628 19:07:38.753067  7628 solver.cpp:371]     Train net output #0: loss = 0.0607484 (* 1 = 0.0607484 loss)
I0628 19:07:38.753072  7628 sgd_solver.cpp:137] Iteration 35300, lr = 0.0448438, m = 0.9
I0628 19:07:40.471261  7628 solver.cpp:349] Iteration 35400 (58.2032 iter/s, 1.71812s/100 iter), loss = 0.00911259
I0628 19:07:40.471284  7628 solver.cpp:371]     Train net output #0: loss = 0.00911287 (* 1 = 0.00911287 loss)
I0628 19:07:40.471290  7628 sgd_solver.cpp:137] Iteration 35400, lr = 0.0446875, m = 0.9
I0628 19:07:42.190755  7628 solver.cpp:349] Iteration 35500 (58.1599 iter/s, 1.7194s/100 iter), loss = 0.0329522
I0628 19:07:42.190811  7628 solver.cpp:371]     Train net output #0: loss = 0.0329525 (* 1 = 0.0329525 loss)
I0628 19:07:42.190817  7628 sgd_solver.cpp:137] Iteration 35500, lr = 0.0445313, m = 0.9
I0628 19:07:43.909140  7628 solver.cpp:349] Iteration 35600 (58.1986 iter/s, 1.71825s/100 iter), loss = 0.0115108
I0628 19:07:43.909164  7628 solver.cpp:371]     Train net output #0: loss = 0.0115111 (* 1 = 0.0115111 loss)
I0628 19:07:43.909168  7628 sgd_solver.cpp:137] Iteration 35600, lr = 0.044375, m = 0.9
I0628 19:07:45.626662  7628 solver.cpp:349] Iteration 35700 (58.2267 iter/s, 1.71742s/100 iter), loss = 0.00237727
I0628 19:07:45.626684  7628 solver.cpp:371]     Train net output #0: loss = 0.00237758 (* 1 = 0.00237758 loss)
I0628 19:07:45.626688  7628 sgd_solver.cpp:137] Iteration 35700, lr = 0.0442187, m = 0.9
I0628 19:07:47.340960  7628 solver.cpp:349] Iteration 35800 (58.3361 iter/s, 1.7142s/100 iter), loss = 0.0121522
I0628 19:07:47.340981  7628 solver.cpp:371]     Train net output #0: loss = 0.0121525 (* 1 = 0.0121525 loss)
I0628 19:07:47.340986  7628 sgd_solver.cpp:137] Iteration 35800, lr = 0.0440625, m = 0.9
I0628 19:07:49.055169  7628 solver.cpp:349] Iteration 35900 (58.3392 iter/s, 1.71411s/100 iter), loss = 0.00802933
I0628 19:07:49.055194  7628 solver.cpp:371]     Train net output #0: loss = 0.00802965 (* 1 = 0.00802965 loss)
I0628 19:07:49.055200  7628 sgd_solver.cpp:137] Iteration 35900, lr = 0.0439062, m = 0.9
I0628 19:07:49.638314  7613 data_reader.cpp:262] Starting prefetch of epoch 46
I0628 19:07:50.753692  7628 solver.cpp:545] Iteration 36000, Testing net (#0)
I0628 19:07:51.761230  7626 data_reader.cpp:262] Starting prefetch of epoch 36
I0628 19:07:51.781632  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8596
I0628 19:07:51.781647  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9906
I0628 19:07:51.781654  7628 solver.cpp:630]     Test net output #2: loss = 0.537938 (* 1 = 0.537938 loss)
I0628 19:07:51.781671  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02794s
I0628 19:07:51.798995  7628 solver.cpp:349] Iteration 36000 (36.4472 iter/s, 2.74369s/100 iter), loss = 0.0272031
I0628 19:07:51.799018  7628 solver.cpp:371]     Train net output #0: loss = 0.0272035 (* 1 = 0.0272035 loss)
I0628 19:07:51.799023  7628 sgd_solver.cpp:137] Iteration 36000, lr = 0.04375, m = 0.9
I0628 19:07:53.514238  7628 solver.cpp:349] Iteration 36100 (58.3041 iter/s, 1.71515s/100 iter), loss = 0.0089812
I0628 19:07:53.514261  7628 solver.cpp:371]     Train net output #0: loss = 0.00898156 (* 1 = 0.00898156 loss)
I0628 19:07:53.514266  7628 sgd_solver.cpp:137] Iteration 36100, lr = 0.0435938, m = 0.9
I0628 19:07:55.232810  7628 solver.cpp:349] Iteration 36200 (58.1912 iter/s, 1.71847s/100 iter), loss = 0.0155364
I0628 19:07:55.232832  7628 solver.cpp:371]     Train net output #0: loss = 0.0155368 (* 1 = 0.0155368 loss)
I0628 19:07:55.232836  7628 sgd_solver.cpp:137] Iteration 36200, lr = 0.0434375, m = 0.9
I0628 19:07:56.952695  7628 solver.cpp:349] Iteration 36300 (58.1467 iter/s, 1.71979s/100 iter), loss = 0.0181644
I0628 19:07:56.952718  7628 solver.cpp:371]     Train net output #0: loss = 0.0181647 (* 1 = 0.0181647 loss)
I0628 19:07:56.952724  7628 sgd_solver.cpp:137] Iteration 36300, lr = 0.0432813, m = 0.9
I0628 19:07:58.667616  7628 solver.cpp:349] Iteration 36400 (58.3152 iter/s, 1.71482s/100 iter), loss = 0.0159306
I0628 19:07:58.667639  7628 solver.cpp:371]     Train net output #0: loss = 0.015931 (* 1 = 0.015931 loss)
I0628 19:07:58.667644  7628 sgd_solver.cpp:137] Iteration 36400, lr = 0.043125, m = 0.9
I0628 19:08:00.381820  7628 solver.cpp:349] Iteration 36500 (58.3394 iter/s, 1.71411s/100 iter), loss = 0.0107752
I0628 19:08:00.381842  7628 solver.cpp:371]     Train net output #0: loss = 0.0107755 (* 1 = 0.0107755 loss)
I0628 19:08:00.381846  7628 sgd_solver.cpp:137] Iteration 36500, lr = 0.0429688, m = 0.9
I0628 19:08:02.102459  7628 solver.cpp:349] Iteration 36600 (58.1211 iter/s, 1.72054s/100 iter), loss = 0.024237
I0628 19:08:02.102483  7628 solver.cpp:371]     Train net output #0: loss = 0.0242374 (* 1 = 0.0242374 loss)
I0628 19:08:02.102501  7628 sgd_solver.cpp:137] Iteration 36600, lr = 0.0428125, m = 0.9
I0628 19:08:03.816993  7628 solver.cpp:349] Iteration 36700 (58.3287 iter/s, 1.71442s/100 iter), loss = 0.0201448
I0628 19:08:03.817014  7628 solver.cpp:371]     Train net output #0: loss = 0.0201452 (* 1 = 0.0201452 loss)
I0628 19:08:03.817018  7628 sgd_solver.cpp:137] Iteration 36700, lr = 0.0426563, m = 0.9
I0628 19:08:04.073990  7613 data_reader.cpp:262] Starting prefetch of epoch 47
I0628 19:08:05.531886  7628 solver.cpp:349] Iteration 36800 (58.3159 iter/s, 1.7148s/100 iter), loss = 0.00588903
I0628 19:08:05.531909  7628 solver.cpp:371]     Train net output #0: loss = 0.00588941 (* 1 = 0.00588941 loss)
I0628 19:08:05.531914  7628 sgd_solver.cpp:137] Iteration 36800, lr = 0.0425, m = 0.9
I0628 19:08:07.246667  7628 solver.cpp:349] Iteration 36900 (58.3198 iter/s, 1.71468s/100 iter), loss = 0.101347
I0628 19:08:07.246690  7628 solver.cpp:371]     Train net output #0: loss = 0.101348 (* 1 = 0.101348 loss)
I0628 19:08:07.246695  7628 sgd_solver.cpp:137] Iteration 36900, lr = 0.0423437, m = 0.9
I0628 19:08:08.946420  7628 solver.cpp:545] Iteration 37000, Testing net (#0)
I0628 19:08:09.952678  7626 data_reader.cpp:262] Starting prefetch of epoch 37
I0628 19:08:09.975914  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8732
I0628 19:08:09.975927  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:08:09.975935  7628 solver.cpp:630]     Test net output #2: loss = 0.46482 (* 1 = 0.46482 loss)
I0628 19:08:09.975951  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.0295s
I0628 19:08:09.993208  7628 solver.cpp:349] Iteration 37000 (36.4111 iter/s, 2.74641s/100 iter), loss = 0.00667003
I0628 19:08:09.993232  7628 solver.cpp:371]     Train net output #0: loss = 0.00667039 (* 1 = 0.00667039 loss)
I0628 19:08:09.993235  7628 sgd_solver.cpp:137] Iteration 37000, lr = 0.0421875, m = 0.9
I0628 19:08:11.708642  7628 solver.cpp:349] Iteration 37100 (58.2976 iter/s, 1.71534s/100 iter), loss = 0.00462772
I0628 19:08:11.708664  7628 solver.cpp:371]     Train net output #0: loss = 0.00462809 (* 1 = 0.00462809 loss)
I0628 19:08:11.708669  7628 sgd_solver.cpp:137] Iteration 37100, lr = 0.0420313, m = 0.9
I0628 19:08:13.427738  7628 solver.cpp:349] Iteration 37200 (58.1734 iter/s, 1.719s/100 iter), loss = 0.0112082
I0628 19:08:13.427810  7628 solver.cpp:371]     Train net output #0: loss = 0.0112086 (* 1 = 0.0112086 loss)
I0628 19:08:13.427819  7628 sgd_solver.cpp:137] Iteration 37200, lr = 0.041875, m = 0.9
I0628 19:08:15.148217  7628 solver.cpp:349] Iteration 37300 (58.1288 iter/s, 1.72032s/100 iter), loss = 0.0235273
I0628 19:08:15.148241  7628 solver.cpp:371]     Train net output #0: loss = 0.0235276 (* 1 = 0.0235276 loss)
I0628 19:08:15.148247  7628 sgd_solver.cpp:137] Iteration 37300, lr = 0.0417188, m = 0.9
I0628 19:08:16.867226  7628 solver.cpp:349] Iteration 37400 (58.1764 iter/s, 1.71891s/100 iter), loss = 0.0162059
I0628 19:08:16.867249  7628 solver.cpp:371]     Train net output #0: loss = 0.0162063 (* 1 = 0.0162063 loss)
I0628 19:08:16.867254  7628 sgd_solver.cpp:137] Iteration 37400, lr = 0.0415625, m = 0.9
I0628 19:08:18.514746  7613 data_reader.cpp:262] Starting prefetch of epoch 48
I0628 19:08:18.583156  7628 solver.cpp:349] Iteration 37500 (58.2807 iter/s, 1.71583s/100 iter), loss = 0.0309206
I0628 19:08:18.583181  7628 solver.cpp:371]     Train net output #0: loss = 0.030921 (* 1 = 0.030921 loss)
I0628 19:08:18.583186  7628 sgd_solver.cpp:137] Iteration 37500, lr = 0.0414063, m = 0.9
I0628 19:08:20.304745  7628 solver.cpp:349] Iteration 37600 (58.0892 iter/s, 1.72149s/100 iter), loss = 0.0701489
I0628 19:08:20.304769  7628 solver.cpp:371]     Train net output #0: loss = 0.0701492 (* 1 = 0.0701492 loss)
I0628 19:08:20.304774  7628 sgd_solver.cpp:137] Iteration 37600, lr = 0.04125, m = 0.9
I0628 19:08:22.018965  7628 solver.cpp:349] Iteration 37700 (58.3389 iter/s, 1.71412s/100 iter), loss = 0.0114137
I0628 19:08:22.018987  7628 solver.cpp:371]     Train net output #0: loss = 0.0114141 (* 1 = 0.0114141 loss)
I0628 19:08:22.018993  7628 sgd_solver.cpp:137] Iteration 37700, lr = 0.0410937, m = 0.9
I0628 19:08:23.733577  7628 solver.cpp:349] Iteration 37800 (58.3256 iter/s, 1.71451s/100 iter), loss = 0.012169
I0628 19:08:23.733600  7628 solver.cpp:371]     Train net output #0: loss = 0.0121694 (* 1 = 0.0121694 loss)
I0628 19:08:23.733605  7628 sgd_solver.cpp:137] Iteration 37800, lr = 0.0409375, m = 0.9
I0628 19:08:25.450923  7628 solver.cpp:349] Iteration 37900 (58.2327 iter/s, 1.71725s/100 iter), loss = 0.0327604
I0628 19:08:25.450947  7628 solver.cpp:371]     Train net output #0: loss = 0.0327607 (* 1 = 0.0327607 loss)
I0628 19:08:25.450953  7628 sgd_solver.cpp:137] Iteration 37900, lr = 0.0407812, m = 0.9
I0628 19:08:27.155746  7628 solver.cpp:545] Iteration 38000, Testing net (#0)
I0628 19:08:28.163497  7626 data_reader.cpp:262] Starting prefetch of epoch 38
I0628 19:08:28.183831  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.849599
I0628 19:08:28.183845  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.991
I0628 19:08:28.183851  7628 solver.cpp:630]     Test net output #2: loss = 0.612599 (* 1 = 0.612599 loss)
I0628 19:08:28.183867  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02809s
I0628 19:08:28.201110  7628 solver.cpp:349] Iteration 38000 (36.3629 iter/s, 2.75005s/100 iter), loss = 0.0171528
I0628 19:08:28.201133  7628 solver.cpp:371]     Train net output #0: loss = 0.0171532 (* 1 = 0.0171532 loss)
I0628 19:08:28.201138  7628 sgd_solver.cpp:137] Iteration 38000, lr = 0.040625, m = 0.9
I0628 19:08:29.918480  7628 solver.cpp:349] Iteration 38100 (58.2319 iter/s, 1.71727s/100 iter), loss = 0.00206838
I0628 19:08:29.918501  7628 solver.cpp:371]     Train net output #0: loss = 0.00206876 (* 1 = 0.00206876 loss)
I0628 19:08:29.918505  7628 sgd_solver.cpp:137] Iteration 38100, lr = 0.0404688, m = 0.9
I0628 19:08:31.632891  7628 solver.cpp:349] Iteration 38200 (58.3323 iter/s, 1.71432s/100 iter), loss = 0.00361431
I0628 19:08:31.632913  7628 solver.cpp:371]     Train net output #0: loss = 0.00361472 (* 1 = 0.00361472 loss)
I0628 19:08:31.632918  7628 sgd_solver.cpp:137] Iteration 38200, lr = 0.0403125, m = 0.9
I0628 19:08:32.975740  7613 data_reader.cpp:262] Starting prefetch of epoch 49
I0628 19:08:33.352918  7628 solver.cpp:349] Iteration 38300 (58.1419 iter/s, 1.71993s/100 iter), loss = 0.00566228
I0628 19:08:33.352957  7628 solver.cpp:371]     Train net output #0: loss = 0.00566269 (* 1 = 0.00566269 loss)
I0628 19:08:33.352962  7628 sgd_solver.cpp:137] Iteration 38300, lr = 0.0401563, m = 0.9
I0628 19:08:35.084727  7628 solver.cpp:349] Iteration 38400 (57.7469 iter/s, 1.73169s/100 iter), loss = 0.00626966
I0628 19:08:35.084748  7628 solver.cpp:371]     Train net output #0: loss = 0.00627008 (* 1 = 0.00627008 loss)
I0628 19:08:35.084753  7628 sgd_solver.cpp:137] Iteration 38400, lr = 0.04, m = 0.9
I0628 19:08:36.797818  7628 solver.cpp:349] Iteration 38500 (58.3772 iter/s, 1.713s/100 iter), loss = 0.0153058
I0628 19:08:36.797842  7628 solver.cpp:371]     Train net output #0: loss = 0.0153063 (* 1 = 0.0153063 loss)
I0628 19:08:36.797845  7628 sgd_solver.cpp:137] Iteration 38500, lr = 0.0398437, m = 0.9
I0628 19:08:38.519796  7628 solver.cpp:349] Iteration 38600 (58.0759 iter/s, 1.72188s/100 iter), loss = 0.0286314
I0628 19:08:38.519820  7628 solver.cpp:371]     Train net output #0: loss = 0.0286318 (* 1 = 0.0286318 loss)
I0628 19:08:38.519826  7628 sgd_solver.cpp:137] Iteration 38600, lr = 0.0396875, m = 0.9
I0628 19:08:40.238936  7628 solver.cpp:349] Iteration 38700 (58.172 iter/s, 1.71904s/100 iter), loss = 0.0444973
I0628 19:08:40.238958  7628 solver.cpp:371]     Train net output #0: loss = 0.0444977 (* 1 = 0.0444977 loss)
I0628 19:08:40.238962  7628 sgd_solver.cpp:137] Iteration 38700, lr = 0.0395312, m = 0.9
I0628 19:08:41.958065  7628 solver.cpp:349] Iteration 38800 (58.1722 iter/s, 1.71903s/100 iter), loss = 0.00743446
I0628 19:08:41.958086  7628 solver.cpp:371]     Train net output #0: loss = 0.00743485 (* 1 = 0.00743485 loss)
I0628 19:08:41.958089  7628 sgd_solver.cpp:137] Iteration 38800, lr = 0.039375, m = 0.9
I0628 19:08:43.679538  7628 solver.cpp:349] Iteration 38900 (58.0929 iter/s, 1.72138s/100 iter), loss = 0.0333561
I0628 19:08:43.679596  7628 solver.cpp:371]     Train net output #0: loss = 0.0333565 (* 1 = 0.0333565 loss)
I0628 19:08:43.679602  7628 sgd_solver.cpp:137] Iteration 38900, lr = 0.0392187, m = 0.9
I0628 19:08:45.380416  7628 solver.cpp:545] Iteration 39000, Testing net (#0)
I0628 19:08:46.386996  7626 data_reader.cpp:262] Starting prefetch of epoch 39
I0628 19:08:46.412103  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8528
I0628 19:08:46.412117  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9922
I0628 19:08:46.412122  7628 solver.cpp:630]     Test net output #2: loss = 0.576209 (* 1 = 0.576209 loss)
I0628 19:08:46.412137  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03169s
I0628 19:08:46.429533  7628 solver.cpp:349] Iteration 39000 (36.3659 iter/s, 2.74983s/100 iter), loss = 0.00407377
I0628 19:08:46.429560  7628 solver.cpp:371]     Train net output #0: loss = 0.00407415 (* 1 = 0.00407415 loss)
I0628 19:08:46.429564  7628 sgd_solver.cpp:137] Iteration 39000, lr = 0.0390625, m = 0.9
I0628 19:08:47.440009  7613 data_reader.cpp:262] Starting prefetch of epoch 50
I0628 19:08:48.144347  7628 solver.cpp:349] Iteration 39100 (58.3188 iter/s, 1.71471s/100 iter), loss = 0.0473808
I0628 19:08:48.144369  7628 solver.cpp:371]     Train net output #0: loss = 0.0473812 (* 1 = 0.0473812 loss)
I0628 19:08:48.144374  7628 sgd_solver.cpp:137] Iteration 39100, lr = 0.0389063, m = 0.9
I0628 19:08:49.861182  7628 solver.cpp:349] Iteration 39200 (58.2499 iter/s, 1.71674s/100 iter), loss = 0.00407912
I0628 19:08:49.861203  7628 solver.cpp:371]     Train net output #0: loss = 0.00407951 (* 1 = 0.00407951 loss)
I0628 19:08:49.861207  7628 sgd_solver.cpp:137] Iteration 39200, lr = 0.03875, m = 0.9
I0628 19:08:51.579151  7628 solver.cpp:349] Iteration 39300 (58.2115 iter/s, 1.71787s/100 iter), loss = 0.00519119
I0628 19:08:51.579174  7628 solver.cpp:371]     Train net output #0: loss = 0.00519157 (* 1 = 0.00519157 loss)
I0628 19:08:51.579179  7628 sgd_solver.cpp:137] Iteration 39300, lr = 0.0385938, m = 0.9
I0628 19:08:53.299473  7628 solver.cpp:349] Iteration 39400 (58.1319 iter/s, 1.72022s/100 iter), loss = 0.0811738
I0628 19:08:53.299494  7628 solver.cpp:371]     Train net output #0: loss = 0.0811741 (* 1 = 0.0811741 loss)
I0628 19:08:53.299499  7628 sgd_solver.cpp:137] Iteration 39400, lr = 0.0384375, m = 0.9
I0628 19:08:55.016705  7628 solver.cpp:349] Iteration 39500 (58.2365 iter/s, 1.71714s/100 iter), loss = 0.0086869
I0628 19:08:55.016726  7628 solver.cpp:371]     Train net output #0: loss = 0.00868726 (* 1 = 0.00868726 loss)
I0628 19:08:55.016731  7628 sgd_solver.cpp:137] Iteration 39500, lr = 0.0382813, m = 0.9
I0628 19:08:56.735754  7628 solver.cpp:349] Iteration 39600 (58.1749 iter/s, 1.71895s/100 iter), loss = 0.00970481
I0628 19:08:56.735780  7628 solver.cpp:371]     Train net output #0: loss = 0.0097052 (* 1 = 0.0097052 loss)
I0628 19:08:56.735785  7628 sgd_solver.cpp:137] Iteration 39600, lr = 0.038125, m = 0.9
I0628 19:08:58.452507  7628 solver.cpp:349] Iteration 39700 (58.253 iter/s, 1.71665s/100 iter), loss = 0.00522529
I0628 19:08:58.452530  7628 solver.cpp:371]     Train net output #0: loss = 0.00522569 (* 1 = 0.00522569 loss)
I0628 19:08:58.452534  7628 sgd_solver.cpp:137] Iteration 39700, lr = 0.0379688, m = 0.9
I0628 19:09:00.170562  7628 solver.cpp:349] Iteration 39800 (58.2087 iter/s, 1.71796s/100 iter), loss = 0.01124
I0628 19:09:00.170588  7628 solver.cpp:371]     Train net output #0: loss = 0.0112404 (* 1 = 0.0112404 loss)
I0628 19:09:00.170593  7628 sgd_solver.cpp:137] Iteration 39800, lr = 0.0378125, m = 0.9
I0628 19:09:00.859266  7613 data_reader.cpp:262] Starting prefetch of epoch 51
I0628 19:09:01.888404  7628 solver.cpp:349] Iteration 39900 (58.2159 iter/s, 1.71774s/100 iter), loss = 0.0106123
I0628 19:09:01.888427  7628 solver.cpp:371]     Train net output #0: loss = 0.0106127 (* 1 = 0.0106127 loss)
I0628 19:09:01.888430  7628 sgd_solver.cpp:137] Iteration 39900, lr = 0.0376562, m = 0.9
I0628 19:09:03.590788  7628 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_40000.caffemodel
I0628 19:09:03.598634  7628 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_40000.solverstate
I0628 19:09:03.602140  7628 solver.cpp:545] Iteration 40000, Testing net (#0)
I0628 19:09:04.607772  7626 data_reader.cpp:262] Starting prefetch of epoch 40
I0628 19:09:04.628270  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8754
I0628 19:09:04.628288  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9966
I0628 19:09:04.628293  7628 solver.cpp:630]     Test net output #2: loss = 0.496466 (* 1 = 0.496466 loss)
I0628 19:09:04.628307  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02613s
I0628 19:09:04.645763  7628 solver.cpp:349] Iteration 40000 (36.2683 iter/s, 2.75723s/100 iter), loss = 0.0346598
I0628 19:09:04.645784  7628 solver.cpp:371]     Train net output #0: loss = 0.0346602 (* 1 = 0.0346602 loss)
I0628 19:09:04.645788  7628 sgd_solver.cpp:137] Iteration 40000, lr = 0.0375, m = 0.9
I0628 19:09:06.363258  7628 solver.cpp:349] Iteration 40100 (58.2275 iter/s, 1.7174s/100 iter), loss = 0.0196048
I0628 19:09:06.363281  7628 solver.cpp:371]     Train net output #0: loss = 0.0196052 (* 1 = 0.0196052 loss)
I0628 19:09:06.363286  7628 sgd_solver.cpp:137] Iteration 40100, lr = 0.0373438, m = 0.9
I0628 19:09:08.083108  7628 solver.cpp:349] Iteration 40200 (58.148 iter/s, 1.71975s/100 iter), loss = 0.113466
I0628 19:09:08.083132  7628 solver.cpp:371]     Train net output #0: loss = 0.113467 (* 1 = 0.113467 loss)
I0628 19:09:08.083138  7628 sgd_solver.cpp:137] Iteration 40200, lr = 0.0371875, m = 0.9
I0628 19:09:09.805130  7628 solver.cpp:349] Iteration 40300 (58.0747 iter/s, 1.72192s/100 iter), loss = 0.0101451
I0628 19:09:09.805153  7628 solver.cpp:371]     Train net output #0: loss = 0.0101455 (* 1 = 0.0101455 loss)
I0628 19:09:09.805158  7628 sgd_solver.cpp:137] Iteration 40300, lr = 0.0370313, m = 0.9
I0628 19:09:11.524960  7628 solver.cpp:349] Iteration 40400 (58.1485 iter/s, 1.71973s/100 iter), loss = 0.0108949
I0628 19:09:11.524984  7628 solver.cpp:371]     Train net output #0: loss = 0.0108954 (* 1 = 0.0108954 loss)
I0628 19:09:11.524988  7628 sgd_solver.cpp:137] Iteration 40400, lr = 0.036875, m = 0.9
I0628 19:09:13.242022  7628 solver.cpp:349] Iteration 40500 (58.2424 iter/s, 1.71696s/100 iter), loss = 0.0207278
I0628 19:09:13.242044  7628 solver.cpp:371]     Train net output #0: loss = 0.0207282 (* 1 = 0.0207282 loss)
I0628 19:09:13.242048  7628 sgd_solver.cpp:137] Iteration 40500, lr = 0.0367188, m = 0.9
I0628 19:09:14.959985  7628 solver.cpp:349] Iteration 40600 (58.2117 iter/s, 1.71787s/100 iter), loss = 0.0356346
I0628 19:09:14.960050  7628 solver.cpp:371]     Train net output #0: loss = 0.035635 (* 1 = 0.035635 loss)
I0628 19:09:14.960055  7628 sgd_solver.cpp:137] Iteration 40600, lr = 0.0365625, m = 0.9
I0628 19:09:15.320765  7613 data_reader.cpp:262] Starting prefetch of epoch 52
I0628 19:09:16.676318  7628 solver.cpp:349] Iteration 40700 (58.2685 iter/s, 1.71619s/100 iter), loss = 0.00262606
I0628 19:09:16.676343  7628 solver.cpp:371]     Train net output #0: loss = 0.00262653 (* 1 = 0.00262653 loss)
I0628 19:09:16.676347  7628 sgd_solver.cpp:137] Iteration 40700, lr = 0.0364062, m = 0.9
I0628 19:09:18.395689  7628 solver.cpp:349] Iteration 40800 (58.1641 iter/s, 1.71927s/100 iter), loss = 0.0360126
I0628 19:09:18.395714  7628 solver.cpp:371]     Train net output #0: loss = 0.0360131 (* 1 = 0.0360131 loss)
I0628 19:09:18.395717  7628 sgd_solver.cpp:137] Iteration 40800, lr = 0.03625, m = 0.9
I0628 19:09:20.112407  7628 solver.cpp:349] Iteration 40900 (58.254 iter/s, 1.71662s/100 iter), loss = 0.100507
I0628 19:09:20.112431  7628 solver.cpp:371]     Train net output #0: loss = 0.100508 (* 1 = 0.100508 loss)
I0628 19:09:20.112435  7628 sgd_solver.cpp:137] Iteration 40900, lr = 0.0360937, m = 0.9
I0628 19:09:21.810118  7628 solver.cpp:545] Iteration 41000, Testing net (#0)
I0628 19:09:22.815233  7626 data_reader.cpp:262] Starting prefetch of epoch 41
I0628 19:09:22.838134  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8754
I0628 19:09:22.838146  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9928
I0628 19:09:22.838151  7628 solver.cpp:630]     Test net output #2: loss = 0.52067 (* 1 = 0.52067 loss)
I0628 19:09:22.838165  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02801s
I0628 19:09:22.855538  7628 solver.cpp:349] Iteration 41000 (36.4564 iter/s, 2.743s/100 iter), loss = 0.0448198
I0628 19:09:22.855566  7628 solver.cpp:371]     Train net output #0: loss = 0.0448203 (* 1 = 0.0448203 loss)
I0628 19:09:22.855571  7628 sgd_solver.cpp:137] Iteration 41000, lr = 0.0359375, m = 0.9
I0628 19:09:24.573632  7628 solver.cpp:349] Iteration 41100 (58.2075 iter/s, 1.71799s/100 iter), loss = 0.0312866
I0628 19:09:24.573654  7628 solver.cpp:371]     Train net output #0: loss = 0.031287 (* 1 = 0.031287 loss)
I0628 19:09:24.573659  7628 sgd_solver.cpp:137] Iteration 41100, lr = 0.0357813, m = 0.9
I0628 19:09:26.292304  7628 solver.cpp:349] Iteration 41200 (58.1877 iter/s, 1.71857s/100 iter), loss = 0.0465467
I0628 19:09:26.292323  7628 solver.cpp:371]     Train net output #0: loss = 0.0465472 (* 1 = 0.0465472 loss)
I0628 19:09:26.292327  7628 sgd_solver.cpp:137] Iteration 41200, lr = 0.035625, m = 0.9
I0628 19:09:28.007899  7628 solver.cpp:349] Iteration 41300 (58.2919 iter/s, 1.7155s/100 iter), loss = 0.00172372
I0628 19:09:28.007922  7628 solver.cpp:371]     Train net output #0: loss = 0.0017242 (* 1 = 0.0017242 loss)
I0628 19:09:28.007926  7628 sgd_solver.cpp:137] Iteration 41300, lr = 0.0354688, m = 0.9
I0628 19:09:29.724270  7628 solver.cpp:349] Iteration 41400 (58.2657 iter/s, 1.71628s/100 iter), loss = 0.00651548
I0628 19:09:29.724293  7628 solver.cpp:371]     Train net output #0: loss = 0.00651595 (* 1 = 0.00651595 loss)
I0628 19:09:29.724298  7628 sgd_solver.cpp:137] Iteration 41400, lr = 0.0353125, m = 0.9
I0628 19:09:29.775957  7613 data_reader.cpp:262] Starting prefetch of epoch 53
I0628 19:09:31.441108  7628 solver.cpp:349] Iteration 41500 (58.2554 iter/s, 1.71658s/100 iter), loss = 0.0578199
I0628 19:09:31.441131  7628 solver.cpp:371]     Train net output #0: loss = 0.0578204 (* 1 = 0.0578204 loss)
I0628 19:09:31.441136  7628 sgd_solver.cpp:137] Iteration 41500, lr = 0.0351562, m = 0.9
I0628 19:09:33.162967  7628 solver.cpp:349] Iteration 41600 (58.087 iter/s, 1.72156s/100 iter), loss = 0.0221522
I0628 19:09:33.162992  7628 solver.cpp:371]     Train net output #0: loss = 0.0221526 (* 1 = 0.0221526 loss)
I0628 19:09:33.162997  7628 sgd_solver.cpp:137] Iteration 41600, lr = 0.035, m = 0.9
I0628 19:09:34.880414  7628 solver.cpp:349] Iteration 41700 (58.2364 iter/s, 1.71714s/100 iter), loss = 0.0124277
I0628 19:09:34.880447  7628 solver.cpp:371]     Train net output #0: loss = 0.0124281 (* 1 = 0.0124281 loss)
I0628 19:09:34.880452  7628 sgd_solver.cpp:137] Iteration 41700, lr = 0.0348438, m = 0.9
I0628 19:09:36.598331  7628 solver.cpp:349] Iteration 41800 (58.2208 iter/s, 1.7176s/100 iter), loss = 0.00652132
I0628 19:09:36.598354  7628 solver.cpp:371]     Train net output #0: loss = 0.00652179 (* 1 = 0.00652179 loss)
I0628 19:09:36.598358  7628 sgd_solver.cpp:137] Iteration 41800, lr = 0.0346875, m = 0.9
I0628 19:09:38.321244  7628 solver.cpp:349] Iteration 41900 (58.0515 iter/s, 1.72261s/100 iter), loss = 0.0147461
I0628 19:09:38.321266  7628 solver.cpp:371]     Train net output #0: loss = 0.0147465 (* 1 = 0.0147465 loss)
I0628 19:09:38.321270  7628 sgd_solver.cpp:137] Iteration 41900, lr = 0.0345312, m = 0.9
I0628 19:09:40.021109  7628 solver.cpp:545] Iteration 42000, Testing net (#0)
I0628 19:09:41.027163  7626 data_reader.cpp:262] Starting prefetch of epoch 42
I0628 19:09:41.049722  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8882
I0628 19:09:41.049736  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:09:41.049741  7628 solver.cpp:630]     Test net output #2: loss = 0.461238 (* 1 = 0.461238 loss)
I0628 19:09:41.049754  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02849s
I0628 19:09:41.066990  7628 solver.cpp:349] Iteration 42000 (36.4261 iter/s, 2.74529s/100 iter), loss = 0.00966658
I0628 19:09:41.067013  7628 solver.cpp:371]     Train net output #0: loss = 0.00966707 (* 1 = 0.00966707 loss)
I0628 19:09:41.067016  7628 sgd_solver.cpp:137] Iteration 42000, lr = 0.034375, m = 0.9
I0628 19:09:42.780144  7628 solver.cpp:349] Iteration 42100 (58.3822 iter/s, 1.71285s/100 iter), loss = 0.00243018
I0628 19:09:42.780166  7628 solver.cpp:371]     Train net output #0: loss = 0.00243066 (* 1 = 0.00243066 loss)
I0628 19:09:42.780170  7628 sgd_solver.cpp:137] Iteration 42100, lr = 0.0342188, m = 0.9
I0628 19:09:44.223675  7613 data_reader.cpp:262] Starting prefetch of epoch 54
I0628 19:09:44.497489  7628 solver.cpp:349] Iteration 42200 (58.2397 iter/s, 1.71704s/100 iter), loss = 0.00100872
I0628 19:09:44.497510  7628 solver.cpp:371]     Train net output #0: loss = 0.00100921 (* 1 = 0.00100921 loss)
I0628 19:09:44.497514  7628 sgd_solver.cpp:137] Iteration 42200, lr = 0.0340625, m = 0.9
I0628 19:09:46.220155  7628 solver.cpp:349] Iteration 42300 (58.0598 iter/s, 1.72236s/100 iter), loss = 0.00575902
I0628 19:09:46.221843  7628 solver.cpp:371]     Train net output #0: loss = 0.00575952 (* 1 = 0.00575952 loss)
I0628 19:09:46.221851  7628 sgd_solver.cpp:137] Iteration 42300, lr = 0.0339063, m = 0.9
I0628 19:09:47.936377  7628 solver.cpp:349] Iteration 42400 (58.3347 iter/s, 1.71425s/100 iter), loss = 0.00137994
I0628 19:09:47.936395  7628 solver.cpp:371]     Train net output #0: loss = 0.00138043 (* 1 = 0.00138043 loss)
I0628 19:09:47.936399  7628 sgd_solver.cpp:137] Iteration 42400, lr = 0.03375, m = 0.9
I0628 19:09:49.651553  7628 solver.cpp:349] Iteration 42500 (58.3132 iter/s, 1.71488s/100 iter), loss = 0.00101983
I0628 19:09:49.651576  7628 solver.cpp:371]     Train net output #0: loss = 0.00102033 (* 1 = 0.00102033 loss)
I0628 19:09:49.651579  7628 sgd_solver.cpp:137] Iteration 42500, lr = 0.0335938, m = 0.9
I0628 19:09:51.364567  7628 solver.cpp:349] Iteration 42600 (58.3869 iter/s, 1.71271s/100 iter), loss = 0.0333354
I0628 19:09:51.364589  7628 solver.cpp:371]     Train net output #0: loss = 0.0333359 (* 1 = 0.0333359 loss)
I0628 19:09:51.364593  7628 sgd_solver.cpp:137] Iteration 42600, lr = 0.0334375, m = 0.9
I0628 19:09:53.085614  7628 solver.cpp:349] Iteration 42700 (58.1144 iter/s, 1.72074s/100 iter), loss = 0.0114139
I0628 19:09:53.085636  7628 solver.cpp:371]     Train net output #0: loss = 0.0114144 (* 1 = 0.0114144 loss)
I0628 19:09:53.085640  7628 sgd_solver.cpp:137] Iteration 42700, lr = 0.0332812, m = 0.9
I0628 19:09:54.801475  7628 solver.cpp:349] Iteration 42800 (58.2902 iter/s, 1.71556s/100 iter), loss = 0.00239018
I0628 19:09:54.801496  7628 solver.cpp:371]     Train net output #0: loss = 0.00239066 (* 1 = 0.00239066 loss)
I0628 19:09:54.801501  7628 sgd_solver.cpp:137] Iteration 42800, lr = 0.033125, m = 0.9
I0628 19:09:56.521100  7628 solver.cpp:349] Iteration 42900 (58.1624 iter/s, 1.71932s/100 iter), loss = 0.0131129
I0628 19:09:56.521121  7628 solver.cpp:371]     Train net output #0: loss = 0.0131134 (* 1 = 0.0131134 loss)
I0628 19:09:56.521126  7628 sgd_solver.cpp:137] Iteration 42900, lr = 0.0329687, m = 0.9
I0628 19:09:57.637574  7613 data_reader.cpp:262] Starting prefetch of epoch 55
I0628 19:09:58.221505  7628 solver.cpp:545] Iteration 43000, Testing net (#0)
I0628 19:09:59.226701  7626 data_reader.cpp:262] Starting prefetch of epoch 43
I0628 19:09:59.249558  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8992
I0628 19:09:59.249572  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9942
I0628 19:09:59.249577  7628 solver.cpp:630]     Test net output #2: loss = 0.413229 (* 1 = 0.413229 loss)
I0628 19:09:59.249590  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02793s
I0628 19:09:59.266927  7628 solver.cpp:349] Iteration 43000 (36.425 iter/s, 2.74537s/100 iter), loss = 0.0030306
I0628 19:09:59.266949  7628 solver.cpp:371]     Train net output #0: loss = 0.00303108 (* 1 = 0.00303108 loss)
I0628 19:09:59.266953  7628 sgd_solver.cpp:137] Iteration 43000, lr = 0.0328125, m = 0.9
I0628 19:10:00.981752  7628 solver.cpp:349] Iteration 43100 (58.3252 iter/s, 1.71452s/100 iter), loss = 0.00149266
I0628 19:10:00.981775  7628 solver.cpp:371]     Train net output #0: loss = 0.00149314 (* 1 = 0.00149314 loss)
I0628 19:10:00.981777  7628 sgd_solver.cpp:137] Iteration 43100, lr = 0.0326563, m = 0.9
I0628 19:10:02.695204  7628 solver.cpp:349] Iteration 43200 (58.372 iter/s, 1.71315s/100 iter), loss = 0.00574449
I0628 19:10:02.695226  7628 solver.cpp:371]     Train net output #0: loss = 0.00574497 (* 1 = 0.00574497 loss)
I0628 19:10:02.695230  7628 sgd_solver.cpp:137] Iteration 43200, lr = 0.0325, m = 0.9
I0628 19:10:04.410310  7628 solver.cpp:349] Iteration 43300 (58.3157 iter/s, 1.7148s/100 iter), loss = 0.0112118
I0628 19:10:04.410332  7628 solver.cpp:371]     Train net output #0: loss = 0.0112122 (* 1 = 0.0112122 loss)
I0628 19:10:04.410336  7628 sgd_solver.cpp:137] Iteration 43300, lr = 0.0323438, m = 0.9
I0628 19:10:06.128149  7628 solver.cpp:349] Iteration 43400 (58.2229 iter/s, 1.71754s/100 iter), loss = 0.00467569
I0628 19:10:06.128170  7628 solver.cpp:371]     Train net output #0: loss = 0.00467617 (* 1 = 0.00467617 loss)
I0628 19:10:06.128190  7628 sgd_solver.cpp:137] Iteration 43400, lr = 0.0321875, m = 0.9
I0628 19:10:07.845494  7628 solver.cpp:349] Iteration 43500 (58.2402 iter/s, 1.71703s/100 iter), loss = 0.00383569
I0628 19:10:07.845516  7628 solver.cpp:371]     Train net output #0: loss = 0.00383617 (* 1 = 0.00383617 loss)
I0628 19:10:07.845520  7628 sgd_solver.cpp:137] Iteration 43500, lr = 0.0320312, m = 0.9
I0628 19:10:09.561127  7628 solver.cpp:349] Iteration 43600 (58.2979 iter/s, 1.71533s/100 iter), loss = 0.00209082
I0628 19:10:09.561151  7628 solver.cpp:371]     Train net output #0: loss = 0.00209129 (* 1 = 0.00209129 loss)
I0628 19:10:09.561154  7628 sgd_solver.cpp:137] Iteration 43600, lr = 0.031875, m = 0.9
I0628 19:10:11.279901  7628 solver.cpp:349] Iteration 43700 (58.1914 iter/s, 1.71847s/100 iter), loss = 0.00163946
I0628 19:10:11.279922  7628 solver.cpp:371]     Train net output #0: loss = 0.00163994 (* 1 = 0.00163994 loss)
I0628 19:10:11.279927  7628 sgd_solver.cpp:137] Iteration 43700, lr = 0.0317187, m = 0.9
I0628 19:10:12.067689  7613 data_reader.cpp:262] Starting prefetch of epoch 56
I0628 19:10:12.995774  7628 solver.cpp:349] Iteration 43800 (58.2897 iter/s, 1.71557s/100 iter), loss = 0.00229173
I0628 19:10:12.995795  7628 solver.cpp:371]     Train net output #0: loss = 0.00229221 (* 1 = 0.00229221 loss)
I0628 19:10:12.995798  7628 sgd_solver.cpp:137] Iteration 43800, lr = 0.0315625, m = 0.9
I0628 19:10:14.707870  7628 solver.cpp:349] Iteration 43900 (58.4182 iter/s, 1.7118s/100 iter), loss = 0.00158572
I0628 19:10:14.707892  7628 solver.cpp:371]     Train net output #0: loss = 0.0015862 (* 1 = 0.0015862 loss)
I0628 19:10:14.707896  7628 sgd_solver.cpp:137] Iteration 43900, lr = 0.0314062, m = 0.9
I0628 19:10:16.407232  7628 solver.cpp:545] Iteration 44000, Testing net (#0)
I0628 19:10:17.417415  7626 data_reader.cpp:262] Starting prefetch of epoch 44
I0628 19:10:17.437829  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.8966
I0628 19:10:17.437842  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9966
I0628 19:10:17.437847  7628 solver.cpp:630]     Test net output #2: loss = 0.41162 (* 1 = 0.41162 loss)
I0628 19:10:17.437860  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03047s
I0628 19:10:17.455255  7628 solver.cpp:349] Iteration 44000 (36.4043 iter/s, 2.74693s/100 iter), loss = 0.00776767
I0628 19:10:17.455277  7628 solver.cpp:371]     Train net output #0: loss = 0.00776816 (* 1 = 0.00776816 loss)
I0628 19:10:17.455281  7628 sgd_solver.cpp:137] Iteration 44000, lr = 0.03125, m = 0.9
I0628 19:10:19.170554  7628 solver.cpp:349] Iteration 44100 (58.3092 iter/s, 1.71499s/100 iter), loss = 0.00415644
I0628 19:10:19.170574  7628 solver.cpp:371]     Train net output #0: loss = 0.00415693 (* 1 = 0.00415693 loss)
I0628 19:10:19.170578  7628 sgd_solver.cpp:137] Iteration 44100, lr = 0.0310938, m = 0.9
I0628 19:10:20.886026  7628 solver.cpp:349] Iteration 44200 (58.3032 iter/s, 1.71517s/100 iter), loss = 0.000475857
I0628 19:10:20.886050  7628 solver.cpp:371]     Train net output #0: loss = 0.000476353 (* 1 = 0.000476353 loss)
I0628 19:10:20.886055  7628 sgd_solver.cpp:137] Iteration 44200, lr = 0.0309375, m = 0.9
I0628 19:10:22.604846  7628 solver.cpp:349] Iteration 44300 (58.1899 iter/s, 1.71851s/100 iter), loss = 0.000835079
I0628 19:10:22.604869  7628 solver.cpp:371]     Train net output #0: loss = 0.000835574 (* 1 = 0.000835574 loss)
I0628 19:10:22.604874  7628 sgd_solver.cpp:137] Iteration 44300, lr = 0.0307813, m = 0.9
I0628 19:10:24.322022  7628 solver.cpp:349] Iteration 44400 (58.2456 iter/s, 1.71687s/100 iter), loss = 0.000569043
I0628 19:10:24.322044  7628 solver.cpp:371]     Train net output #0: loss = 0.000569534 (* 1 = 0.000569534 loss)
I0628 19:10:24.322048  7628 sgd_solver.cpp:137] Iteration 44400, lr = 0.030625, m = 0.9
I0628 19:10:26.036743  7628 solver.cpp:349] Iteration 44500 (58.3288 iter/s, 1.71442s/100 iter), loss = 0.00224332
I0628 19:10:26.036763  7628 solver.cpp:371]     Train net output #0: loss = 0.00224381 (* 1 = 0.00224381 loss)
I0628 19:10:26.036767  7628 sgd_solver.cpp:137] Iteration 44500, lr = 0.0304688, m = 0.9
I0628 19:10:26.516526  7613 data_reader.cpp:262] Starting prefetch of epoch 57
I0628 19:10:27.751324  7628 solver.cpp:349] Iteration 44600 (58.3335 iter/s, 1.71428s/100 iter), loss = 0.00182194
I0628 19:10:27.751346  7628 solver.cpp:371]     Train net output #0: loss = 0.00182244 (* 1 = 0.00182244 loss)
I0628 19:10:27.751350  7628 sgd_solver.cpp:137] Iteration 44600, lr = 0.0303125, m = 0.9
I0628 19:10:29.467573  7628 solver.cpp:349] Iteration 44700 (58.2768 iter/s, 1.71595s/100 iter), loss = 0.00302607
I0628 19:10:29.467597  7628 solver.cpp:371]     Train net output #0: loss = 0.00302656 (* 1 = 0.00302656 loss)
I0628 19:10:29.467602  7628 sgd_solver.cpp:137] Iteration 44700, lr = 0.0301562, m = 0.9
I0628 19:10:31.182684  7628 solver.cpp:349] Iteration 44800 (58.3156 iter/s, 1.71481s/100 iter), loss = 0.00257459
I0628 19:10:31.182706  7628 solver.cpp:371]     Train net output #0: loss = 0.00257508 (* 1 = 0.00257508 loss)
I0628 19:10:31.182710  7628 sgd_solver.cpp:137] Iteration 44800, lr = 0.03, m = 0.9
I0628 19:10:32.904532  7628 solver.cpp:349] Iteration 44900 (58.0874 iter/s, 1.72154s/100 iter), loss = 0.0129254
I0628 19:10:32.904554  7628 solver.cpp:371]     Train net output #0: loss = 0.0129259 (* 1 = 0.0129259 loss)
I0628 19:10:32.904558  7628 sgd_solver.cpp:137] Iteration 44900, lr = 0.0298437, m = 0.9
I0628 19:10:34.604908  7628 solver.cpp:545] Iteration 45000, Testing net (#0)
I0628 19:10:35.609802  7626 data_reader.cpp:262] Starting prefetch of epoch 45
I0628 19:10:35.633929  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.906
I0628 19:10:35.633944  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:10:35.633950  7628 solver.cpp:630]     Test net output #2: loss = 0.363261 (* 1 = 0.363261 loss)
I0628 19:10:35.633977  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02891s
I0628 19:10:35.651216  7628 solver.cpp:349] Iteration 45000 (36.4137 iter/s, 2.74622s/100 iter), loss = 0.00116706
I0628 19:10:35.651237  7628 solver.cpp:371]     Train net output #0: loss = 0.00116756 (* 1 = 0.00116756 loss)
I0628 19:10:35.651243  7628 sgd_solver.cpp:137] Iteration 45000, lr = 0.0296875, m = 0.9
I0628 19:10:37.368077  7628 solver.cpp:349] Iteration 45100 (58.2562 iter/s, 1.71656s/100 iter), loss = 0.00119502
I0628 19:10:37.368101  7628 solver.cpp:371]     Train net output #0: loss = 0.00119552 (* 1 = 0.00119552 loss)
I0628 19:10:37.368106  7628 sgd_solver.cpp:137] Iteration 45100, lr = 0.0295313, m = 0.9
I0628 19:10:39.084430  7628 solver.cpp:349] Iteration 45200 (58.2735 iter/s, 1.71605s/100 iter), loss = 0.00137846
I0628 19:10:39.084452  7628 solver.cpp:371]     Train net output #0: loss = 0.00137896 (* 1 = 0.00137896 loss)
I0628 19:10:39.084456  7628 sgd_solver.cpp:137] Iteration 45200, lr = 0.029375, m = 0.9
I0628 19:10:40.800128  7628 solver.cpp:349] Iteration 45300 (58.2956 iter/s, 1.7154s/100 iter), loss = 0.00118251
I0628 19:10:40.800149  7628 solver.cpp:371]     Train net output #0: loss = 0.00118301 (* 1 = 0.00118301 loss)
I0628 19:10:40.800153  7628 sgd_solver.cpp:137] Iteration 45300, lr = 0.0292188, m = 0.9
I0628 19:10:40.955957  7613 data_reader.cpp:262] Starting prefetch of epoch 58
I0628 19:10:42.517959  7628 solver.cpp:349] Iteration 45400 (58.2232 iter/s, 1.71753s/100 iter), loss = 0.00054028
I0628 19:10:42.517982  7628 solver.cpp:371]     Train net output #0: loss = 0.000540779 (* 1 = 0.000540779 loss)
I0628 19:10:42.517985  7628 sgd_solver.cpp:137] Iteration 45400, lr = 0.0290625, m = 0.9
I0628 19:10:44.237550  7628 solver.cpp:349] Iteration 45500 (58.1638 iter/s, 1.71928s/100 iter), loss = 0.000857456
I0628 19:10:44.237572  7628 solver.cpp:371]     Train net output #0: loss = 0.000857955 (* 1 = 0.000857955 loss)
I0628 19:10:44.237576  7628 sgd_solver.cpp:137] Iteration 45500, lr = 0.0289063, m = 0.9
I0628 19:10:45.954308  7628 solver.cpp:349] Iteration 45600 (58.2597 iter/s, 1.71645s/100 iter), loss = 0.000725433
I0628 19:10:45.954327  7628 solver.cpp:371]     Train net output #0: loss = 0.000725933 (* 1 = 0.000725933 loss)
I0628 19:10:45.954331  7628 sgd_solver.cpp:137] Iteration 45600, lr = 0.02875, m = 0.9
I0628 19:10:47.673192  7628 solver.cpp:349] Iteration 45700 (58.1875 iter/s, 1.71858s/100 iter), loss = 0.00171231
I0628 19:10:47.673260  7628 solver.cpp:371]     Train net output #0: loss = 0.00171281 (* 1 = 0.00171281 loss)
I0628 19:10:47.673267  7628 sgd_solver.cpp:137] Iteration 45700, lr = 0.0285937, m = 0.9
I0628 19:10:49.395074  7628 solver.cpp:349] Iteration 45800 (58.088 iter/s, 1.72153s/100 iter), loss = 0.000452113
I0628 19:10:49.395097  7628 solver.cpp:371]     Train net output #0: loss = 0.000452608 (* 1 = 0.000452608 loss)
I0628 19:10:49.395102  7628 sgd_solver.cpp:137] Iteration 45800, lr = 0.0284375, m = 0.9
I0628 19:10:51.115139  7628 solver.cpp:349] Iteration 45900 (58.1478 iter/s, 1.71976s/100 iter), loss = 0.000580397
I0628 19:10:51.115161  7628 solver.cpp:371]     Train net output #0: loss = 0.000580893 (* 1 = 0.000580893 loss)
I0628 19:10:51.115166  7628 sgd_solver.cpp:137] Iteration 45900, lr = 0.0282812, m = 0.9
I0628 19:10:52.812131  7628 solver.cpp:545] Iteration 46000, Testing net (#0)
I0628 19:10:53.820148  7626 data_reader.cpp:262] Starting prefetch of epoch 46
I0628 19:10:53.844609  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9074
I0628 19:10:53.844620  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:10:53.844625  7628 solver.cpp:630]     Test net output #2: loss = 0.374992 (* 1 = 0.374992 loss)
I0628 19:10:53.844640  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03235s
I0628 19:10:53.862095  7628 solver.cpp:349] Iteration 46000 (36.41 iter/s, 2.74649s/100 iter), loss = 0.000904135
I0628 19:10:53.862118  7628 solver.cpp:371]     Train net output #0: loss = 0.000904629 (* 1 = 0.000904629 loss)
I0628 19:10:53.862121  7628 sgd_solver.cpp:137] Iteration 46000, lr = 0.028125, m = 0.9
I0628 19:10:55.409126  7613 data_reader.cpp:262] Starting prefetch of epoch 59
I0628 19:10:55.580260  7628 solver.cpp:349] Iteration 46100 (58.212 iter/s, 1.71786s/100 iter), loss = 0.00528782
I0628 19:10:55.580281  7628 solver.cpp:371]     Train net output #0: loss = 0.00528832 (* 1 = 0.00528832 loss)
I0628 19:10:55.580286  7628 sgd_solver.cpp:137] Iteration 46100, lr = 0.0279688, m = 0.9
I0628 19:10:57.294173  7628 solver.cpp:349] Iteration 46200 (58.3563 iter/s, 1.71361s/100 iter), loss = 0.00419687
I0628 19:10:57.294194  7628 solver.cpp:371]     Train net output #0: loss = 0.00419737 (* 1 = 0.00419737 loss)
I0628 19:10:57.294198  7628 sgd_solver.cpp:137] Iteration 46200, lr = 0.0278125, m = 0.9
I0628 19:10:59.014798  7628 solver.cpp:349] Iteration 46300 (58.1286 iter/s, 1.72032s/100 iter), loss = 0.000930343
I0628 19:10:59.014820  7628 solver.cpp:371]     Train net output #0: loss = 0.000930837 (* 1 = 0.000930837 loss)
I0628 19:10:59.014824  7628 sgd_solver.cpp:137] Iteration 46300, lr = 0.0276563, m = 0.9
I0628 19:11:00.733325  7628 solver.cpp:349] Iteration 46400 (58.1996 iter/s, 1.71823s/100 iter), loss = 0.00102227
I0628 19:11:00.733347  7628 solver.cpp:371]     Train net output #0: loss = 0.00102276 (* 1 = 0.00102276 loss)
I0628 19:11:00.733351  7628 sgd_solver.cpp:137] Iteration 46400, lr = 0.0275, m = 0.9
I0628 19:11:02.448709  7628 solver.cpp:349] Iteration 46500 (58.3063 iter/s, 1.71508s/100 iter), loss = 0.00174281
I0628 19:11:02.448731  7628 solver.cpp:371]     Train net output #0: loss = 0.0017433 (* 1 = 0.0017433 loss)
I0628 19:11:02.448735  7628 sgd_solver.cpp:137] Iteration 46500, lr = 0.0273438, m = 0.9
I0628 19:11:04.165575  7628 solver.cpp:349] Iteration 46600 (58.256 iter/s, 1.71656s/100 iter), loss = 0.00100025
I0628 19:11:04.165596  7628 solver.cpp:371]     Train net output #0: loss = 0.00100075 (* 1 = 0.00100075 loss)
I0628 19:11:04.165601  7628 sgd_solver.cpp:137] Iteration 46600, lr = 0.0271875, m = 0.9
I0628 19:11:05.880583  7628 solver.cpp:349] Iteration 46700 (58.3191 iter/s, 1.71471s/100 iter), loss = 0.00040306
I0628 19:11:05.880604  7628 solver.cpp:371]     Train net output #0: loss = 0.000403554 (* 1 = 0.000403554 loss)
I0628 19:11:05.880609  7628 sgd_solver.cpp:137] Iteration 46700, lr = 0.0270312, m = 0.9
I0628 19:11:07.604487  7628 solver.cpp:349] Iteration 46800 (58.018 iter/s, 1.7236s/100 iter), loss = 0.00099551
I0628 19:11:07.604511  7628 solver.cpp:371]     Train net output #0: loss = 0.000996004 (* 1 = 0.000996004 loss)
I0628 19:11:07.604531  7628 sgd_solver.cpp:137] Iteration 46800, lr = 0.026875, m = 0.9
I0628 19:11:08.826562  7613 data_reader.cpp:262] Starting prefetch of epoch 60
I0628 19:11:09.324095  7628 solver.cpp:349] Iteration 46900 (58.1638 iter/s, 1.71928s/100 iter), loss = 0.00249307
I0628 19:11:09.324117  7628 solver.cpp:371]     Train net output #0: loss = 0.00249356 (* 1 = 0.00249356 loss)
I0628 19:11:09.324122  7628 sgd_solver.cpp:137] Iteration 46900, lr = 0.0267187, m = 0.9
I0628 19:11:11.024080  7628 solver.cpp:545] Iteration 47000, Testing net (#0)
I0628 19:11:12.029095  7626 data_reader.cpp:262] Starting prefetch of epoch 47
I0628 19:11:12.052038  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9168
I0628 19:11:12.052052  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:11:12.052059  7628 solver.cpp:630]     Test net output #2: loss = 0.33035 (* 1 = 0.33035 loss)
I0628 19:11:12.052076  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02784s
I0628 19:11:12.069401  7628 solver.cpp:349] Iteration 47000 (36.432 iter/s, 2.74484s/100 iter), loss = 0.00146492
I0628 19:11:12.069423  7628 solver.cpp:371]     Train net output #0: loss = 0.00146541 (* 1 = 0.00146541 loss)
I0628 19:11:12.069427  7628 sgd_solver.cpp:137] Iteration 47000, lr = 0.0265625, m = 0.9
I0628 19:11:13.782934  7628 solver.cpp:349] Iteration 47100 (58.3692 iter/s, 1.71323s/100 iter), loss = 0.00141971
I0628 19:11:13.782958  7628 solver.cpp:371]     Train net output #0: loss = 0.00142021 (* 1 = 0.00142021 loss)
I0628 19:11:13.782961  7628 sgd_solver.cpp:137] Iteration 47100, lr = 0.0264063, m = 0.9
I0628 19:11:15.500231  7628 solver.cpp:349] Iteration 47200 (58.2414 iter/s, 1.71699s/100 iter), loss = 0.000828162
I0628 19:11:15.500253  7628 solver.cpp:371]     Train net output #0: loss = 0.000828657 (* 1 = 0.000828657 loss)
I0628 19:11:15.500257  7628 sgd_solver.cpp:137] Iteration 47200, lr = 0.02625, m = 0.9
I0628 19:11:17.214612  7628 solver.cpp:349] Iteration 47300 (58.3404 iter/s, 1.71408s/100 iter), loss = 0.000408608
I0628 19:11:17.214634  7628 solver.cpp:371]     Train net output #0: loss = 0.000409102 (* 1 = 0.000409102 loss)
I0628 19:11:17.214639  7628 sgd_solver.cpp:137] Iteration 47300, lr = 0.0260938, m = 0.9
I0628 19:11:18.930631  7628 solver.cpp:349] Iteration 47400 (58.2847 iter/s, 1.71571s/100 iter), loss = 0.0012414
I0628 19:11:18.930706  7628 solver.cpp:371]     Train net output #0: loss = 0.0012419 (* 1 = 0.0012419 loss)
I0628 19:11:18.930713  7628 sgd_solver.cpp:137] Iteration 47400, lr = 0.0259375, m = 0.9
I0628 19:11:20.645803  7628 solver.cpp:349] Iteration 47500 (58.3154 iter/s, 1.71481s/100 iter), loss = 0.000378691
I0628 19:11:20.645825  7628 solver.cpp:371]     Train net output #0: loss = 0.000379185 (* 1 = 0.000379185 loss)
I0628 19:11:20.645829  7628 sgd_solver.cpp:137] Iteration 47500, lr = 0.0257812, m = 0.9
I0628 19:11:22.366601  7628 solver.cpp:349] Iteration 47600 (58.1228 iter/s, 1.72049s/100 iter), loss = 0.000732727
I0628 19:11:22.366623  7628 solver.cpp:371]     Train net output #0: loss = 0.000733222 (* 1 = 0.000733222 loss)
I0628 19:11:22.366627  7628 sgd_solver.cpp:137] Iteration 47600, lr = 0.025625, m = 0.9
I0628 19:11:23.278105  7613 data_reader.cpp:262] Starting prefetch of epoch 61
I0628 19:11:24.083503  7628 solver.cpp:349] Iteration 47700 (58.2547 iter/s, 1.7166s/100 iter), loss = 0.000534719
I0628 19:11:24.083524  7628 solver.cpp:371]     Train net output #0: loss = 0.000535214 (* 1 = 0.000535214 loss)
I0628 19:11:24.083530  7628 sgd_solver.cpp:137] Iteration 47700, lr = 0.0254687, m = 0.9
I0628 19:11:25.797927  7628 solver.cpp:349] Iteration 47800 (58.339 iter/s, 1.71412s/100 iter), loss = 0.000481959
I0628 19:11:25.797951  7628 solver.cpp:371]     Train net output #0: loss = 0.000482454 (* 1 = 0.000482454 loss)
I0628 19:11:25.797956  7628 sgd_solver.cpp:137] Iteration 47800, lr = 0.0253125, m = 0.9
I0628 19:11:27.512107  7628 solver.cpp:349] Iteration 47900 (58.3474 iter/s, 1.71387s/100 iter), loss = 0.000864414
I0628 19:11:27.512128  7628 solver.cpp:371]     Train net output #0: loss = 0.000864909 (* 1 = 0.000864909 loss)
I0628 19:11:27.512133  7628 sgd_solver.cpp:137] Iteration 47900, lr = 0.0251562, m = 0.9
I0628 19:11:29.207978  7628 solver.cpp:545] Iteration 48000, Testing net (#0)
I0628 19:11:30.215631  7626 data_reader.cpp:262] Starting prefetch of epoch 48
I0628 19:11:30.239089  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.916
I0628 19:11:30.239101  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:11:30.239106  7628 solver.cpp:630]     Test net output #2: loss = 0.321366 (* 1 = 0.321366 loss)
I0628 19:11:30.239120  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03098s
I0628 19:11:30.256501  7628 solver.cpp:349] Iteration 48000 (36.4441 iter/s, 2.74393s/100 iter), loss = 0.00435377
I0628 19:11:30.256523  7628 solver.cpp:371]     Train net output #0: loss = 0.00435426 (* 1 = 0.00435426 loss)
I0628 19:11:30.256527  7628 sgd_solver.cpp:137] Iteration 48000, lr = 0.025, m = 0.9
I0628 19:11:31.973134  7628 solver.cpp:349] Iteration 48100 (58.2638 iter/s, 1.71633s/100 iter), loss = 0.000986121
I0628 19:11:31.973157  7628 solver.cpp:371]     Train net output #0: loss = 0.000986616 (* 1 = 0.000986616 loss)
I0628 19:11:31.973161  7628 sgd_solver.cpp:137] Iteration 48100, lr = 0.0248438, m = 0.9
I0628 19:11:33.689941  7628 solver.cpp:349] Iteration 48200 (58.258 iter/s, 1.7165s/100 iter), loss = 0.000934479
I0628 19:11:33.689963  7628 solver.cpp:371]     Train net output #0: loss = 0.000934975 (* 1 = 0.000934975 loss)
I0628 19:11:33.689967  7628 sgd_solver.cpp:137] Iteration 48200, lr = 0.0246875, m = 0.9
I0628 19:11:35.406158  7628 solver.cpp:349] Iteration 48300 (58.2779 iter/s, 1.71592s/100 iter), loss = 0.000775601
I0628 19:11:35.406182  7628 solver.cpp:371]     Train net output #0: loss = 0.000776096 (* 1 = 0.000776096 loss)
I0628 19:11:35.406185  7628 sgd_solver.cpp:137] Iteration 48300, lr = 0.0245313, m = 0.9
I0628 19:11:37.124022  7628 solver.cpp:349] Iteration 48400 (58.2221 iter/s, 1.71756s/100 iter), loss = 0.000967483
I0628 19:11:37.124043  7628 solver.cpp:371]     Train net output #0: loss = 0.000967979 (* 1 = 0.000967979 loss)
I0628 19:11:37.124047  7628 sgd_solver.cpp:137] Iteration 48400, lr = 0.024375, m = 0.9
I0628 19:11:37.709177  7613 data_reader.cpp:262] Starting prefetch of epoch 62
I0628 19:11:38.838964  7628 solver.cpp:349] Iteration 48500 (58.3212 iter/s, 1.71464s/100 iter), loss = 0.00231042
I0628 19:11:38.839002  7628 solver.cpp:371]     Train net output #0: loss = 0.00231092 (* 1 = 0.00231092 loss)
I0628 19:11:38.839006  7628 sgd_solver.cpp:137] Iteration 48500, lr = 0.0242188, m = 0.9
I0628 19:11:40.558801  7628 solver.cpp:349] Iteration 48600 (58.1559 iter/s, 1.71952s/100 iter), loss = 0.000637642
I0628 19:11:40.558825  7628 solver.cpp:371]     Train net output #0: loss = 0.000638137 (* 1 = 0.000638137 loss)
I0628 19:11:40.558828  7628 sgd_solver.cpp:137] Iteration 48600, lr = 0.0240625, m = 0.9
I0628 19:11:42.275573  7628 solver.cpp:349] Iteration 48700 (58.2592 iter/s, 1.71647s/100 iter), loss = 0.00146904
I0628 19:11:42.275595  7628 solver.cpp:371]     Train net output #0: loss = 0.00146953 (* 1 = 0.00146953 loss)
I0628 19:11:42.275599  7628 sgd_solver.cpp:137] Iteration 48700, lr = 0.0239062, m = 0.9
I0628 19:11:43.991360  7628 solver.cpp:349] Iteration 48800 (58.2926 iter/s, 1.71548s/100 iter), loss = 0.000547666
I0628 19:11:43.991381  7628 solver.cpp:371]     Train net output #0: loss = 0.000548162 (* 1 = 0.000548162 loss)
I0628 19:11:43.991385  7628 sgd_solver.cpp:137] Iteration 48800, lr = 0.02375, m = 0.9
I0628 19:11:45.707096  7628 solver.cpp:349] Iteration 48900 (58.2943 iter/s, 1.71543s/100 iter), loss = 0.000985773
I0628 19:11:45.707120  7628 solver.cpp:371]     Train net output #0: loss = 0.000986268 (* 1 = 0.000986268 loss)
I0628 19:11:45.707125  7628 sgd_solver.cpp:137] Iteration 48900, lr = 0.0235937, m = 0.9
I0628 19:11:47.406577  7628 solver.cpp:545] Iteration 49000, Testing net (#0)
I0628 19:11:48.415715  7626 data_reader.cpp:262] Starting prefetch of epoch 49
I0628 19:11:48.436552  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9156
I0628 19:11:48.436564  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9968
I0628 19:11:48.436569  7628 solver.cpp:630]     Test net output #2: loss = 0.314365 (* 1 = 0.314365 loss)
I0628 19:11:48.436583  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02985s
I0628 19:11:48.453876  7628 solver.cpp:349] Iteration 49000 (36.4125 iter/s, 2.74631s/100 iter), loss = 0.000900694
I0628 19:11:48.453899  7628 solver.cpp:371]     Train net output #0: loss = 0.000901189 (* 1 = 0.000901189 loss)
I0628 19:11:48.453903  7628 sgd_solver.cpp:137] Iteration 49000, lr = 0.0234375, m = 0.9
I0628 19:11:50.171735  7628 solver.cpp:349] Iteration 49100 (58.2224 iter/s, 1.71755s/100 iter), loss = 0.000571003
I0628 19:11:50.171833  7628 solver.cpp:371]     Train net output #0: loss = 0.000571498 (* 1 = 0.000571498 loss)
I0628 19:11:50.171838  7628 sgd_solver.cpp:137] Iteration 49100, lr = 0.0232813, m = 0.9
I0628 19:11:51.886750  7628 solver.cpp:349] Iteration 49200 (58.3214 iter/s, 1.71464s/100 iter), loss = 0.000793613
I0628 19:11:51.886773  7628 solver.cpp:371]     Train net output #0: loss = 0.000794108 (* 1 = 0.000794108 loss)
I0628 19:11:51.886777  7628 sgd_solver.cpp:137] Iteration 49200, lr = 0.023125, m = 0.9
I0628 19:11:52.146584  7613 data_reader.cpp:262] Starting prefetch of epoch 63
I0628 19:11:53.609701  7628 solver.cpp:349] Iteration 49300 (58.0502 iter/s, 1.72265s/100 iter), loss = 0.000874412
I0628 19:11:53.609724  7628 solver.cpp:371]     Train net output #0: loss = 0.000874907 (* 1 = 0.000874907 loss)
I0628 19:11:53.609728  7628 sgd_solver.cpp:137] Iteration 49300, lr = 0.0229688, m = 0.9
I0628 19:11:55.327529  7628 solver.cpp:349] Iteration 49400 (58.2233 iter/s, 1.71753s/100 iter), loss = 0.00109905
I0628 19:11:55.327551  7628 solver.cpp:371]     Train net output #0: loss = 0.00109954 (* 1 = 0.00109954 loss)
I0628 19:11:55.327555  7628 sgd_solver.cpp:137] Iteration 49400, lr = 0.0228125, m = 0.9
I0628 19:11:57.047526  7628 solver.cpp:349] Iteration 49500 (58.1499 iter/s, 1.71969s/100 iter), loss = 0.00146004
I0628 19:11:57.047551  7628 solver.cpp:371]     Train net output #0: loss = 0.00146053 (* 1 = 0.00146053 loss)
I0628 19:11:57.047555  7628 sgd_solver.cpp:137] Iteration 49500, lr = 0.0226563, m = 0.9
I0628 19:11:58.762517  7628 solver.cpp:349] Iteration 49600 (58.3197 iter/s, 1.71469s/100 iter), loss = 0.00134828
I0628 19:11:58.762540  7628 solver.cpp:371]     Train net output #0: loss = 0.00134877 (* 1 = 0.00134877 loss)
I0628 19:11:58.762544  7628 sgd_solver.cpp:137] Iteration 49600, lr = 0.0225, m = 0.9
I0628 19:12:00.480763  7628 solver.cpp:349] Iteration 49700 (58.2092 iter/s, 1.71794s/100 iter), loss = 0.000630087
I0628 19:12:00.480785  7628 solver.cpp:371]     Train net output #0: loss = 0.000630582 (* 1 = 0.000630582 loss)
I0628 19:12:00.480789  7628 sgd_solver.cpp:137] Iteration 49700, lr = 0.0223437, m = 0.9
I0628 19:12:02.197926  7628 solver.cpp:349] Iteration 49800 (58.2459 iter/s, 1.71686s/100 iter), loss = 0.00144857
I0628 19:12:02.197947  7628 solver.cpp:371]     Train net output #0: loss = 0.00144907 (* 1 = 0.00144907 loss)
I0628 19:12:02.197950  7628 sgd_solver.cpp:137] Iteration 49800, lr = 0.0221875, m = 0.9
I0628 19:12:03.914445  7628 solver.cpp:349] Iteration 49900 (58.2677 iter/s, 1.71622s/100 iter), loss = 0.00029653
I0628 19:12:03.914468  7628 solver.cpp:371]     Train net output #0: loss = 0.000297025 (* 1 = 0.000297025 loss)
I0628 19:12:03.914471  7628 sgd_solver.cpp:137] Iteration 49900, lr = 0.0220312, m = 0.9
I0628 19:12:05.563856  7613 data_reader.cpp:262] Starting prefetch of epoch 64
I0628 19:12:05.615854  7628 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_50000.caffemodel
I0628 19:12:05.624622  7628 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_50000.solverstate
I0628 19:12:05.628192  7628 solver.cpp:545] Iteration 50000, Testing net (#0)
I0628 19:12:06.636804  7626 data_reader.cpp:262] Starting prefetch of epoch 50
I0628 19:12:06.657254  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:12:06.657274  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9966
I0628 19:12:06.657279  7628 solver.cpp:630]     Test net output #2: loss = 0.31463 (* 1 = 0.31463 loss)
I0628 19:12:06.657294  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02894s
I0628 19:12:06.674578  7628 solver.cpp:349] Iteration 50000 (36.2362 iter/s, 2.75967s/100 iter), loss = 0.00104589
I0628 19:12:06.674600  7628 solver.cpp:371]     Train net output #0: loss = 0.00104638 (* 1 = 0.00104638 loss)
I0628 19:12:06.674604  7628 sgd_solver.cpp:137] Iteration 50000, lr = 0.021875, m = 0.9
I0628 19:12:08.390637  7628 solver.cpp:349] Iteration 50100 (58.2839 iter/s, 1.71574s/100 iter), loss = 0.00130691
I0628 19:12:08.390661  7628 solver.cpp:371]     Train net output #0: loss = 0.00130741 (* 1 = 0.00130741 loss)
I0628 19:12:08.390664  7628 sgd_solver.cpp:137] Iteration 50100, lr = 0.0217188, m = 0.9
I0628 19:12:10.110435  7628 solver.cpp:349] Iteration 50200 (58.1566 iter/s, 1.71949s/100 iter), loss = 0.00180283
I0628 19:12:10.110456  7628 solver.cpp:371]     Train net output #0: loss = 0.00180332 (* 1 = 0.00180332 loss)
I0628 19:12:10.110461  7628 sgd_solver.cpp:137] Iteration 50200, lr = 0.0215625, m = 0.9
I0628 19:12:11.821542  7628 solver.cpp:349] Iteration 50300 (58.452 iter/s, 1.7108s/100 iter), loss = 0.000863771
I0628 19:12:11.821568  7628 solver.cpp:371]     Train net output #0: loss = 0.000864266 (* 1 = 0.000864266 loss)
I0628 19:12:11.821573  7628 sgd_solver.cpp:137] Iteration 50300, lr = 0.0214063, m = 0.9
I0628 19:12:13.540753  7628 solver.cpp:349] Iteration 50400 (58.1766 iter/s, 1.7189s/100 iter), loss = 0.00155872
I0628 19:12:13.540776  7628 solver.cpp:371]     Train net output #0: loss = 0.00155922 (* 1 = 0.00155922 loss)
I0628 19:12:13.540779  7628 sgd_solver.cpp:137] Iteration 50400, lr = 0.02125, m = 0.9
I0628 19:12:15.256536  7628 solver.cpp:349] Iteration 50500 (58.2927 iter/s, 1.71548s/100 iter), loss = 0.000717441
I0628 19:12:15.256558  7628 solver.cpp:371]     Train net output #0: loss = 0.000717936 (* 1 = 0.000717936 loss)
I0628 19:12:15.256562  7628 sgd_solver.cpp:137] Iteration 50500, lr = 0.0210938, m = 0.9
I0628 19:12:16.979229  7628 solver.cpp:349] Iteration 50600 (58.0588 iter/s, 1.72239s/100 iter), loss = 0.000640474
I0628 19:12:16.979254  7628 solver.cpp:371]     Train net output #0: loss = 0.000640969 (* 1 = 0.000640969 loss)
I0628 19:12:16.979257  7628 sgd_solver.cpp:137] Iteration 50600, lr = 0.0209375, m = 0.9
I0628 19:12:18.695534  7628 solver.cpp:349] Iteration 50700 (58.275 iter/s, 1.716s/100 iter), loss = 0.00142492
I0628 19:12:18.695559  7628 solver.cpp:371]     Train net output #0: loss = 0.00142542 (* 1 = 0.00142542 loss)
I0628 19:12:18.695562  7628 sgd_solver.cpp:137] Iteration 50700, lr = 0.0207812, m = 0.9
I0628 19:12:20.035614  7613 data_reader.cpp:262] Starting prefetch of epoch 65
I0628 19:12:20.415071  7628 solver.cpp:349] Iteration 50800 (58.1656 iter/s, 1.71923s/100 iter), loss = 0.000468509
I0628 19:12:20.415139  7628 solver.cpp:371]     Train net output #0: loss = 0.000469004 (* 1 = 0.000469004 loss)
I0628 19:12:20.415145  7628 sgd_solver.cpp:137] Iteration 50800, lr = 0.020625, m = 0.9
I0628 19:12:22.136160  7628 solver.cpp:349] Iteration 50900 (58.1146 iter/s, 1.72074s/100 iter), loss = 0.000852344
I0628 19:12:22.136183  7628 solver.cpp:371]     Train net output #0: loss = 0.00085284 (* 1 = 0.00085284 loss)
I0628 19:12:22.136186  7628 sgd_solver.cpp:137] Iteration 50900, lr = 0.0204687, m = 0.9
I0628 19:12:23.835794  7628 solver.cpp:545] Iteration 51000, Testing net (#0)
I0628 19:12:24.842306  7626 data_reader.cpp:262] Starting prefetch of epoch 51
I0628 19:12:24.863960  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.917
I0628 19:12:24.863978  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9968
I0628 19:12:24.863983  7628 solver.cpp:630]     Test net output #2: loss = 0.312147 (* 1 = 0.312147 loss)
I0628 19:12:24.863998  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02805s
I0628 19:12:24.881219  7628 solver.cpp:349] Iteration 51000 (36.4352 iter/s, 2.7446s/100 iter), loss = 0.00205634
I0628 19:12:24.881242  7628 solver.cpp:371]     Train net output #0: loss = 0.00205683 (* 1 = 0.00205683 loss)
I0628 19:12:24.881247  7628 sgd_solver.cpp:137] Iteration 51000, lr = 0.0203125, m = 0.9
I0628 19:12:26.596889  7628 solver.cpp:349] Iteration 51100 (58.2966 iter/s, 1.71537s/100 iter), loss = 0.00198327
I0628 19:12:26.596912  7628 solver.cpp:371]     Train net output #0: loss = 0.00198376 (* 1 = 0.00198376 loss)
I0628 19:12:26.596916  7628 sgd_solver.cpp:137] Iteration 51100, lr = 0.0201563, m = 0.9
I0628 19:12:28.313642  7628 solver.cpp:349] Iteration 51200 (58.2598 iter/s, 1.71645s/100 iter), loss = 0.00128275
I0628 19:12:28.313663  7628 solver.cpp:371]     Train net output #0: loss = 0.00128324 (* 1 = 0.00128324 loss)
I0628 19:12:28.313668  7628 sgd_solver.cpp:137] Iteration 51200, lr = 0.02, m = 0.9
I0628 19:12:30.031705  7628 solver.cpp:349] Iteration 51300 (58.2154 iter/s, 1.71776s/100 iter), loss = 0.00125445
I0628 19:12:30.031726  7628 solver.cpp:371]     Train net output #0: loss = 0.00125495 (* 1 = 0.00125495 loss)
I0628 19:12:30.031730  7628 sgd_solver.cpp:137] Iteration 51300, lr = 0.0198438, m = 0.9
I0628 19:12:31.749881  7628 solver.cpp:349] Iteration 51400 (58.2115 iter/s, 1.71787s/100 iter), loss = 0.000987464
I0628 19:12:31.749904  7628 solver.cpp:371]     Train net output #0: loss = 0.00098796 (* 1 = 0.00098796 loss)
I0628 19:12:31.749908  7628 sgd_solver.cpp:137] Iteration 51400, lr = 0.0196875, m = 0.9
I0628 19:12:33.465088  7628 solver.cpp:349] Iteration 51500 (58.3123 iter/s, 1.7149s/100 iter), loss = 0.000573213
I0628 19:12:33.465111  7628 solver.cpp:371]     Train net output #0: loss = 0.000573709 (* 1 = 0.000573709 loss)
I0628 19:12:33.465116  7628 sgd_solver.cpp:137] Iteration 51500, lr = 0.0195312, m = 0.9
I0628 19:12:34.479270  7613 data_reader.cpp:262] Starting prefetch of epoch 66
I0628 19:12:35.182248  7628 solver.cpp:349] Iteration 51600 (58.246 iter/s, 1.71686s/100 iter), loss = 0.0012108
I0628 19:12:35.182271  7628 solver.cpp:371]     Train net output #0: loss = 0.0012113 (* 1 = 0.0012113 loss)
I0628 19:12:35.182274  7628 sgd_solver.cpp:137] Iteration 51600, lr = 0.019375, m = 0.9
I0628 19:12:36.895792  7628 solver.cpp:349] Iteration 51700 (58.3689 iter/s, 1.71324s/100 iter), loss = 0.000878255
I0628 19:12:36.895814  7628 solver.cpp:371]     Train net output #0: loss = 0.000878751 (* 1 = 0.000878751 loss)
I0628 19:12:36.895818  7628 sgd_solver.cpp:137] Iteration 51700, lr = 0.0192187, m = 0.9
I0628 19:12:38.615947  7628 solver.cpp:349] Iteration 51800 (58.1445 iter/s, 1.71985s/100 iter), loss = 0.00111304
I0628 19:12:38.615970  7628 solver.cpp:371]     Train net output #0: loss = 0.00111354 (* 1 = 0.00111354 loss)
I0628 19:12:38.615974  7628 sgd_solver.cpp:137] Iteration 51800, lr = 0.0190625, m = 0.9
I0628 19:12:40.337216  7628 solver.cpp:349] Iteration 51900 (58.1069 iter/s, 1.72096s/100 iter), loss = 0.000501029
I0628 19:12:40.337239  7628 solver.cpp:371]     Train net output #0: loss = 0.000501525 (* 1 = 0.000501525 loss)
I0628 19:12:40.337260  7628 sgd_solver.cpp:137] Iteration 51900, lr = 0.0189062, m = 0.9
I0628 19:12:42.033964  7628 solver.cpp:545] Iteration 52000, Testing net (#0)
I0628 19:12:43.039011  7626 data_reader.cpp:262] Starting prefetch of epoch 52
I0628 19:12:43.062808  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9178
I0628 19:12:43.062827  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:12:43.062834  7628 solver.cpp:630]     Test net output #2: loss = 0.308237 (* 1 = 0.308237 loss)
I0628 19:12:43.062855  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02873s
I0628 19:12:43.082193  7628 solver.cpp:349] Iteration 52000 (36.4366 iter/s, 2.74449s/100 iter), loss = 0.00161176
I0628 19:12:43.082212  7628 solver.cpp:371]     Train net output #0: loss = 0.00161226 (* 1 = 0.00161226 loss)
I0628 19:12:43.082216  7628 sgd_solver.cpp:137] Iteration 52000, lr = 0.01875, m = 0.9
I0628 19:12:44.803483  7628 solver.cpp:349] Iteration 52100 (58.1061 iter/s, 1.72099s/100 iter), loss = 0.000605589
I0628 19:12:44.803506  7628 solver.cpp:371]     Train net output #0: loss = 0.000606085 (* 1 = 0.000606085 loss)
I0628 19:12:44.803510  7628 sgd_solver.cpp:137] Iteration 52100, lr = 0.0185938, m = 0.9
I0628 19:12:46.520481  7628 solver.cpp:349] Iteration 52200 (58.2515 iter/s, 1.71669s/100 iter), loss = 0.000699406
I0628 19:12:46.520503  7628 solver.cpp:371]     Train net output #0: loss = 0.000699902 (* 1 = 0.000699902 loss)
I0628 19:12:46.520508  7628 sgd_solver.cpp:137] Iteration 52200, lr = 0.0184375, m = 0.9
I0628 19:12:48.240588  7628 solver.cpp:349] Iteration 52300 (58.1462 iter/s, 1.7198s/100 iter), loss = 0.00253099
I0628 19:12:48.240612  7628 solver.cpp:371]     Train net output #0: loss = 0.00253149 (* 1 = 0.00253149 loss)
I0628 19:12:48.240617  7628 sgd_solver.cpp:137] Iteration 52300, lr = 0.0182813, m = 0.9
I0628 19:12:48.926971  7613 data_reader.cpp:262] Starting prefetch of epoch 67
I0628 19:12:49.954496  7628 solver.cpp:349] Iteration 52400 (58.3567 iter/s, 1.7136s/100 iter), loss = 0.000589798
I0628 19:12:49.954520  7628 solver.cpp:371]     Train net output #0: loss = 0.000590295 (* 1 = 0.000590295 loss)
I0628 19:12:49.954525  7628 sgd_solver.cpp:137] Iteration 52400, lr = 0.018125, m = 0.9
I0628 19:12:51.674468  7628 solver.cpp:349] Iteration 52500 (58.1509 iter/s, 1.71966s/100 iter), loss = 0.00186909
I0628 19:12:51.674530  7628 solver.cpp:371]     Train net output #0: loss = 0.00186959 (* 1 = 0.00186959 loss)
I0628 19:12:51.674536  7628 sgd_solver.cpp:137] Iteration 52500, lr = 0.0179687, m = 0.9
I0628 19:12:53.389947  7628 solver.cpp:349] Iteration 52600 (58.3046 iter/s, 1.71513s/100 iter), loss = 0.00192031
I0628 19:12:53.389969  7628 solver.cpp:371]     Train net output #0: loss = 0.0019208 (* 1 = 0.0019208 loss)
I0628 19:12:53.389973  7628 sgd_solver.cpp:137] Iteration 52600, lr = 0.0178125, m = 0.9
I0628 19:12:55.107594  7628 solver.cpp:349] Iteration 52700 (58.2295 iter/s, 1.71734s/100 iter), loss = 0.000886136
I0628 19:12:55.107614  7628 solver.cpp:371]     Train net output #0: loss = 0.000886632 (* 1 = 0.000886632 loss)
I0628 19:12:55.107619  7628 sgd_solver.cpp:137] Iteration 52700, lr = 0.0176562, m = 0.9
I0628 19:12:56.826196  7628 solver.cpp:349] Iteration 52800 (58.1971 iter/s, 1.7183s/100 iter), loss = 0.000856207
I0628 19:12:56.826218  7628 solver.cpp:371]     Train net output #0: loss = 0.000856703 (* 1 = 0.000856703 loss)
I0628 19:12:56.826222  7628 sgd_solver.cpp:137] Iteration 52800, lr = 0.0175, m = 0.9
I0628 19:12:58.547482  7628 solver.cpp:349] Iteration 52900 (58.1064 iter/s, 1.72098s/100 iter), loss = 0.000759574
I0628 19:12:58.547502  7628 solver.cpp:371]     Train net output #0: loss = 0.00076007 (* 1 = 0.00076007 loss)
I0628 19:12:58.547507  7628 sgd_solver.cpp:137] Iteration 52900, lr = 0.0173437, m = 0.9
I0628 19:13:00.246402  7628 solver.cpp:545] Iteration 53000, Testing net (#0)
I0628 19:13:01.252332  7626 data_reader.cpp:262] Starting prefetch of epoch 53
I0628 19:13:01.276298  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:13:01.276310  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:13:01.276315  7628 solver.cpp:630]     Test net output #2: loss = 0.305927 (* 1 = 0.305927 loss)
I0628 19:13:01.276329  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02977s
I0628 19:13:01.293685  7628 solver.cpp:349] Iteration 53000 (36.42 iter/s, 2.74574s/100 iter), loss = 0.000957383
I0628 19:13:01.293709  7628 solver.cpp:371]     Train net output #0: loss = 0.00095788 (* 1 = 0.00095788 loss)
I0628 19:13:01.293714  7628 sgd_solver.cpp:137] Iteration 53000, lr = 0.0171875, m = 0.9
I0628 19:13:03.007062  7628 solver.cpp:349] Iteration 53100 (58.3746 iter/s, 1.71307s/100 iter), loss = 0.00104925
I0628 19:13:03.007086  7628 solver.cpp:371]     Train net output #0: loss = 0.00104974 (* 1 = 0.00104974 loss)
I0628 19:13:03.007089  7628 sgd_solver.cpp:137] Iteration 53100, lr = 0.0170313, m = 0.9
I0628 19:13:03.370810  7613 data_reader.cpp:262] Starting prefetch of epoch 68
I0628 19:13:04.723975  7628 solver.cpp:349] Iteration 53200 (58.2543 iter/s, 1.71661s/100 iter), loss = 0.00124105
I0628 19:13:04.723997  7628 solver.cpp:371]     Train net output #0: loss = 0.00124155 (* 1 = 0.00124155 loss)
I0628 19:13:04.724001  7628 sgd_solver.cpp:137] Iteration 53200, lr = 0.016875, m = 0.9
I0628 19:13:06.438504  7628 solver.cpp:349] Iteration 53300 (58.3354 iter/s, 1.71423s/100 iter), loss = 0.00101389
I0628 19:13:06.438525  7628 solver.cpp:371]     Train net output #0: loss = 0.00101439 (* 1 = 0.00101439 loss)
I0628 19:13:06.438529  7628 sgd_solver.cpp:137] Iteration 53300, lr = 0.0167188, m = 0.9
I0628 19:13:08.154439  7628 solver.cpp:349] Iteration 53400 (58.2875 iter/s, 1.71563s/100 iter), loss = 0.000756935
I0628 19:13:08.154460  7628 solver.cpp:371]     Train net output #0: loss = 0.000757431 (* 1 = 0.000757431 loss)
I0628 19:13:08.154464  7628 sgd_solver.cpp:137] Iteration 53400, lr = 0.0165625, m = 0.9
I0628 19:13:09.868989  7628 solver.cpp:349] Iteration 53500 (58.3346 iter/s, 1.71425s/100 iter), loss = 0.000881029
I0628 19:13:09.869012  7628 solver.cpp:371]     Train net output #0: loss = 0.000881526 (* 1 = 0.000881526 loss)
I0628 19:13:09.869015  7628 sgd_solver.cpp:137] Iteration 53500, lr = 0.0164063, m = 0.9
I0628 19:13:11.582813  7628 solver.cpp:349] Iteration 53600 (58.3595 iter/s, 1.71352s/100 iter), loss = 0.00213722
I0628 19:13:11.582835  7628 solver.cpp:371]     Train net output #0: loss = 0.00213771 (* 1 = 0.00213771 loss)
I0628 19:13:11.582865  7628 sgd_solver.cpp:137] Iteration 53600, lr = 0.01625, m = 0.9
I0628 19:13:13.299230  7628 solver.cpp:349] Iteration 53700 (58.2722 iter/s, 1.71608s/100 iter), loss = 0.00088306
I0628 19:13:13.299254  7628 solver.cpp:371]     Train net output #0: loss = 0.000883556 (* 1 = 0.000883556 loss)
I0628 19:13:13.299259  7628 sgd_solver.cpp:137] Iteration 53700, lr = 0.0160937, m = 0.9
I0628 19:13:15.014704  7628 solver.cpp:349] Iteration 53800 (58.3034 iter/s, 1.71517s/100 iter), loss = 0.00128973
I0628 19:13:15.014727  7628 solver.cpp:371]     Train net output #0: loss = 0.00129022 (* 1 = 0.00129022 loss)
I0628 19:13:15.014731  7628 sgd_solver.cpp:137] Iteration 53800, lr = 0.0159375, m = 0.9
I0628 19:13:16.732082  7628 solver.cpp:349] Iteration 53900 (58.2385 iter/s, 1.71708s/100 iter), loss = 0.00193199
I0628 19:13:16.732106  7628 solver.cpp:371]     Train net output #0: loss = 0.00193248 (* 1 = 0.00193248 loss)
I0628 19:13:16.732110  7628 sgd_solver.cpp:137] Iteration 53900, lr = 0.0157812, m = 0.9
I0628 19:13:16.784112  7613 data_reader.cpp:262] Starting prefetch of epoch 69
I0628 19:13:18.432446  7628 solver.cpp:545] Iteration 54000, Testing net (#0)
I0628 19:13:19.437641  7626 data_reader.cpp:262] Starting prefetch of epoch 54
I0628 19:13:19.459843  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9178
I0628 19:13:19.459857  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:13:19.459864  7628 solver.cpp:630]     Test net output #2: loss = 0.305362 (* 1 = 0.305362 loss)
I0628 19:13:19.459882  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02728s
I0628 19:13:19.477337  7628 solver.cpp:349] Iteration 54000 (36.4326 iter/s, 2.74479s/100 iter), loss = 0.0012479
I0628 19:13:19.477363  7628 solver.cpp:371]     Train net output #0: loss = 0.0012484 (* 1 = 0.0012484 loss)
I0628 19:13:19.477368  7628 sgd_solver.cpp:137] Iteration 54000, lr = 0.015625, m = 0.9
I0628 19:13:21.200141  7628 solver.cpp:349] Iteration 54100 (58.0554 iter/s, 1.72249s/100 iter), loss = 0.00111788
I0628 19:13:21.200163  7628 solver.cpp:371]     Train net output #0: loss = 0.00111838 (* 1 = 0.00111838 loss)
I0628 19:13:21.200167  7628 sgd_solver.cpp:137] Iteration 54100, lr = 0.0154688, m = 0.9
I0628 19:13:22.916797  7628 solver.cpp:349] Iteration 54200 (58.263 iter/s, 1.71635s/100 iter), loss = 0.000801612
I0628 19:13:22.916874  7628 solver.cpp:371]     Train net output #0: loss = 0.000802108 (* 1 = 0.000802108 loss)
I0628 19:13:22.916882  7628 sgd_solver.cpp:137] Iteration 54200, lr = 0.0153125, m = 0.9
I0628 19:13:24.631489  7628 solver.cpp:349] Iteration 54300 (58.3318 iter/s, 1.71433s/100 iter), loss = 0.00134284
I0628 19:13:24.631511  7628 solver.cpp:371]     Train net output #0: loss = 0.00134334 (* 1 = 0.00134334 loss)
I0628 19:13:24.631516  7628 sgd_solver.cpp:137] Iteration 54300, lr = 0.0151563, m = 0.9
I0628 19:13:26.345990  7628 solver.cpp:349] Iteration 54400 (58.3363 iter/s, 1.7142s/100 iter), loss = 0.00063919
I0628 19:13:26.346014  7628 solver.cpp:371]     Train net output #0: loss = 0.000639687 (* 1 = 0.000639687 loss)
I0628 19:13:26.346017  7628 sgd_solver.cpp:137] Iteration 54400, lr = 0.015, m = 0.9
I0628 19:13:28.067685  7628 solver.cpp:349] Iteration 54500 (58.0926 iter/s, 1.72139s/100 iter), loss = 0.00256845
I0628 19:13:28.067706  7628 solver.cpp:371]     Train net output #0: loss = 0.00256894 (* 1 = 0.00256894 loss)
I0628 19:13:28.067711  7628 sgd_solver.cpp:137] Iteration 54500, lr = 0.0148437, m = 0.9
I0628 19:13:29.786275  7628 solver.cpp:349] Iteration 54600 (58.1975 iter/s, 1.71829s/100 iter), loss = 0.00202671
I0628 19:13:29.786298  7628 solver.cpp:371]     Train net output #0: loss = 0.00202721 (* 1 = 0.00202721 loss)
I0628 19:13:29.786301  7628 sgd_solver.cpp:137] Iteration 54600, lr = 0.0146875, m = 0.9
I0628 19:13:31.229182  7613 data_reader.cpp:262] Starting prefetch of epoch 70
I0628 19:13:31.502985  7628 solver.cpp:349] Iteration 54700 (58.2612 iter/s, 1.71641s/100 iter), loss = 0.000649771
I0628 19:13:31.503008  7628 solver.cpp:371]     Train net output #0: loss = 0.000650267 (* 1 = 0.000650267 loss)
I0628 19:13:31.503012  7628 sgd_solver.cpp:137] Iteration 54700, lr = 0.0145312, m = 0.9
I0628 19:13:33.221186  7628 solver.cpp:349] Iteration 54800 (58.2107 iter/s, 1.7179s/100 iter), loss = 0.00210659
I0628 19:13:33.221209  7628 solver.cpp:371]     Train net output #0: loss = 0.00210709 (* 1 = 0.00210709 loss)
I0628 19:13:33.221212  7628 sgd_solver.cpp:137] Iteration 54800, lr = 0.014375, m = 0.9
I0628 19:13:34.935330  7628 solver.cpp:349] Iteration 54900 (58.3485 iter/s, 1.71384s/100 iter), loss = 0.0007056
I0628 19:13:34.935353  7628 solver.cpp:371]     Train net output #0: loss = 0.000706096 (* 1 = 0.000706096 loss)
I0628 19:13:34.935359  7628 sgd_solver.cpp:137] Iteration 54900, lr = 0.0142187, m = 0.9
I0628 19:13:36.635212  7628 solver.cpp:545] Iteration 55000, Testing net (#0)
I0628 19:13:37.642582  7626 data_reader.cpp:262] Starting prefetch of epoch 55
I0628 19:13:37.665961  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9174
I0628 19:13:37.665973  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:13:37.665978  7628 solver.cpp:630]     Test net output #2: loss = 0.305114 (* 1 = 0.305114 loss)
I0628 19:13:37.665992  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03062s
I0628 19:13:37.683476  7628 solver.cpp:349] Iteration 55000 (36.3943 iter/s, 2.74768s/100 iter), loss = 0.00127458
I0628 19:13:37.683501  7628 solver.cpp:371]     Train net output #0: loss = 0.00127508 (* 1 = 0.00127508 loss)
I0628 19:13:37.683506  7628 sgd_solver.cpp:137] Iteration 55000, lr = 0.0140625, m = 0.9
I0628 19:13:39.400568  7628 solver.cpp:349] Iteration 55100 (58.2484 iter/s, 1.71678s/100 iter), loss = 0.001173
I0628 19:13:39.400590  7628 solver.cpp:371]     Train net output #0: loss = 0.0011735 (* 1 = 0.0011735 loss)
I0628 19:13:39.400594  7628 sgd_solver.cpp:137] Iteration 55100, lr = 0.0139063, m = 0.9
I0628 19:13:41.117389  7628 solver.cpp:349] Iteration 55200 (58.2575 iter/s, 1.71652s/100 iter), loss = 0.00150837
I0628 19:13:41.117411  7628 solver.cpp:371]     Train net output #0: loss = 0.00150886 (* 1 = 0.00150886 loss)
I0628 19:13:41.117415  7628 sgd_solver.cpp:137] Iteration 55200, lr = 0.01375, m = 0.9
I0628 19:13:42.834484  7628 solver.cpp:349] Iteration 55300 (58.2481 iter/s, 1.71679s/100 iter), loss = 0.00158128
I0628 19:13:42.834504  7628 solver.cpp:371]     Train net output #0: loss = 0.00158177 (* 1 = 0.00158177 loss)
I0628 19:13:42.834525  7628 sgd_solver.cpp:137] Iteration 55300, lr = 0.0135938, m = 0.9
I0628 19:13:44.551364  7628 solver.cpp:349] Iteration 55400 (58.2561 iter/s, 1.71656s/100 iter), loss = 0.00105118
I0628 19:13:44.551386  7628 solver.cpp:371]     Train net output #0: loss = 0.00105168 (* 1 = 0.00105168 loss)
I0628 19:13:44.551391  7628 sgd_solver.cpp:137] Iteration 55400, lr = 0.0134375, m = 0.9
I0628 19:13:45.666088  7613 data_reader.cpp:262] Starting prefetch of epoch 71
I0628 19:13:46.270015  7628 solver.cpp:349] Iteration 55500 (58.1955 iter/s, 1.71834s/100 iter), loss = 0.000792137
I0628 19:13:46.270036  7628 solver.cpp:371]     Train net output #0: loss = 0.000792633 (* 1 = 0.000792633 loss)
I0628 19:13:46.270040  7628 sgd_solver.cpp:137] Iteration 55500, lr = 0.0132813, m = 0.9
I0628 19:13:47.993338  7628 solver.cpp:349] Iteration 55600 (58.0376 iter/s, 1.72302s/100 iter), loss = 0.0018869
I0628 19:13:47.993360  7628 solver.cpp:371]     Train net output #0: loss = 0.0018874 (* 1 = 0.0018874 loss)
I0628 19:13:47.993362  7628 sgd_solver.cpp:137] Iteration 55600, lr = 0.013125, m = 0.9
I0628 19:13:49.707345  7628 solver.cpp:349] Iteration 55700 (58.3531 iter/s, 1.71371s/100 iter), loss = 0.0013089
I0628 19:13:49.707366  7628 solver.cpp:371]     Train net output #0: loss = 0.0013094 (* 1 = 0.0013094 loss)
I0628 19:13:49.707370  7628 sgd_solver.cpp:137] Iteration 55700, lr = 0.0129687, m = 0.9
I0628 19:13:51.431846  7628 solver.cpp:349] Iteration 55800 (57.998 iter/s, 1.7242s/100 iter), loss = 0.00149494
I0628 19:13:51.431869  7628 solver.cpp:371]     Train net output #0: loss = 0.00149543 (* 1 = 0.00149543 loss)
I0628 19:13:51.431872  7628 sgd_solver.cpp:137] Iteration 55800, lr = 0.0128125, m = 0.9
I0628 19:13:53.147600  7628 solver.cpp:349] Iteration 55900 (58.2938 iter/s, 1.71545s/100 iter), loss = 0.000781932
I0628 19:13:53.147670  7628 solver.cpp:371]     Train net output #0: loss = 0.000782429 (* 1 = 0.000782429 loss)
I0628 19:13:53.147675  7628 sgd_solver.cpp:137] Iteration 55900, lr = 0.0126562, m = 0.9
I0628 19:13:54.845824  7628 solver.cpp:545] Iteration 56000, Testing net (#0)
I0628 19:13:55.850599  7626 data_reader.cpp:262] Starting prefetch of epoch 56
I0628 19:13:55.873781  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9172
I0628 19:13:55.873795  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9966
I0628 19:13:55.873800  7628 solver.cpp:630]     Test net output #2: loss = 0.3025 (* 1 = 0.3025 loss)
I0628 19:13:55.873813  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02783s
I0628 19:13:55.891119  7628 solver.cpp:349] Iteration 56000 (36.4563 iter/s, 2.74301s/100 iter), loss = 0.00162678
I0628 19:13:55.891141  7628 solver.cpp:371]     Train net output #0: loss = 0.00162727 (* 1 = 0.00162727 loss)
I0628 19:13:55.891145  7628 sgd_solver.cpp:137] Iteration 56000, lr = 0.0125, m = 0.9
I0628 19:13:57.613221  7628 solver.cpp:349] Iteration 56100 (58.0788 iter/s, 1.7218s/100 iter), loss = 0.000808359
I0628 19:13:57.613245  7628 solver.cpp:371]     Train net output #0: loss = 0.000808856 (* 1 = 0.000808856 loss)
I0628 19:13:57.613248  7628 sgd_solver.cpp:137] Iteration 56100, lr = 0.0123438, m = 0.9
I0628 19:13:59.334744  7628 solver.cpp:349] Iteration 56200 (58.0983 iter/s, 1.72122s/100 iter), loss = 0.000844953
I0628 19:13:59.334766  7628 solver.cpp:371]     Train net output #0: loss = 0.000845449 (* 1 = 0.000845449 loss)
I0628 19:13:59.334770  7628 sgd_solver.cpp:137] Iteration 56200, lr = 0.0121875, m = 0.9
I0628 19:14:00.124487  7613 data_reader.cpp:262] Starting prefetch of epoch 72
I0628 19:14:01.051942  7628 solver.cpp:349] Iteration 56300 (58.2447 iter/s, 1.71689s/100 iter), loss = 0.000623107
I0628 19:14:01.051964  7628 solver.cpp:371]     Train net output #0: loss = 0.000623603 (* 1 = 0.000623603 loss)
I0628 19:14:01.051967  7628 sgd_solver.cpp:137] Iteration 56300, lr = 0.0120313, m = 0.9
I0628 19:14:02.767623  7628 solver.cpp:349] Iteration 56400 (58.2962 iter/s, 1.71538s/100 iter), loss = 0.000770935
I0628 19:14:02.767647  7628 solver.cpp:371]     Train net output #0: loss = 0.000771431 (* 1 = 0.000771431 loss)
I0628 19:14:02.767650  7628 sgd_solver.cpp:137] Iteration 56400, lr = 0.011875, m = 0.9
I0628 19:14:04.481848  7628 solver.cpp:349] Iteration 56500 (58.3458 iter/s, 1.71392s/100 iter), loss = 0.000951465
I0628 19:14:04.481869  7628 solver.cpp:371]     Train net output #0: loss = 0.000951961 (* 1 = 0.000951961 loss)
I0628 19:14:04.481873  7628 sgd_solver.cpp:137] Iteration 56500, lr = 0.0117188, m = 0.9
I0628 19:14:06.201721  7628 solver.cpp:349] Iteration 56600 (58.154 iter/s, 1.71957s/100 iter), loss = 0.00127786
I0628 19:14:06.201741  7628 solver.cpp:371]     Train net output #0: loss = 0.00127836 (* 1 = 0.00127836 loss)
I0628 19:14:06.201745  7628 sgd_solver.cpp:137] Iteration 56600, lr = 0.0115625, m = 0.9
I0628 19:14:07.916896  7628 solver.cpp:349] Iteration 56700 (58.3133 iter/s, 1.71487s/100 iter), loss = 0.000729334
I0628 19:14:07.916918  7628 solver.cpp:371]     Train net output #0: loss = 0.000729829 (* 1 = 0.000729829 loss)
I0628 19:14:07.916923  7628 sgd_solver.cpp:137] Iteration 56700, lr = 0.0114062, m = 0.9
I0628 19:14:09.635399  7628 solver.cpp:349] Iteration 56800 (58.2004 iter/s, 1.7182s/100 iter), loss = 0.000479912
I0628 19:14:09.635421  7628 solver.cpp:371]     Train net output #0: loss = 0.000480408 (* 1 = 0.000480408 loss)
I0628 19:14:09.635426  7628 sgd_solver.cpp:137] Iteration 56800, lr = 0.01125, m = 0.9
I0628 19:14:11.351325  7628 solver.cpp:349] Iteration 56900 (58.288 iter/s, 1.71562s/100 iter), loss = 0.000857208
I0628 19:14:11.351346  7628 solver.cpp:371]     Train net output #0: loss = 0.000857703 (* 1 = 0.000857703 loss)
I0628 19:14:11.351351  7628 sgd_solver.cpp:137] Iteration 56900, lr = 0.0110937, m = 0.9
I0628 19:14:13.047166  7628 solver.cpp:545] Iteration 57000, Testing net (#0)
I0628 19:14:14.057441  7626 data_reader.cpp:262] Starting prefetch of epoch 57
I0628 19:14:14.077788  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9164
I0628 19:14:14.077800  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9968
I0628 19:14:14.077805  7628 solver.cpp:630]     Test net output #2: loss = 0.303103 (* 1 = 0.303103 loss)
I0628 19:14:14.077817  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03049s
I0628 19:14:14.095127  7628 solver.cpp:349] Iteration 57000 (36.4519 iter/s, 2.74334s/100 iter), loss = 0.0011335
I0628 19:14:14.095150  7628 solver.cpp:371]     Train net output #0: loss = 0.001134 (* 1 = 0.001134 loss)
I0628 19:14:14.095154  7628 sgd_solver.cpp:137] Iteration 57000, lr = 0.0109375, m = 0.9
I0628 19:14:14.576170  7613 data_reader.cpp:262] Starting prefetch of epoch 73
I0628 19:14:15.810928  7628 solver.cpp:349] Iteration 57100 (58.2921 iter/s, 1.7155s/100 iter), loss = 0.000464545
I0628 19:14:15.810952  7628 solver.cpp:371]     Train net output #0: loss = 0.000465041 (* 1 = 0.000465041 loss)
I0628 19:14:15.810956  7628 sgd_solver.cpp:137] Iteration 57100, lr = 0.0107813, m = 0.9
I0628 19:14:17.527192  7628 solver.cpp:349] Iteration 57200 (58.2764 iter/s, 1.71596s/100 iter), loss = 0.000797259
I0628 19:14:17.527217  7628 solver.cpp:371]     Train net output #0: loss = 0.000797755 (* 1 = 0.000797755 loss)
I0628 19:14:17.527222  7628 sgd_solver.cpp:137] Iteration 57200, lr = 0.010625, m = 0.9
I0628 19:14:19.247623  7628 solver.cpp:349] Iteration 57300 (58.1354 iter/s, 1.72012s/100 iter), loss = 0.00145732
I0628 19:14:19.247644  7628 solver.cpp:371]     Train net output #0: loss = 0.00145782 (* 1 = 0.00145782 loss)
I0628 19:14:19.247648  7628 sgd_solver.cpp:137] Iteration 57300, lr = 0.0104688, m = 0.9
I0628 19:14:20.968912  7628 solver.cpp:349] Iteration 57400 (58.1063 iter/s, 1.72098s/100 iter), loss = 0.00124758
I0628 19:14:20.968935  7628 solver.cpp:371]     Train net output #0: loss = 0.00124807 (* 1 = 0.00124807 loss)
I0628 19:14:20.968940  7628 sgd_solver.cpp:137] Iteration 57400, lr = 0.0103125, m = 0.9
I0628 19:14:22.684162  7628 solver.cpp:349] Iteration 57500 (58.3108 iter/s, 1.71495s/100 iter), loss = 0.00128449
I0628 19:14:22.684185  7628 solver.cpp:371]     Train net output #0: loss = 0.00128499 (* 1 = 0.00128499 loss)
I0628 19:14:22.684190  7628 sgd_solver.cpp:137] Iteration 57500, lr = 0.0101563, m = 0.9
I0628 19:14:24.407841  7628 solver.cpp:349] Iteration 57600 (58.0257 iter/s, 1.72338s/100 iter), loss = 0.00138138
I0628 19:14:24.407922  7628 solver.cpp:371]     Train net output #0: loss = 0.00138188 (* 1 = 0.00138188 loss)
I0628 19:14:24.407927  7628 sgd_solver.cpp:137] Iteration 57600, lr = 0.01, m = 0.9
I0628 19:14:26.121163  7628 solver.cpp:349] Iteration 57700 (58.3786 iter/s, 1.71296s/100 iter), loss = 0.00163093
I0628 19:14:26.121186  7628 solver.cpp:371]     Train net output #0: loss = 0.00163143 (* 1 = 0.00163143 loss)
I0628 19:14:26.121189  7628 sgd_solver.cpp:137] Iteration 57700, lr = 0.00984375, m = 0.9
I0628 19:14:27.836798  7628 solver.cpp:349] Iteration 57800 (58.2977 iter/s, 1.71533s/100 iter), loss = 0.0013781
I0628 19:14:27.836822  7628 solver.cpp:371]     Train net output #0: loss = 0.00137859 (* 1 = 0.00137859 loss)
I0628 19:14:27.836825  7628 sgd_solver.cpp:137] Iteration 57800, lr = 0.0096875, m = 0.9
I0628 19:14:27.991729  7613 data_reader.cpp:262] Starting prefetch of epoch 74
I0628 19:14:29.553380  7628 solver.cpp:349] Iteration 57900 (58.2655 iter/s, 1.71628s/100 iter), loss = 0.00126192
I0628 19:14:29.553402  7628 solver.cpp:371]     Train net output #0: loss = 0.00126242 (* 1 = 0.00126242 loss)
I0628 19:14:29.553406  7628 sgd_solver.cpp:137] Iteration 57900, lr = 0.00953125, m = 0.9
I0628 19:14:31.250757  7628 solver.cpp:545] Iteration 58000, Testing net (#0)
I0628 19:14:32.256024  7626 data_reader.cpp:262] Starting prefetch of epoch 58
I0628 19:14:32.280632  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9178
I0628 19:14:32.280644  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:14:32.280649  7628 solver.cpp:630]     Test net output #2: loss = 0.30209 (* 1 = 0.30209 loss)
I0628 19:14:32.280664  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02975s
I0628 19:14:32.297901  7628 solver.cpp:349] Iteration 58000 (36.4424 iter/s, 2.74406s/100 iter), loss = 0.000792703
I0628 19:14:32.297924  7628 solver.cpp:371]     Train net output #0: loss = 0.000793198 (* 1 = 0.000793198 loss)
I0628 19:14:32.297927  7628 sgd_solver.cpp:137] Iteration 58000, lr = 0.009375, m = 0.9
I0628 19:14:34.015252  7628 solver.cpp:349] Iteration 58100 (58.2395 iter/s, 1.71705s/100 iter), loss = 0.000582912
I0628 19:14:34.015275  7628 solver.cpp:371]     Train net output #0: loss = 0.000583407 (* 1 = 0.000583407 loss)
I0628 19:14:34.015280  7628 sgd_solver.cpp:137] Iteration 58100, lr = 0.00921875, m = 0.9
I0628 19:14:35.731423  7628 solver.cpp:349] Iteration 58200 (58.2797 iter/s, 1.71586s/100 iter), loss = 0.00137347
I0628 19:14:35.731446  7628 solver.cpp:371]     Train net output #0: loss = 0.00137396 (* 1 = 0.00137396 loss)
I0628 19:14:35.731451  7628 sgd_solver.cpp:137] Iteration 58200, lr = 0.0090625, m = 0.9
I0628 19:14:37.450810  7628 solver.cpp:349] Iteration 58300 (58.1706 iter/s, 1.71908s/100 iter), loss = 0.00108824
I0628 19:14:37.450834  7628 solver.cpp:371]     Train net output #0: loss = 0.00108873 (* 1 = 0.00108873 loss)
I0628 19:14:37.450837  7628 sgd_solver.cpp:137] Iteration 58300, lr = 0.00890625, m = 0.9
I0628 19:14:39.164475  7628 solver.cpp:349] Iteration 58400 (58.3648 iter/s, 1.71336s/100 iter), loss = 0.000856623
I0628 19:14:39.164499  7628 solver.cpp:371]     Train net output #0: loss = 0.000857118 (* 1 = 0.000857118 loss)
I0628 19:14:39.164502  7628 sgd_solver.cpp:137] Iteration 58400, lr = 0.00875, m = 0.9
I0628 19:14:40.878182  7628 solver.cpp:349] Iteration 58500 (58.3634 iter/s, 1.7134s/100 iter), loss = 0.00123742
I0628 19:14:40.878204  7628 solver.cpp:371]     Train net output #0: loss = 0.00123792 (* 1 = 0.00123792 loss)
I0628 19:14:40.878208  7628 sgd_solver.cpp:137] Iteration 58500, lr = 0.00859375, m = 0.9
I0628 19:14:42.425743  7613 data_reader.cpp:262] Starting prefetch of epoch 75
I0628 19:14:42.596977  7628 solver.cpp:349] Iteration 58600 (58.1905 iter/s, 1.71849s/100 iter), loss = 0.000853292
I0628 19:14:42.597002  7628 solver.cpp:371]     Train net output #0: loss = 0.000853787 (* 1 = 0.000853787 loss)
I0628 19:14:42.597005  7628 sgd_solver.cpp:137] Iteration 58600, lr = 0.0084375, m = 0.9
I0628 19:14:44.314164  7628 solver.cpp:349] Iteration 58700 (58.2451 iter/s, 1.71688s/100 iter), loss = 0.00157045
I0628 19:14:44.314209  7628 solver.cpp:371]     Train net output #0: loss = 0.00157095 (* 1 = 0.00157095 loss)
I0628 19:14:44.314214  7628 sgd_solver.cpp:137] Iteration 58700, lr = 0.00828125, m = 0.9
I0628 19:14:46.032122  7628 solver.cpp:349] Iteration 58800 (58.2197 iter/s, 1.71763s/100 iter), loss = 0.00121077
I0628 19:14:46.032143  7628 solver.cpp:371]     Train net output #0: loss = 0.00121126 (* 1 = 0.00121126 loss)
I0628 19:14:46.032147  7628 sgd_solver.cpp:137] Iteration 58800, lr = 0.008125, m = 0.9
I0628 19:14:47.750437  7628 solver.cpp:349] Iteration 58900 (58.2068 iter/s, 1.71801s/100 iter), loss = 0.000546711
I0628 19:14:47.750458  7628 solver.cpp:371]     Train net output #0: loss = 0.000547207 (* 1 = 0.000547207 loss)
I0628 19:14:47.750461  7628 sgd_solver.cpp:137] Iteration 58900, lr = 0.00796875, m = 0.9
I0628 19:14:49.451138  7628 solver.cpp:545] Iteration 59000, Testing net (#0)
I0628 19:14:50.459617  7626 data_reader.cpp:262] Starting prefetch of epoch 59
I0628 19:14:50.479953  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:14:50.479964  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:14:50.479969  7628 solver.cpp:630]     Test net output #2: loss = 0.302434 (* 1 = 0.302434 loss)
I0628 19:14:50.479981  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02869s
I0628 19:14:50.497279  7628 solver.cpp:349] Iteration 59000 (36.4115 iter/s, 2.74638s/100 iter), loss = 0.0015475
I0628 19:14:50.497303  7628 solver.cpp:371]     Train net output #0: loss = 0.00154799 (* 1 = 0.00154799 loss)
I0628 19:14:50.497306  7628 sgd_solver.cpp:137] Iteration 59000, lr = 0.0078125, m = 0.9
I0628 19:14:52.219291  7628 solver.cpp:349] Iteration 59100 (58.0819 iter/s, 1.72171s/100 iter), loss = 0.000893756
I0628 19:14:52.219311  7628 solver.cpp:371]     Train net output #0: loss = 0.000894251 (* 1 = 0.000894251 loss)
I0628 19:14:52.219316  7628 sgd_solver.cpp:137] Iteration 59100, lr = 0.00765625, m = 0.9
I0628 19:14:53.935809  7628 solver.cpp:349] Iteration 59200 (58.2677 iter/s, 1.71622s/100 iter), loss = 0.000492424
I0628 19:14:53.935832  7628 solver.cpp:371]     Train net output #0: loss = 0.000492919 (* 1 = 0.000492919 loss)
I0628 19:14:53.935837  7628 sgd_solver.cpp:137] Iteration 59200, lr = 0.0075, m = 0.9
I0628 19:14:55.650110  7628 solver.cpp:349] Iteration 59300 (58.3432 iter/s, 1.71399s/100 iter), loss = 0.00119854
I0628 19:14:55.650161  7628 solver.cpp:371]     Train net output #0: loss = 0.00119904 (* 1 = 0.00119904 loss)
I0628 19:14:55.650168  7628 sgd_solver.cpp:137] Iteration 59300, lr = 0.00734375, m = 0.9
I0628 19:14:56.871307  7613 data_reader.cpp:262] Starting prefetch of epoch 76
I0628 19:14:57.369377  7628 solver.cpp:349] Iteration 59400 (58.1758 iter/s, 1.71893s/100 iter), loss = 0.00127121
I0628 19:14:57.369400  7628 solver.cpp:371]     Train net output #0: loss = 0.00127171 (* 1 = 0.00127171 loss)
I0628 19:14:57.369405  7628 sgd_solver.cpp:137] Iteration 59400, lr = 0.0071875, m = 0.9
I0628 19:14:59.089146  7628 solver.cpp:349] Iteration 59500 (58.1577 iter/s, 1.71946s/100 iter), loss = 0.00142835
I0628 19:14:59.089169  7628 solver.cpp:371]     Train net output #0: loss = 0.00142885 (* 1 = 0.00142885 loss)
I0628 19:14:59.089175  7628 sgd_solver.cpp:137] Iteration 59500, lr = 0.00703125, m = 0.9
I0628 19:15:00.807771  7628 solver.cpp:349] Iteration 59600 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.0014177
I0628 19:15:00.807795  7628 solver.cpp:371]     Train net output #0: loss = 0.00141819 (* 1 = 0.00141819 loss)
I0628 19:15:00.807798  7628 sgd_solver.cpp:137] Iteration 59600, lr = 0.006875, m = 0.9
I0628 19:15:02.522892  7628 solver.cpp:349] Iteration 59700 (58.3152 iter/s, 1.71482s/100 iter), loss = 0.0012766
I0628 19:15:02.522915  7628 solver.cpp:371]     Train net output #0: loss = 0.0012771 (* 1 = 0.0012771 loss)
I0628 19:15:02.522919  7628 sgd_solver.cpp:137] Iteration 59700, lr = 0.00671875, m = 0.9
I0628 19:15:04.239603  7628 solver.cpp:349] Iteration 59800 (58.2613 iter/s, 1.71641s/100 iter), loss = 0.000773642
I0628 19:15:04.239622  7628 solver.cpp:371]     Train net output #0: loss = 0.000774138 (* 1 = 0.000774138 loss)
I0628 19:15:04.239626  7628 sgd_solver.cpp:137] Iteration 59800, lr = 0.0065625, m = 0.9
I0628 19:15:05.959028  7628 solver.cpp:349] Iteration 59900 (58.1691 iter/s, 1.71913s/100 iter), loss = 0.0019883
I0628 19:15:05.959049  7628 solver.cpp:371]     Train net output #0: loss = 0.0019888 (* 1 = 0.0019888 loss)
I0628 19:15:05.959053  7628 sgd_solver.cpp:137] Iteration 59900, lr = 0.00640625, m = 0.9
I0628 19:15:07.659164  7628 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_60000.caffemodel
I0628 19:15:07.667017  7628 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_60000.solverstate
I0628 19:15:07.670486  7628 solver.cpp:545] Iteration 60000, Testing net (#0)
I0628 19:15:08.675168  7626 data_reader.cpp:262] Starting prefetch of epoch 60
I0628 19:15:08.695619  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:15:08.695636  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:15:08.695642  7628 solver.cpp:630]     Test net output #2: loss = 0.300749 (* 1 = 0.300749 loss)
I0628 19:15:08.695655  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02501s
I0628 19:15:08.714131  7628 solver.cpp:349] Iteration 60000 (36.3024 iter/s, 2.75464s/100 iter), loss = 0.000588658
I0628 19:15:08.714161  7628 solver.cpp:371]     Train net output #0: loss = 0.000589153 (* 1 = 0.000589153 loss)
I0628 19:15:08.714165  7628 sgd_solver.cpp:137] Iteration 60000, lr = 0.00625, m = 0.9
I0628 19:15:10.429891  7628 solver.cpp:349] Iteration 60100 (58.2938 iter/s, 1.71545s/100 iter), loss = 0.000986898
I0628 19:15:10.429913  7628 solver.cpp:371]     Train net output #0: loss = 0.000987393 (* 1 = 0.000987393 loss)
I0628 19:15:10.429916  7628 sgd_solver.cpp:137] Iteration 60100, lr = 0.00609375, m = 0.9
I0628 19:15:11.345257  7613 data_reader.cpp:262] Starting prefetch of epoch 77
I0628 19:15:12.149569  7628 solver.cpp:349] Iteration 60200 (58.1607 iter/s, 1.71937s/100 iter), loss = 0.000731806
I0628 19:15:12.149591  7628 solver.cpp:371]     Train net output #0: loss = 0.000732302 (* 1 = 0.000732302 loss)
I0628 19:15:12.149595  7628 sgd_solver.cpp:137] Iteration 60200, lr = 0.0059375, m = 0.9
I0628 19:15:13.867714  7628 solver.cpp:349] Iteration 60300 (58.213 iter/s, 1.71783s/100 iter), loss = 0.001069
I0628 19:15:13.867738  7628 solver.cpp:371]     Train net output #0: loss = 0.0010695 (* 1 = 0.0010695 loss)
I0628 19:15:13.867741  7628 sgd_solver.cpp:137] Iteration 60300, lr = 0.00578125, m = 0.9
I0628 19:15:15.583225  7628 solver.cpp:349] Iteration 60400 (58.302 iter/s, 1.71521s/100 iter), loss = 0.00150259
I0628 19:15:15.583246  7628 solver.cpp:371]     Train net output #0: loss = 0.00150308 (* 1 = 0.00150308 loss)
I0628 19:15:15.583251  7628 sgd_solver.cpp:137] Iteration 60400, lr = 0.005625, m = 0.9
I0628 19:15:17.298167  7628 solver.cpp:349] Iteration 60500 (58.3213 iter/s, 1.71464s/100 iter), loss = 0.00127904
I0628 19:15:17.298190  7628 solver.cpp:371]     Train net output #0: loss = 0.00127953 (* 1 = 0.00127953 loss)
I0628 19:15:17.298194  7628 sgd_solver.cpp:137] Iteration 60500, lr = 0.00546875, m = 0.9
I0628 19:15:19.010416  7628 solver.cpp:349] Iteration 60600 (58.4131 iter/s, 1.71195s/100 iter), loss = 0.00191661
I0628 19:15:19.010438  7628 solver.cpp:371]     Train net output #0: loss = 0.00191711 (* 1 = 0.00191711 loss)
I0628 19:15:19.010442  7628 sgd_solver.cpp:137] Iteration 60600, lr = 0.0053125, m = 0.9
I0628 19:15:20.724943  7628 solver.cpp:349] Iteration 60700 (58.3354 iter/s, 1.71422s/100 iter), loss = 0.000639156
I0628 19:15:20.724966  7628 solver.cpp:371]     Train net output #0: loss = 0.000639651 (* 1 = 0.000639651 loss)
I0628 19:15:20.724969  7628 sgd_solver.cpp:137] Iteration 60700, lr = 0.00515625, m = 0.9
I0628 19:15:22.444485  7628 solver.cpp:349] Iteration 60800 (58.1652 iter/s, 1.71924s/100 iter), loss = 0.00112943
I0628 19:15:22.444506  7628 solver.cpp:371]     Train net output #0: loss = 0.00112993 (* 1 = 0.00112993 loss)
I0628 19:15:22.444511  7628 sgd_solver.cpp:137] Iteration 60800, lr = 0.005, m = 0.9
I0628 19:15:24.161047  7628 solver.cpp:349] Iteration 60900 (58.2663 iter/s, 1.71626s/100 iter), loss = 0.00110679
I0628 19:15:24.161070  7628 solver.cpp:371]     Train net output #0: loss = 0.00110729 (* 1 = 0.00110729 loss)
I0628 19:15:24.161074  7628 sgd_solver.cpp:137] Iteration 60900, lr = 0.00484375, m = 0.9
I0628 19:15:24.743489  7613 data_reader.cpp:262] Starting prefetch of epoch 78
I0628 19:15:25.861317  7628 solver.cpp:545] Iteration 61000, Testing net (#0)
I0628 19:15:26.867300  7626 data_reader.cpp:262] Starting prefetch of epoch 61
I0628 19:15:26.888324  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9164
I0628 19:15:26.888337  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:15:26.888344  7628 solver.cpp:630]     Test net output #2: loss = 0.301281 (* 1 = 0.301281 loss)
I0628 19:15:26.888360  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02688s
I0628 19:15:26.905768  7628 solver.cpp:349] Iteration 61000 (36.4397 iter/s, 2.74426s/100 iter), loss = 0.00231112
I0628 19:15:26.905791  7628 solver.cpp:371]     Train net output #0: loss = 0.00231162 (* 1 = 0.00231162 loss)
I0628 19:15:26.905794  7628 sgd_solver.cpp:137] Iteration 61000, lr = 0.0046875, m = 0.9
I0628 19:15:28.627837  7628 solver.cpp:349] Iteration 61100 (58.0799 iter/s, 1.72177s/100 iter), loss = 0.0010903
I0628 19:15:28.627862  7628 solver.cpp:371]     Train net output #0: loss = 0.0010908 (* 1 = 0.0010908 loss)
I0628 19:15:28.627867  7628 sgd_solver.cpp:137] Iteration 61100, lr = 0.00453125, m = 0.9
I0628 19:15:30.346869  7628 solver.cpp:349] Iteration 61200 (58.1827 iter/s, 1.71872s/100 iter), loss = 0.00116113
I0628 19:15:30.346891  7628 solver.cpp:371]     Train net output #0: loss = 0.00116163 (* 1 = 0.00116163 loss)
I0628 19:15:30.346895  7628 sgd_solver.cpp:137] Iteration 61200, lr = 0.004375, m = 0.9
I0628 19:15:32.063922  7628 solver.cpp:349] Iteration 61300 (58.2496 iter/s, 1.71675s/100 iter), loss = 0.000634547
I0628 19:15:32.063944  7628 solver.cpp:371]     Train net output #0: loss = 0.000635042 (* 1 = 0.000635042 loss)
I0628 19:15:32.063948  7628 sgd_solver.cpp:137] Iteration 61300, lr = 0.00421875, m = 0.9
I0628 19:15:33.778012  7628 solver.cpp:349] Iteration 61400 (58.3503 iter/s, 1.71379s/100 iter), loss = 0.00123137
I0628 19:15:33.778034  7628 solver.cpp:371]     Train net output #0: loss = 0.00123187 (* 1 = 0.00123187 loss)
I0628 19:15:33.778038  7628 sgd_solver.cpp:137] Iteration 61400, lr = 0.0040625, m = 0.9
I0628 19:15:35.494246  7628 solver.cpp:349] Iteration 61500 (58.2775 iter/s, 1.71593s/100 iter), loss = 0.00100535
I0628 19:15:35.494268  7628 solver.cpp:371]     Train net output #0: loss = 0.00100585 (* 1 = 0.00100585 loss)
I0628 19:15:35.494272  7628 sgd_solver.cpp:137] Iteration 61500, lr = 0.00390625, m = 0.9
I0628 19:15:37.215955  7628 solver.cpp:349] Iteration 61600 (58.0922 iter/s, 1.7214s/100 iter), loss = 0.00180023
I0628 19:15:37.215975  7628 solver.cpp:371]     Train net output #0: loss = 0.00180073 (* 1 = 0.00180073 loss)
I0628 19:15:37.215981  7628 sgd_solver.cpp:137] Iteration 61600, lr = 0.00375, m = 0.9
I0628 19:15:38.929664  7628 solver.cpp:349] Iteration 61700 (58.3632 iter/s, 1.71341s/100 iter), loss = 0.00148542
I0628 19:15:38.929687  7628 solver.cpp:371]     Train net output #0: loss = 0.00148591 (* 1 = 0.00148591 loss)
I0628 19:15:38.929690  7628 sgd_solver.cpp:137] Iteration 61700, lr = 0.00359375, m = 0.9
I0628 19:15:39.187254  7613 data_reader.cpp:262] Starting prefetch of epoch 79
I0628 19:15:40.647749  7628 solver.cpp:349] Iteration 61800 (58.2147 iter/s, 1.71778s/100 iter), loss = 0.00118566
I0628 19:15:40.647773  7628 solver.cpp:371]     Train net output #0: loss = 0.00118616 (* 1 = 0.00118616 loss)
I0628 19:15:40.647778  7628 sgd_solver.cpp:137] Iteration 61800, lr = 0.0034375, m = 0.9
I0628 19:15:42.371969  7628 solver.cpp:349] Iteration 61900 (58.0077 iter/s, 1.72391s/100 iter), loss = 0.000947787
I0628 19:15:42.371994  7628 solver.cpp:371]     Train net output #0: loss = 0.000948282 (* 1 = 0.000948282 loss)
I0628 19:15:42.372001  7628 sgd_solver.cpp:137] Iteration 61900, lr = 0.00328125, m = 0.9
I0628 19:15:44.076521  7628 solver.cpp:545] Iteration 62000, Testing net (#0)
I0628 19:15:45.082273  7626 data_reader.cpp:262] Starting prefetch of epoch 62
I0628 19:15:45.108429  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9174
I0628 19:15:45.108449  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:15:45.108455  7628 solver.cpp:630]     Test net output #2: loss = 0.301195 (* 1 = 0.301195 loss)
I0628 19:15:45.108486  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.03181s
I0628 19:15:45.125819  7628 solver.cpp:349] Iteration 62000 (36.319 iter/s, 2.75338s/100 iter), loss = 0.001339
I0628 19:15:45.125835  7628 solver.cpp:371]     Train net output #0: loss = 0.00133949 (* 1 = 0.00133949 loss)
I0628 19:15:45.125839  7628 sgd_solver.cpp:137] Iteration 62000, lr = 0.003125, m = 0.9
I0628 19:15:46.840582  7628 solver.cpp:349] Iteration 62100 (58.3272 iter/s, 1.71447s/100 iter), loss = 0.00165798
I0628 19:15:46.840606  7628 solver.cpp:371]     Train net output #0: loss = 0.00165847 (* 1 = 0.00165847 loss)
I0628 19:15:46.840610  7628 sgd_solver.cpp:137] Iteration 62100, lr = 0.00296875, m = 0.9
I0628 19:15:48.557597  7628 solver.cpp:349] Iteration 62200 (58.251 iter/s, 1.71671s/100 iter), loss = 0.00075679
I0628 19:15:48.557620  7628 solver.cpp:371]     Train net output #0: loss = 0.000757284 (* 1 = 0.000757284 loss)
I0628 19:15:48.557623  7628 sgd_solver.cpp:137] Iteration 62200, lr = 0.0028125, m = 0.9
I0628 19:15:50.272292  7628 solver.cpp:349] Iteration 62300 (58.3297 iter/s, 1.71439s/100 iter), loss = 0.00135423
I0628 19:15:50.272312  7628 solver.cpp:371]     Train net output #0: loss = 0.00135473 (* 1 = 0.00135473 loss)
I0628 19:15:50.272316  7628 sgd_solver.cpp:137] Iteration 62300, lr = 0.00265625, m = 0.9
I0628 19:15:51.998705  7628 solver.cpp:349] Iteration 62400 (57.9337 iter/s, 1.72611s/100 iter), loss = 0.00063447
I0628 19:15:51.998733  7628 solver.cpp:371]     Train net output #0: loss = 0.000634965 (* 1 = 0.000634965 loss)
I0628 19:15:51.998739  7628 sgd_solver.cpp:137] Iteration 62400, lr = 0.0025, m = 0.9
I0628 19:15:53.652981  7613 data_reader.cpp:262] Starting prefetch of epoch 80
I0628 19:15:53.721344  7628 solver.cpp:349] Iteration 62500 (58.0611 iter/s, 1.72232s/100 iter), loss = 0.0012559
I0628 19:15:53.721369  7628 solver.cpp:371]     Train net output #0: loss = 0.0012564 (* 1 = 0.0012564 loss)
I0628 19:15:53.721372  7628 sgd_solver.cpp:137] Iteration 62500, lr = 0.00234375, m = 0.9
I0628 19:15:55.435757  7628 solver.cpp:349] Iteration 62600 (58.3394 iter/s, 1.71411s/100 iter), loss = 0.000651107
I0628 19:15:55.435778  7628 solver.cpp:371]     Train net output #0: loss = 0.000651601 (* 1 = 0.000651601 loss)
I0628 19:15:55.435782  7628 sgd_solver.cpp:137] Iteration 62600, lr = 0.0021875, m = 0.9
I0628 19:15:57.152112  7628 solver.cpp:349] Iteration 62700 (58.2733 iter/s, 1.71605s/100 iter), loss = 0.00214806
I0628 19:15:57.152189  7628 solver.cpp:371]     Train net output #0: loss = 0.00214855 (* 1 = 0.00214855 loss)
I0628 19:15:57.152194  7628 sgd_solver.cpp:137] Iteration 62700, lr = 0.00203125, m = 0.9
I0628 19:15:58.866389  7628 solver.cpp:349] Iteration 62800 (58.3458 iter/s, 1.71392s/100 iter), loss = 0.00167872
I0628 19:15:58.866410  7628 solver.cpp:371]     Train net output #0: loss = 0.00167921 (* 1 = 0.00167921 loss)
I0628 19:15:58.866415  7628 sgd_solver.cpp:137] Iteration 62800, lr = 0.001875, m = 0.9
I0628 19:16:00.580729  7628 solver.cpp:349] Iteration 62900 (58.3418 iter/s, 1.71404s/100 iter), loss = 0.00144808
I0628 19:16:00.580752  7628 solver.cpp:371]     Train net output #0: loss = 0.00144857 (* 1 = 0.00144857 loss)
I0628 19:16:00.580755  7628 sgd_solver.cpp:137] Iteration 62900, lr = 0.00171875, m = 0.9
I0628 19:16:02.279505  7628 solver.cpp:545] Iteration 63000, Testing net (#0)
I0628 19:16:03.284483  7626 data_reader.cpp:262] Starting prefetch of epoch 63
I0628 19:16:03.308208  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9184
I0628 19:16:03.308228  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:16:03.308233  7628 solver.cpp:630]     Test net output #2: loss = 0.302289 (* 1 = 0.302289 loss)
I0628 19:16:03.308246  7628 solver.cpp:305] [MultiGPU] Tests completed in 1.02858s
I0628 19:16:03.325546  7628 solver.cpp:349] Iteration 63000 (36.4384 iter/s, 2.74436s/100 iter), loss = 0.000795351
I0628 19:16:03.325577  7628 solver.cpp:371]     Train net output #0: loss = 0.000795846 (* 1 = 0.000795846 loss)
I0628 19:16:03.325583  7628 sgd_solver.cpp:137] Iteration 63000, lr = 0.0015625, m = 0.9
I0628 19:16:05.039954  7628 solver.cpp:349] Iteration 63100 (58.34 iter/s, 1.71409s/100 iter), loss = 0.0010553
I0628 19:16:05.039975  7628 solver.cpp:371]     Train net output #0: loss = 0.0010558 (* 1 = 0.0010558 loss)
I0628 19:16:05.039979  7628 sgd_solver.cpp:137] Iteration 63100, lr = 0.00140625, m = 0.9
I0628 19:16:06.756326  7628 solver.cpp:349] Iteration 63200 (58.2726 iter/s, 1.71607s/100 iter), loss = 0.00124562
I0628 19:16:06.756350  7628 solver.cpp:371]     Train net output #0: loss = 0.00124611 (* 1 = 0.00124611 loss)
I0628 19:16:06.756353  7628 sgd_solver.cpp:137] Iteration 63200, lr = 0.00125, m = 0.9
I0628 19:16:08.095125  7613 data_reader.cpp:262] Starting prefetch of epoch 81
I0628 19:16:08.482486  7628 solver.cpp:349] Iteration 63300 (57.9423 iter/s, 1.72585s/100 iter), loss = 0.000814115
I0628 19:16:08.482509  7628 solver.cpp:371]     Train net output #0: loss = 0.00081461 (* 1 = 0.00081461 loss)
I0628 19:16:08.482516  7628 sgd_solver.cpp:137] Iteration 63300, lr = 0.00109375, m = 0.9
I0628 19:16:10.196393  7628 solver.cpp:349] Iteration 63400 (58.3567 iter/s, 1.7136s/100 iter), loss = 0.00121793
I0628 19:16:10.196416  7628 solver.cpp:371]     Train net output #0: loss = 0.00121843 (* 1 = 0.00121843 loss)
I0628 19:16:10.196422  7628 sgd_solver.cpp:137] Iteration 63400, lr = 0.000937498, m = 0.9
I0628 19:16:11.916324  7628 solver.cpp:349] Iteration 63500 (58.1523 iter/s, 1.71962s/100 iter), loss = 0.0019167
I0628 19:16:11.916347  7628 solver.cpp:371]     Train net output #0: loss = 0.00191719 (* 1 = 0.00191719 loss)
I0628 19:16:11.916352  7628 sgd_solver.cpp:137] Iteration 63500, lr = 0.00078125, m = 0.9
I0628 19:16:13.631709  7628 solver.cpp:349] Iteration 63600 (58.3062 iter/s, 1.71508s/100 iter), loss = 0.00234713
I0628 19:16:13.631732  7628 solver.cpp:371]     Train net output #0: loss = 0.00234762 (* 1 = 0.00234762 loss)
I0628 19:16:13.631736  7628 sgd_solver.cpp:137] Iteration 63600, lr = 0.000625002, m = 0.9
I0628 19:16:15.347185  7628 solver.cpp:349] Iteration 63700 (58.3032 iter/s, 1.71517s/100 iter), loss = 0.00175486
I0628 19:16:15.347208  7628 solver.cpp:371]     Train net output #0: loss = 0.00175536 (* 1 = 0.00175536 loss)
I0628 19:16:15.347211  7628 sgd_solver.cpp:137] Iteration 63700, lr = 0.000468749, m = 0.9
I0628 19:16:17.064702  7628 solver.cpp:349] Iteration 63800 (58.2339 iter/s, 1.71721s/100 iter), loss = 0.00118228
I0628 19:16:17.064726  7628 solver.cpp:371]     Train net output #0: loss = 0.00118277 (* 1 = 0.00118277 loss)
I0628 19:16:17.064749  7628 sgd_solver.cpp:137] Iteration 63800, lr = 0.000312501, m = 0.9
I0628 19:16:18.783427  7628 solver.cpp:349] Iteration 63900 (58.1937 iter/s, 1.7184s/100 iter), loss = 0.00138932
I0628 19:16:18.783448  7628 solver.cpp:371]     Train net output #0: loss = 0.00138981 (* 1 = 0.00138981 loss)
I0628 19:16:18.783452  7628 sgd_solver.cpp:137] Iteration 63900, lr = 0.000156248, m = 0.9
I0628 19:16:20.487411  7628 solver.cpp:349] Iteration 63999 (58.1094 iter/s, 1.70368s/100 iter), loss = 0.000932424
I0628 19:16:20.487433  7628 solver.cpp:371]     Train net output #0: loss = 0.000932919 (* 1 = 0.000932919 loss)
I0628 19:16:20.487496  7628 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0628 19:16:20.495379  7628 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_64000.solverstate
I0628 19:16:20.505228  7628 solver.cpp:522] Iteration 64000, loss = 0.000714081
I0628 19:16:20.505247  7628 solver.cpp:545] Iteration 64000, Testing net (#0)
I0628 19:16:21.511070  7626 data_reader.cpp:262] Starting prefetch of epoch 64
I0628 19:16:21.531349  7628 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9166
I0628 19:16:21.531363  7628 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:16:21.531368  7628 solver.cpp:630]     Test net output #2: loss = 0.300819 (* 1 = 0.300819 loss)
I0628 19:16:21.532887  7562 parallel.cpp:71] Root Solver performance on device 0: 55.01 * 32 = 1760 img/sec
I0628 19:16:21.532897  7562 parallel.cpp:76]      Solver performance on device 1: 55.01 * 32 = 1760 img/sec
I0628 19:16:21.532899  7562 parallel.cpp:79] Overall multi-GPU performance: 3520.43 img/sec
I0628 19:16:21.568742  7562 caffe.cpp:247] Optimization Done in 19m 28s
training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse
I0628 19:16:22.766230 32034 caffe.cpp:608] This is NVCaffe 0.16.2 started at Wed Jun 28 19:16:22 2017
I0628 19:16:22.766347 32034 caffe.cpp:611] CuDNN version: 6.0.21
I0628 19:16:22.766351 32034 caffe.cpp:612] CuBLAS version: 8000
I0628 19:16:22.766353 32034 caffe.cpp:613] CUDA version: 8000
I0628 19:16:22.766355 32034 caffe.cpp:614] CUDA driver version: 8000
I0628 19:16:22.885723 32034 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0628 19:16:22.886231 32034 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8277393408, dev_info[0]: total=8506769408 free=8277393408
I0628 19:16:22.886687 32034 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8277393408, dev_info[1]: total=8508145664 free=8379236352
I0628 19:16:22.886698 32034 caffe.cpp:208] Using GPUs 0, 1
I0628 19:16:22.886965 32034 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0628 19:16:22.887228 32034 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0628 19:16:22.887264 32034 solver.cpp:42] Solver data type: FLOAT
I0628 19:16:22.887300 32034 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0628 19:16:22.893164 32034 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/train.prototxt
I0628 19:16:22.893627 32034 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0628 19:16:22.893635 32034 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0628 19:16:22.893822 32034 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 32
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0628 19:16:22.893920 32034 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:22.893924 32034 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:22.893928 32034 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0628 19:16:22.893930 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:22.893982 32034 net.cpp:183] Created Layer data (0)
I0628 19:16:22.893990 32034 net.cpp:529] data -> data
I0628 19:16:22.894001 32034 net.cpp:529] data -> label
I0628 19:16:22.894023 32034 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 32
I0628 19:16:22.894042 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:22.894714 32050 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0628 19:16:22.895442 32034 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:22.895496 32034 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:22.895503 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:22.895521 32034 net.cpp:244] Setting up data
I0628 19:16:22.895532 32034 net.cpp:251] TRAIN Top shape for layer 0 'data' 32 3 32 32 (98304)
I0628 19:16:22.895542 32034 net.cpp:251] TRAIN Top shape for layer 0 'data' 32 (32)
I0628 19:16:22.895547 32034 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0628 19:16:22.895551 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:22.895560 32034 net.cpp:183] Created Layer data/bias (1)
I0628 19:16:22.895563 32034 net.cpp:560] data/bias <- data
I0628 19:16:22.895570 32034 net.cpp:529] data/bias -> data/bias
I0628 19:16:22.897207 32034 net.cpp:244] Setting up data/bias
I0628 19:16:22.897217 32034 net.cpp:251] TRAIN Top shape for layer 1 'data/bias' 32 3 32 32 (98304)
I0628 19:16:22.897225 32034 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0628 19:16:22.897228 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:22.897243 32034 net.cpp:183] Created Layer conv1a (2)
I0628 19:16:22.897245 32034 net.cpp:560] conv1a <- data/bias
I0628 19:16:22.897248 32034 net.cpp:529] conv1a -> conv1a
I0628 19:16:23.180197 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0628 19:16:23.180227 32034 net.cpp:244] Setting up conv1a
I0628 19:16:23.180236 32034 net.cpp:251] TRAIN Top shape for layer 2 'conv1a' 32 32 32 32 (1048576)
I0628 19:16:23.180248 32034 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0628 19:16:23.180255 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.180269 32034 net.cpp:183] Created Layer conv1a/bn (3)
I0628 19:16:23.180272 32034 net.cpp:560] conv1a/bn <- conv1a
I0628 19:16:23.180276 32034 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0628 19:16:23.180835 32034 net.cpp:244] Setting up conv1a/bn
I0628 19:16:23.180842 32034 net.cpp:251] TRAIN Top shape for layer 3 'conv1a/bn' 32 32 32 32 (1048576)
I0628 19:16:23.180850 32034 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0628 19:16:23.180852 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.180856 32034 net.cpp:183] Created Layer conv1a/relu (4)
I0628 19:16:23.180860 32034 net.cpp:560] conv1a/relu <- conv1a
I0628 19:16:23.180861 32034 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0628 19:16:23.180876 32034 net.cpp:244] Setting up conv1a/relu
I0628 19:16:23.180881 32034 net.cpp:251] TRAIN Top shape for layer 4 'conv1a/relu' 32 32 32 32 (1048576)
I0628 19:16:23.180882 32034 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0628 19:16:23.180886 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.180896 32034 net.cpp:183] Created Layer conv1b (5)
I0628 19:16:23.180898 32034 net.cpp:560] conv1b <- conv1a
I0628 19:16:23.180902 32034 net.cpp:529] conv1b -> conv1b
I0628 19:16:23.187296 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0628 19:16:23.187315 32034 net.cpp:244] Setting up conv1b
I0628 19:16:23.187324 32034 net.cpp:251] TRAIN Top shape for layer 5 'conv1b' 32 32 32 32 (1048576)
I0628 19:16:23.187335 32034 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0628 19:16:23.187341 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.187350 32034 net.cpp:183] Created Layer conv1b/bn (6)
I0628 19:16:23.187355 32034 net.cpp:560] conv1b/bn <- conv1b
I0628 19:16:23.187360 32034 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0628 19:16:23.187897 32034 net.cpp:244] Setting up conv1b/bn
I0628 19:16:23.187906 32034 net.cpp:251] TRAIN Top shape for layer 6 'conv1b/bn' 32 32 32 32 (1048576)
I0628 19:16:23.187924 32034 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0628 19:16:23.187929 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.187935 32034 net.cpp:183] Created Layer conv1b/relu (7)
I0628 19:16:23.187940 32034 net.cpp:560] conv1b/relu <- conv1b
I0628 19:16:23.187947 32034 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0628 19:16:23.187952 32034 net.cpp:244] Setting up conv1b/relu
I0628 19:16:23.187958 32034 net.cpp:251] TRAIN Top shape for layer 7 'conv1b/relu' 32 32 32 32 (1048576)
I0628 19:16:23.187963 32034 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0628 19:16:23.187968 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.187976 32034 net.cpp:183] Created Layer pool1 (8)
I0628 19:16:23.187980 32034 net.cpp:560] pool1 <- conv1b
I0628 19:16:23.187984 32034 net.cpp:529] pool1 -> pool1
I0628 19:16:23.188047 32034 net.cpp:244] Setting up pool1
I0628 19:16:23.188053 32034 net.cpp:251] TRAIN Top shape for layer 8 'pool1' 32 32 32 32 (1048576)
I0628 19:16:23.188057 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0628 19:16:23.188062 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.188073 32034 net.cpp:183] Created Layer res2a_branch2a (9)
I0628 19:16:23.188077 32034 net.cpp:560] res2a_branch2a <- pool1
I0628 19:16:23.188081 32034 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0628 19:16:23.197912 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.11G, req 0G)
I0628 19:16:23.197935 32034 net.cpp:244] Setting up res2a_branch2a
I0628 19:16:23.197945 32034 net.cpp:251] TRAIN Top shape for layer 9 'res2a_branch2a' 32 64 32 32 (2097152)
I0628 19:16:23.197958 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.197964 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.197974 32034 net.cpp:183] Created Layer res2a_branch2a/bn (10)
I0628 19:16:23.197979 32034 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0628 19:16:23.197983 32034 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0628 19:16:23.198557 32034 net.cpp:244] Setting up res2a_branch2a/bn
I0628 19:16:23.198565 32034 net.cpp:251] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 32 64 32 32 (2097152)
I0628 19:16:23.198573 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.198580 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.198585 32034 net.cpp:183] Created Layer res2a_branch2a/relu (11)
I0628 19:16:23.198588 32034 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0628 19:16:23.198592 32034 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0628 19:16:23.198598 32034 net.cpp:244] Setting up res2a_branch2a/relu
I0628 19:16:23.198604 32034 net.cpp:251] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 32 64 32 32 (2097152)
I0628 19:16:23.198608 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0628 19:16:23.198612 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.198623 32034 net.cpp:183] Created Layer res2a_branch2b (12)
I0628 19:16:23.198626 32034 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0628 19:16:23.198629 32034 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0628 19:16:23.204071 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.09G, req 0G)
I0628 19:16:23.204092 32034 net.cpp:244] Setting up res2a_branch2b
I0628 19:16:23.204098 32034 net.cpp:251] TRAIN Top shape for layer 12 'res2a_branch2b' 32 64 32 32 (2097152)
I0628 19:16:23.204107 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.204121 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204131 32034 net.cpp:183] Created Layer res2a_branch2b/bn (13)
I0628 19:16:23.204135 32034 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0628 19:16:23.204139 32034 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0628 19:16:23.204690 32034 net.cpp:244] Setting up res2a_branch2b/bn
I0628 19:16:23.204699 32034 net.cpp:251] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 32 64 32 32 (2097152)
I0628 19:16:23.204706 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.204711 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204717 32034 net.cpp:183] Created Layer res2a_branch2b/relu (14)
I0628 19:16:23.204721 32034 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0628 19:16:23.204726 32034 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0628 19:16:23.204733 32034 net.cpp:244] Setting up res2a_branch2b/relu
I0628 19:16:23.204738 32034 net.cpp:251] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 32 64 32 32 (2097152)
I0628 19:16:23.204743 32034 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0628 19:16:23.204747 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204754 32034 net.cpp:183] Created Layer pool2 (15)
I0628 19:16:23.204758 32034 net.cpp:560] pool2 <- res2a_branch2b
I0628 19:16:23.204761 32034 net.cpp:529] pool2 -> pool2
I0628 19:16:23.204818 32034 net.cpp:244] Setting up pool2
I0628 19:16:23.204823 32034 net.cpp:251] TRAIN Top shape for layer 15 'pool2' 32 64 16 16 (524288)
I0628 19:16:23.204825 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0628 19:16:23.204830 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204840 32034 net.cpp:183] Created Layer res3a_branch2a (16)
I0628 19:16:23.204844 32034 net.cpp:560] res3a_branch2a <- pool2
I0628 19:16:23.204848 32034 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0628 19:16:23.215445 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.08G, req 0.02G)
I0628 19:16:23.215476 32034 net.cpp:244] Setting up res3a_branch2a
I0628 19:16:23.215487 32034 net.cpp:251] TRAIN Top shape for layer 16 'res3a_branch2a' 32 128 16 16 (1048576)
I0628 19:16:23.215498 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.215504 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.215520 32034 net.cpp:183] Created Layer res3a_branch2a/bn (17)
I0628 19:16:23.215525 32034 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0628 19:16:23.215530 32034 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0628 19:16:23.216265 32034 net.cpp:244] Setting up res3a_branch2a/bn
I0628 19:16:23.216274 32034 net.cpp:251] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 32 128 16 16 (1048576)
I0628 19:16:23.216296 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.216302 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.216307 32034 net.cpp:183] Created Layer res3a_branch2a/relu (18)
I0628 19:16:23.216311 32034 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0628 19:16:23.216315 32034 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0628 19:16:23.216322 32034 net.cpp:244] Setting up res3a_branch2a/relu
I0628 19:16:23.216326 32034 net.cpp:251] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 32 128 16 16 (1048576)
I0628 19:16:23.216331 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0628 19:16:23.216334 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.216351 32034 net.cpp:183] Created Layer res3a_branch2b (19)
I0628 19:16:23.216363 32034 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0628 19:16:23.216367 32034 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0628 19:16:23.221616 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.07G, req 0.02G)
I0628 19:16:23.221634 32034 net.cpp:244] Setting up res3a_branch2b
I0628 19:16:23.221640 32034 net.cpp:251] TRAIN Top shape for layer 19 'res3a_branch2b' 32 128 16 16 (1048576)
I0628 19:16:23.221645 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.221649 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.221657 32034 net.cpp:183] Created Layer res3a_branch2b/bn (20)
I0628 19:16:23.221659 32034 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0628 19:16:23.221662 32034 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0628 19:16:23.222177 32034 net.cpp:244] Setting up res3a_branch2b/bn
I0628 19:16:23.222184 32034 net.cpp:251] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 32 128 16 16 (1048576)
I0628 19:16:23.222190 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.222193 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.222195 32034 net.cpp:183] Created Layer res3a_branch2b/relu (21)
I0628 19:16:23.222198 32034 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0628 19:16:23.222201 32034 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0628 19:16:23.222204 32034 net.cpp:244] Setting up res3a_branch2b/relu
I0628 19:16:23.222208 32034 net.cpp:251] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 32 128 16 16 (1048576)
I0628 19:16:23.222209 32034 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0628 19:16:23.222211 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.222215 32034 net.cpp:183] Created Layer pool3 (22)
I0628 19:16:23.222218 32034 net.cpp:560] pool3 <- res3a_branch2b
I0628 19:16:23.222220 32034 net.cpp:529] pool3 -> pool3
I0628 19:16:23.222266 32034 net.cpp:244] Setting up pool3
I0628 19:16:23.222270 32034 net.cpp:251] TRAIN Top shape for layer 22 'pool3' 32 128 16 16 (1048576)
I0628 19:16:23.222272 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0628 19:16:23.222275 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.222281 32034 net.cpp:183] Created Layer res4a_branch2a (23)
I0628 19:16:23.222285 32034 net.cpp:560] res4a_branch2a <- pool3
I0628 19:16:23.222286 32034 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0628 19:16:23.243301 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.04G, req 0.02G)
I0628 19:16:23.243335 32034 net.cpp:244] Setting up res4a_branch2a
I0628 19:16:23.243346 32034 net.cpp:251] TRAIN Top shape for layer 23 'res4a_branch2a' 32 256 16 16 (2097152)
I0628 19:16:23.243357 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.243363 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.243376 32034 net.cpp:183] Created Layer res4a_branch2a/bn (24)
I0628 19:16:23.243381 32034 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0628 19:16:23.243386 32034 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0628 19:16:23.244150 32034 net.cpp:244] Setting up res4a_branch2a/bn
I0628 19:16:23.244161 32034 net.cpp:251] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 32 256 16 16 (2097152)
I0628 19:16:23.244169 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.244174 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.244179 32034 net.cpp:183] Created Layer res4a_branch2a/relu (25)
I0628 19:16:23.244194 32034 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0628 19:16:23.244199 32034 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0628 19:16:23.244206 32034 net.cpp:244] Setting up res4a_branch2a/relu
I0628 19:16:23.244210 32034 net.cpp:251] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 32 256 16 16 (2097152)
I0628 19:16:23.244215 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0628 19:16:23.244218 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.244230 32034 net.cpp:183] Created Layer res4a_branch2b (26)
I0628 19:16:23.244232 32034 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0628 19:16:23.244237 32034 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0628 19:16:23.254741 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.02G, req 0.02G)
I0628 19:16:23.254770 32034 net.cpp:244] Setting up res4a_branch2b
I0628 19:16:23.254778 32034 net.cpp:251] TRAIN Top shape for layer 26 'res4a_branch2b' 32 256 16 16 (2097152)
I0628 19:16:23.254786 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.254791 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.254799 32034 net.cpp:183] Created Layer res4a_branch2b/bn (27)
I0628 19:16:23.254803 32034 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0628 19:16:23.254806 32034 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0628 19:16:23.255373 32034 net.cpp:244] Setting up res4a_branch2b/bn
I0628 19:16:23.255379 32034 net.cpp:251] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 32 256 16 16 (2097152)
I0628 19:16:23.255385 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.255388 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.255393 32034 net.cpp:183] Created Layer res4a_branch2b/relu (28)
I0628 19:16:23.255394 32034 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0628 19:16:23.255398 32034 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0628 19:16:23.255401 32034 net.cpp:244] Setting up res4a_branch2b/relu
I0628 19:16:23.255404 32034 net.cpp:251] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 32 256 16 16 (2097152)
I0628 19:16:23.255408 32034 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0628 19:16:23.255409 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.255414 32034 net.cpp:183] Created Layer pool4 (29)
I0628 19:16:23.255417 32034 net.cpp:560] pool4 <- res4a_branch2b
I0628 19:16:23.255419 32034 net.cpp:529] pool4 -> pool4
I0628 19:16:23.255465 32034 net.cpp:244] Setting up pool4
I0628 19:16:23.255468 32034 net.cpp:251] TRAIN Top shape for layer 29 'pool4' 32 256 8 8 (524288)
I0628 19:16:23.255471 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0628 19:16:23.255475 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.255482 32034 net.cpp:183] Created Layer res5a_branch2a (30)
I0628 19:16:23.255484 32034 net.cpp:560] res5a_branch2a <- pool4
I0628 19:16:23.255487 32034 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0628 19:16:23.302940 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 8G, req 0.02G)
I0628 19:16:23.302964 32034 net.cpp:244] Setting up res5a_branch2a
I0628 19:16:23.302973 32034 net.cpp:251] TRAIN Top shape for layer 30 'res5a_branch2a' 32 512 8 8 (1048576)
I0628 19:16:23.302980 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.302983 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.302992 32034 net.cpp:183] Created Layer res5a_branch2a/bn (31)
I0628 19:16:23.302995 32034 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0628 19:16:23.303009 32034 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0628 19:16:23.303586 32034 net.cpp:244] Setting up res5a_branch2a/bn
I0628 19:16:23.303592 32034 net.cpp:251] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 32 512 8 8 (1048576)
I0628 19:16:23.303598 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.303601 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.303606 32034 net.cpp:183] Created Layer res5a_branch2a/relu (32)
I0628 19:16:23.303607 32034 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0628 19:16:23.303609 32034 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0628 19:16:23.303613 32034 net.cpp:244] Setting up res5a_branch2a/relu
I0628 19:16:23.303617 32034 net.cpp:251] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 32 512 8 8 (1048576)
I0628 19:16:23.303618 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0628 19:16:23.303620 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.303627 32034 net.cpp:183] Created Layer res5a_branch2b (33)
I0628 19:16:23.303630 32034 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0628 19:16:23.303632 32034 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0628 19:16:23.322635 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 5 5  (limit 7.98G, req 0.02G)
I0628 19:16:23.322652 32034 net.cpp:244] Setting up res5a_branch2b
I0628 19:16:23.322657 32034 net.cpp:251] TRAIN Top shape for layer 33 'res5a_branch2b' 32 512 8 8 (1048576)
I0628 19:16:23.322665 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.322667 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.322674 32034 net.cpp:183] Created Layer res5a_branch2b/bn (34)
I0628 19:16:23.322677 32034 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0628 19:16:23.322679 32034 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0628 19:16:23.323210 32034 net.cpp:244] Setting up res5a_branch2b/bn
I0628 19:16:23.323216 32034 net.cpp:251] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 32 512 8 8 (1048576)
I0628 19:16:23.323221 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.323225 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323227 32034 net.cpp:183] Created Layer res5a_branch2b/relu (35)
I0628 19:16:23.323230 32034 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0628 19:16:23.323231 32034 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0628 19:16:23.323235 32034 net.cpp:244] Setting up res5a_branch2b/relu
I0628 19:16:23.323237 32034 net.cpp:251] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 32 512 8 8 (1048576)
I0628 19:16:23.323240 32034 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0628 19:16:23.323241 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323246 32034 net.cpp:183] Created Layer pool5 (36)
I0628 19:16:23.323247 32034 net.cpp:560] pool5 <- res5a_branch2b
I0628 19:16:23.323249 32034 net.cpp:529] pool5 -> pool5
I0628 19:16:23.323271 32034 net.cpp:244] Setting up pool5
I0628 19:16:23.323276 32034 net.cpp:251] TRAIN Top shape for layer 36 'pool5' 32 512 1 1 (16384)
I0628 19:16:23.323277 32034 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0628 19:16:23.323279 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323284 32034 net.cpp:183] Created Layer fc10 (37)
I0628 19:16:23.323287 32034 net.cpp:560] fc10 <- pool5
I0628 19:16:23.323289 32034 net.cpp:529] fc10 -> fc10
I0628 19:16:23.323508 32034 net.cpp:244] Setting up fc10
I0628 19:16:23.323514 32034 net.cpp:251] TRAIN Top shape for layer 37 'fc10' 32 10 (320)
I0628 19:16:23.323528 32034 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0628 19:16:23.323530 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323542 32034 net.cpp:183] Created Layer loss (38)
I0628 19:16:23.323545 32034 net.cpp:560] loss <- fc10
I0628 19:16:23.323547 32034 net.cpp:560] loss <- label
I0628 19:16:23.323551 32034 net.cpp:529] loss -> loss
I0628 19:16:23.323678 32034 net.cpp:244] Setting up loss
I0628 19:16:23.323684 32034 net.cpp:251] TRAIN Top shape for layer 38 'loss' (1)
I0628 19:16:23.323688 32034 net.cpp:255]     with loss weight 1
I0628 19:16:23.323691 32034 net.cpp:322] loss needs backward computation.
I0628 19:16:23.323694 32034 net.cpp:322] fc10 needs backward computation.
I0628 19:16:23.323696 32034 net.cpp:322] pool5 needs backward computation.
I0628 19:16:23.323699 32034 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0628 19:16:23.323701 32034 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0628 19:16:23.323704 32034 net.cpp:322] res5a_branch2b needs backward computation.
I0628 19:16:23.323705 32034 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0628 19:16:23.323707 32034 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0628 19:16:23.323709 32034 net.cpp:322] res5a_branch2a needs backward computation.
I0628 19:16:23.323712 32034 net.cpp:322] pool4 needs backward computation.
I0628 19:16:23.323714 32034 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0628 19:16:23.323717 32034 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0628 19:16:23.323719 32034 net.cpp:322] res4a_branch2b needs backward computation.
I0628 19:16:23.323721 32034 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0628 19:16:23.323724 32034 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0628 19:16:23.323726 32034 net.cpp:322] res4a_branch2a needs backward computation.
I0628 19:16:23.323729 32034 net.cpp:322] pool3 needs backward computation.
I0628 19:16:23.323731 32034 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0628 19:16:23.323734 32034 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0628 19:16:23.323735 32034 net.cpp:322] res3a_branch2b needs backward computation.
I0628 19:16:23.323737 32034 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0628 19:16:23.323740 32034 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0628 19:16:23.323741 32034 net.cpp:322] res3a_branch2a needs backward computation.
I0628 19:16:23.323743 32034 net.cpp:322] pool2 needs backward computation.
I0628 19:16:23.323745 32034 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0628 19:16:23.323747 32034 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0628 19:16:23.323750 32034 net.cpp:322] res2a_branch2b needs backward computation.
I0628 19:16:23.323751 32034 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0628 19:16:23.323755 32034 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0628 19:16:23.323755 32034 net.cpp:322] res2a_branch2a needs backward computation.
I0628 19:16:23.323757 32034 net.cpp:322] pool1 needs backward computation.
I0628 19:16:23.323760 32034 net.cpp:322] conv1b/relu needs backward computation.
I0628 19:16:23.323762 32034 net.cpp:322] conv1b/bn needs backward computation.
I0628 19:16:23.323765 32034 net.cpp:322] conv1b needs backward computation.
I0628 19:16:23.323767 32034 net.cpp:322] conv1a/relu needs backward computation.
I0628 19:16:23.323770 32034 net.cpp:322] conv1a/bn needs backward computation.
I0628 19:16:23.323771 32034 net.cpp:322] conv1a needs backward computation.
I0628 19:16:23.323773 32034 net.cpp:324] data/bias does not need backward computation.
I0628 19:16:23.323776 32034 net.cpp:324] data does not need backward computation.
I0628 19:16:23.323779 32034 net.cpp:366] This network produces output loss
I0628 19:16:23.323806 32034 net.cpp:388] Top memory (TRAIN) required for data: 176160768 diff: 176160776
I0628 19:16:23.323808 32034 net.cpp:391] Bottom memory (TRAIN) required for data: 176160768 diff: 176160768
I0628 19:16:23.323815 32034 net.cpp:394] Shared (in-place) memory (TRAIN) by data: 117440512 diff: 117440512
I0628 19:16:23.323818 32034 net.cpp:397] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0628 19:16:23.323820 32034 net.cpp:400] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0628 19:16:23.323822 32034 net.cpp:406] Network initialization done.
I0628 19:16:23.324174 32034 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/test.prototxt
I0628 19:16:23.324328 32034 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 25
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0628 19:16:23.324414 32034 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:23.324417 32034 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:23.324419 32034 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0628 19:16:23.324422 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.324432 32034 net.cpp:183] Created Layer data (0)
I0628 19:16:23.324435 32034 net.cpp:529] data -> data
I0628 19:16:23.324439 32034 net.cpp:529] data -> label
I0628 19:16:23.324445 32034 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 25
I0628 19:16:23.324452 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:23.325189 32080 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0628 19:16:23.325268 32034 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.325321 32034 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.325325 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:23.325422 32034 net.cpp:244] Setting up data
I0628 19:16:23.325428 32034 net.cpp:251] TEST Top shape for layer 0 'data' 25 3 32 32 (76800)
I0628 19:16:23.325431 32034 net.cpp:251] TEST Top shape for layer 0 'data' 25 (25)
I0628 19:16:23.325434 32034 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0628 19:16:23.325438 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.325443 32034 net.cpp:183] Created Layer label_data_1_split (1)
I0628 19:16:23.325446 32034 net.cpp:560] label_data_1_split <- label
I0628 19:16:23.325449 32034 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0628 19:16:23.325453 32034 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0628 19:16:23.325456 32034 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0628 19:16:23.325505 32034 net.cpp:244] Setting up label_data_1_split
I0628 19:16:23.325510 32034 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 19:16:23.325520 32034 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 19:16:23.325522 32034 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 19:16:23.325525 32034 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0628 19:16:23.325527 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.325532 32034 net.cpp:183] Created Layer data/bias (2)
I0628 19:16:23.325534 32034 net.cpp:560] data/bias <- data
I0628 19:16:23.325537 32034 net.cpp:529] data/bias -> data/bias
I0628 19:16:23.325665 32034 net.cpp:244] Setting up data/bias
I0628 19:16:23.325672 32034 net.cpp:251] TEST Top shape for layer 2 'data/bias' 25 3 32 32 (76800)
I0628 19:16:23.325676 32034 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0628 19:16:23.325680 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.325686 32034 net.cpp:183] Created Layer conv1a (3)
I0628 19:16:23.325690 32034 net.cpp:560] conv1a <- data/bias
I0628 19:16:23.325692 32034 net.cpp:529] conv1a -> conv1a
I0628 19:16:23.326020 32081 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.326027 32081 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.326833 32081 data_layer.cpp:110] [0] Parser threads: 1
I0628 19:16:23.326839 32081 data_layer.cpp:112] [0] Transformer threads: 1
I0628 19:16:23.328567 32034 net.cpp:244] Setting up conv1a
I0628 19:16:23.328577 32034 net.cpp:251] TEST Top shape for layer 3 'conv1a' 25 32 32 32 (819200)
I0628 19:16:23.328583 32034 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0628 19:16:23.328585 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.328591 32034 net.cpp:183] Created Layer conv1a/bn (4)
I0628 19:16:23.328593 32034 net.cpp:560] conv1a/bn <- conv1a
I0628 19:16:23.328596 32034 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0628 19:16:23.329527 32034 net.cpp:244] Setting up conv1a/bn
I0628 19:16:23.329535 32034 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 25 32 32 32 (819200)
I0628 19:16:23.329542 32034 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0628 19:16:23.329545 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.329556 32034 net.cpp:183] Created Layer conv1a/relu (5)
I0628 19:16:23.329560 32034 net.cpp:560] conv1a/relu <- conv1a
I0628 19:16:23.329561 32034 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0628 19:16:23.329566 32034 net.cpp:244] Setting up conv1a/relu
I0628 19:16:23.329568 32034 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 25 32 32 32 (819200)
I0628 19:16:23.329571 32034 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0628 19:16:23.329573 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.329581 32034 net.cpp:183] Created Layer conv1b (6)
I0628 19:16:23.329584 32034 net.cpp:560] conv1b <- conv1a
I0628 19:16:23.329586 32034 net.cpp:529] conv1b -> conv1b
I0628 19:16:23.331485 32034 net.cpp:244] Setting up conv1b
I0628 19:16:23.331495 32034 net.cpp:251] TEST Top shape for layer 6 'conv1b' 25 32 32 32 (819200)
I0628 19:16:23.331501 32034 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0628 19:16:23.331503 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.331508 32034 net.cpp:183] Created Layer conv1b/bn (7)
I0628 19:16:23.331511 32034 net.cpp:560] conv1b/bn <- conv1b
I0628 19:16:23.331513 32034 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0628 19:16:23.332476 32034 net.cpp:244] Setting up conv1b/bn
I0628 19:16:23.332485 32034 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 25 32 32 32 (819200)
I0628 19:16:23.332492 32034 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0628 19:16:23.332495 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.332510 32034 net.cpp:183] Created Layer conv1b/relu (8)
I0628 19:16:23.332514 32034 net.cpp:560] conv1b/relu <- conv1b
I0628 19:16:23.332516 32034 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0628 19:16:23.332520 32034 net.cpp:244] Setting up conv1b/relu
I0628 19:16:23.332525 32034 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 25 32 32 32 (819200)
I0628 19:16:23.332527 32034 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0628 19:16:23.332532 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.332538 32034 net.cpp:183] Created Layer pool1 (9)
I0628 19:16:23.332542 32034 net.cpp:560] pool1 <- conv1b
I0628 19:16:23.332546 32034 net.cpp:529] pool1 -> pool1
I0628 19:16:23.332665 32034 net.cpp:244] Setting up pool1
I0628 19:16:23.332675 32034 net.cpp:251] TEST Top shape for layer 9 'pool1' 25 32 32 32 (819200)
I0628 19:16:23.332682 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0628 19:16:23.332689 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.332708 32034 net.cpp:183] Created Layer res2a_branch2a (10)
I0628 19:16:23.332715 32034 net.cpp:560] res2a_branch2a <- pool1
I0628 19:16:23.332722 32034 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0628 19:16:23.336020 32034 net.cpp:244] Setting up res2a_branch2a
I0628 19:16:23.336032 32034 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 25 64 32 32 (1638400)
I0628 19:16:23.336042 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.336047 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.336056 32034 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0628 19:16:23.336061 32034 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0628 19:16:23.336064 32034 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0628 19:16:23.337332 32034 net.cpp:244] Setting up res2a_branch2a/bn
I0628 19:16:23.337343 32034 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 25 64 32 32 (1638400)
I0628 19:16:23.337353 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.337357 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.337363 32034 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0628 19:16:23.337368 32034 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0628 19:16:23.337373 32034 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0628 19:16:23.337378 32034 net.cpp:244] Setting up res2a_branch2a/relu
I0628 19:16:23.337383 32034 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 25 64 32 32 (1638400)
I0628 19:16:23.337388 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0628 19:16:23.337391 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.337402 32034 net.cpp:183] Created Layer res2a_branch2b (13)
I0628 19:16:23.337406 32034 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0628 19:16:23.337410 32034 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0628 19:16:23.339617 32034 net.cpp:244] Setting up res2a_branch2b
I0628 19:16:23.339630 32034 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 25 64 32 32 (1638400)
I0628 19:16:23.339639 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.339645 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.339656 32034 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0628 19:16:23.339663 32034 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0628 19:16:23.339668 32034 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0628 19:16:23.340932 32034 net.cpp:244] Setting up res2a_branch2b/bn
I0628 19:16:23.340940 32034 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 25 64 32 32 (1638400)
I0628 19:16:23.340955 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.340960 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.340963 32034 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0628 19:16:23.340966 32034 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0628 19:16:23.340970 32034 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0628 19:16:23.340973 32034 net.cpp:244] Setting up res2a_branch2b/relu
I0628 19:16:23.340977 32034 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 25 64 32 32 (1638400)
I0628 19:16:23.340979 32034 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0628 19:16:23.340981 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.340986 32034 net.cpp:183] Created Layer pool2 (16)
I0628 19:16:23.340989 32034 net.cpp:560] pool2 <- res2a_branch2b
I0628 19:16:23.340991 32034 net.cpp:529] pool2 -> pool2
I0628 19:16:23.341047 32034 net.cpp:244] Setting up pool2
I0628 19:16:23.341050 32034 net.cpp:251] TEST Top shape for layer 16 'pool2' 25 64 16 16 (409600)
I0628 19:16:23.341053 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0628 19:16:23.341055 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.341063 32034 net.cpp:183] Created Layer res3a_branch2a (17)
I0628 19:16:23.341066 32034 net.cpp:560] res3a_branch2a <- pool2
I0628 19:16:23.341068 32034 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0628 19:16:23.345630 32034 net.cpp:244] Setting up res3a_branch2a
I0628 19:16:23.345641 32034 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 25 128 16 16 (819200)
I0628 19:16:23.345646 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.345649 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.345654 32034 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0628 19:16:23.345657 32034 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0628 19:16:23.345660 32034 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0628 19:16:23.346604 32034 net.cpp:244] Setting up res3a_branch2a/bn
I0628 19:16:23.346612 32034 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 25 128 16 16 (819200)
I0628 19:16:23.346621 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.346624 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.346627 32034 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0628 19:16:23.346631 32034 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0628 19:16:23.346633 32034 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0628 19:16:23.346638 32034 net.cpp:244] Setting up res3a_branch2a/relu
I0628 19:16:23.346642 32034 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 25 128 16 16 (819200)
I0628 19:16:23.346644 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0628 19:16:23.346647 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.346652 32034 net.cpp:183] Created Layer res3a_branch2b (20)
I0628 19:16:23.346654 32034 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0628 19:16:23.346657 32034 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0628 19:16:23.348682 32034 net.cpp:244] Setting up res3a_branch2b
I0628 19:16:23.348690 32034 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 25 128 16 16 (819200)
I0628 19:16:23.348695 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.348698 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.348703 32034 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0628 19:16:23.348713 32034 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0628 19:16:23.348717 32034 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0628 19:16:23.349653 32034 net.cpp:244] Setting up res3a_branch2b/bn
I0628 19:16:23.349660 32034 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 25 128 16 16 (819200)
I0628 19:16:23.349668 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.349670 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.349674 32034 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0628 19:16:23.349678 32034 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0628 19:16:23.349680 32034 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0628 19:16:23.349684 32034 net.cpp:244] Setting up res3a_branch2b/relu
I0628 19:16:23.349687 32034 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 25 128 16 16 (819200)
I0628 19:16:23.349690 32034 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0628 19:16:23.349694 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.349697 32034 net.cpp:183] Created Layer pool3 (23)
I0628 19:16:23.349699 32034 net.cpp:560] pool3 <- res3a_branch2b
I0628 19:16:23.349701 32034 net.cpp:529] pool3 -> pool3
I0628 19:16:23.349751 32034 net.cpp:244] Setting up pool3
I0628 19:16:23.349756 32034 net.cpp:251] TEST Top shape for layer 23 'pool3' 25 128 16 16 (819200)
I0628 19:16:23.349758 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0628 19:16:23.349761 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.349767 32034 net.cpp:183] Created Layer res4a_branch2a (24)
I0628 19:16:23.349769 32034 net.cpp:560] res4a_branch2a <- pool3
I0628 19:16:23.349772 32034 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0628 19:16:23.359735 32034 net.cpp:244] Setting up res4a_branch2a
I0628 19:16:23.359742 32034 net.cpp:251] TEST Top shape for layer 24 'res4a_branch2a' 25 256 16 16 (1638400)
I0628 19:16:23.359747 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.359750 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.359755 32034 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0628 19:16:23.359757 32034 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0628 19:16:23.359760 32034 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0628 19:16:23.360709 32034 net.cpp:244] Setting up res4a_branch2a/bn
I0628 19:16:23.360718 32034 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a/bn' 25 256 16 16 (1638400)
I0628 19:16:23.360723 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.360725 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.360728 32034 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0628 19:16:23.360730 32034 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0628 19:16:23.360733 32034 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0628 19:16:23.360736 32034 net.cpp:244] Setting up res4a_branch2a/relu
I0628 19:16:23.360739 32034 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/relu' 25 256 16 16 (1638400)
I0628 19:16:23.360741 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0628 19:16:23.360744 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.360749 32034 net.cpp:183] Created Layer res4a_branch2b (27)
I0628 19:16:23.360751 32034 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0628 19:16:23.360754 32034 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0628 19:16:23.366919 32034 net.cpp:244] Setting up res4a_branch2b
I0628 19:16:23.366940 32034 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2b' 25 256 16 16 (1638400)
I0628 19:16:23.366966 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.366971 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.366982 32034 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0628 19:16:23.366988 32034 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0628 19:16:23.366992 32034 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0628 19:16:23.368288 32034 net.cpp:244] Setting up res4a_branch2b/bn
I0628 19:16:23.368299 32034 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b/bn' 25 256 16 16 (1638400)
I0628 19:16:23.368307 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.368311 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.368316 32034 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0628 19:16:23.368320 32034 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0628 19:16:23.368324 32034 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0628 19:16:23.368331 32034 net.cpp:244] Setting up res4a_branch2b/relu
I0628 19:16:23.368336 32034 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/relu' 25 256 16 16 (1638400)
I0628 19:16:23.368340 32034 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0628 19:16:23.368345 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.368351 32034 net.cpp:183] Created Layer pool4 (30)
I0628 19:16:23.368355 32034 net.cpp:560] pool4 <- res4a_branch2b
I0628 19:16:23.368360 32034 net.cpp:529] pool4 -> pool4
I0628 19:16:23.368433 32034 net.cpp:244] Setting up pool4
I0628 19:16:23.368439 32034 net.cpp:251] TEST Top shape for layer 30 'pool4' 25 256 8 8 (409600)
I0628 19:16:23.368443 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0628 19:16:23.368448 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.368465 32034 net.cpp:183] Created Layer res5a_branch2a (31)
I0628 19:16:23.368470 32034 net.cpp:560] res5a_branch2a <- pool4
I0628 19:16:23.368474 32034 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0628 19:16:23.401430 32034 net.cpp:244] Setting up res5a_branch2a
I0628 19:16:23.401461 32034 net.cpp:251] TEST Top shape for layer 31 'res5a_branch2a' 25 512 8 8 (819200)
I0628 19:16:23.401473 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.401482 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.401499 32034 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0628 19:16:23.401509 32034 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0628 19:16:23.401515 32034 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0628 19:16:23.402819 32034 net.cpp:244] Setting up res5a_branch2a/bn
I0628 19:16:23.402828 32034 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a/bn' 25 512 8 8 (819200)
I0628 19:16:23.402835 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.402838 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.402842 32034 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0628 19:16:23.402843 32034 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0628 19:16:23.402846 32034 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0628 19:16:23.402851 32034 net.cpp:244] Setting up res5a_branch2a/relu
I0628 19:16:23.402853 32034 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/relu' 25 512 8 8 (819200)
I0628 19:16:23.402855 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0628 19:16:23.402858 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.402864 32034 net.cpp:183] Created Layer res5a_branch2b (34)
I0628 19:16:23.402866 32034 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0628 19:16:23.402878 32034 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0628 19:16:23.418716 32034 net.cpp:244] Setting up res5a_branch2b
I0628 19:16:23.418727 32034 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2b' 25 512 8 8 (819200)
I0628 19:16:23.418740 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.418743 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.418748 32034 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0628 19:16:23.418751 32034 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0628 19:16:23.418754 32034 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0628 19:16:23.419737 32034 net.cpp:244] Setting up res5a_branch2b/bn
I0628 19:16:23.419745 32034 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b/bn' 25 512 8 8 (819200)
I0628 19:16:23.419751 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.419754 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.419757 32034 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0628 19:16:23.419759 32034 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0628 19:16:23.419761 32034 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0628 19:16:23.419765 32034 net.cpp:244] Setting up res5a_branch2b/relu
I0628 19:16:23.419769 32034 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/relu' 25 512 8 8 (819200)
I0628 19:16:23.419770 32034 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0628 19:16:23.419772 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.419783 32034 net.cpp:183] Created Layer pool5 (37)
I0628 19:16:23.419785 32034 net.cpp:560] pool5 <- res5a_branch2b
I0628 19:16:23.419788 32034 net.cpp:529] pool5 -> pool5
I0628 19:16:23.419816 32034 net.cpp:244] Setting up pool5
I0628 19:16:23.419821 32034 net.cpp:251] TEST Top shape for layer 37 'pool5' 25 512 1 1 (12800)
I0628 19:16:23.419822 32034 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0628 19:16:23.419824 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.419828 32034 net.cpp:183] Created Layer fc10 (38)
I0628 19:16:23.419831 32034 net.cpp:560] fc10 <- pool5
I0628 19:16:23.419832 32034 net.cpp:529] fc10 -> fc10
I0628 19:16:23.420070 32034 net.cpp:244] Setting up fc10
I0628 19:16:23.420078 32034 net.cpp:251] TEST Top shape for layer 38 'fc10' 25 10 (250)
I0628 19:16:23.420081 32034 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0628 19:16:23.420084 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420087 32034 net.cpp:183] Created Layer fc10_fc10_0_split (39)
I0628 19:16:23.420089 32034 net.cpp:560] fc10_fc10_0_split <- fc10
I0628 19:16:23.420091 32034 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0628 19:16:23.420094 32034 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0628 19:16:23.420096 32034 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0628 19:16:23.420147 32034 net.cpp:244] Setting up fc10_fc10_0_split
I0628 19:16:23.420151 32034 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 19:16:23.420155 32034 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 19:16:23.420156 32034 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 19:16:23.420158 32034 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0628 19:16:23.420161 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420164 32034 net.cpp:183] Created Layer loss (40)
I0628 19:16:23.420166 32034 net.cpp:560] loss <- fc10_fc10_0_split_0
I0628 19:16:23.420168 32034 net.cpp:560] loss <- label_data_1_split_0
I0628 19:16:23.420178 32034 net.cpp:529] loss -> loss
I0628 19:16:23.420298 32034 net.cpp:244] Setting up loss
I0628 19:16:23.420303 32034 net.cpp:251] TEST Top shape for layer 40 'loss' (1)
I0628 19:16:23.420306 32034 net.cpp:255]     with loss weight 1
I0628 19:16:23.420311 32034 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0628 19:16:23.420313 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420320 32034 net.cpp:183] Created Layer accuracy/top1 (41)
I0628 19:16:23.420321 32034 net.cpp:560] accuracy/top1 <- fc10_fc10_0_split_1
I0628 19:16:23.420325 32034 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0628 19:16:23.420326 32034 net.cpp:529] accuracy/top1 -> accuracy/top1
I0628 19:16:23.420331 32034 net.cpp:244] Setting up accuracy/top1
I0628 19:16:23.420332 32034 net.cpp:251] TEST Top shape for layer 41 'accuracy/top1' (1)
I0628 19:16:23.420334 32034 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0628 19:16:23.420338 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420342 32034 net.cpp:183] Created Layer accuracy/top5 (42)
I0628 19:16:23.420344 32034 net.cpp:560] accuracy/top5 <- fc10_fc10_0_split_2
I0628 19:16:23.420347 32034 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0628 19:16:23.420349 32034 net.cpp:529] accuracy/top5 -> accuracy/top5
I0628 19:16:23.420353 32034 net.cpp:244] Setting up accuracy/top5
I0628 19:16:23.420356 32034 net.cpp:251] TEST Top shape for layer 42 'accuracy/top5' (1)
I0628 19:16:23.420358 32034 net.cpp:324] accuracy/top5 does not need backward computation.
I0628 19:16:23.420361 32034 net.cpp:324] accuracy/top1 does not need backward computation.
I0628 19:16:23.420362 32034 net.cpp:322] loss needs backward computation.
I0628 19:16:23.420366 32034 net.cpp:322] fc10_fc10_0_split needs backward computation.
I0628 19:16:23.420367 32034 net.cpp:322] fc10 needs backward computation.
I0628 19:16:23.420369 32034 net.cpp:322] pool5 needs backward computation.
I0628 19:16:23.420372 32034 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0628 19:16:23.420373 32034 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0628 19:16:23.420375 32034 net.cpp:322] res5a_branch2b needs backward computation.
I0628 19:16:23.420377 32034 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0628 19:16:23.420379 32034 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0628 19:16:23.420382 32034 net.cpp:322] res5a_branch2a needs backward computation.
I0628 19:16:23.420384 32034 net.cpp:322] pool4 needs backward computation.
I0628 19:16:23.420387 32034 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0628 19:16:23.420388 32034 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0628 19:16:23.420390 32034 net.cpp:322] res4a_branch2b needs backward computation.
I0628 19:16:23.420392 32034 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0628 19:16:23.420394 32034 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0628 19:16:23.420397 32034 net.cpp:322] res4a_branch2a needs backward computation.
I0628 19:16:23.420399 32034 net.cpp:322] pool3 needs backward computation.
I0628 19:16:23.420402 32034 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0628 19:16:23.420403 32034 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0628 19:16:23.420405 32034 net.cpp:322] res3a_branch2b needs backward computation.
I0628 19:16:23.420408 32034 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0628 19:16:23.420410 32034 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0628 19:16:23.420413 32034 net.cpp:322] res3a_branch2a needs backward computation.
I0628 19:16:23.420414 32034 net.cpp:322] pool2 needs backward computation.
I0628 19:16:23.420416 32034 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0628 19:16:23.420418 32034 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0628 19:16:23.420420 32034 net.cpp:322] res2a_branch2b needs backward computation.
I0628 19:16:23.420428 32034 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0628 19:16:23.420429 32034 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0628 19:16:23.420433 32034 net.cpp:322] res2a_branch2a needs backward computation.
I0628 19:16:23.420434 32034 net.cpp:322] pool1 needs backward computation.
I0628 19:16:23.420436 32034 net.cpp:322] conv1b/relu needs backward computation.
I0628 19:16:23.420439 32034 net.cpp:322] conv1b/bn needs backward computation.
I0628 19:16:23.420441 32034 net.cpp:322] conv1b needs backward computation.
I0628 19:16:23.420442 32034 net.cpp:322] conv1a/relu needs backward computation.
I0628 19:16:23.420445 32034 net.cpp:322] conv1a/bn needs backward computation.
I0628 19:16:23.420447 32034 net.cpp:322] conv1a needs backward computation.
I0628 19:16:23.420450 32034 net.cpp:324] data/bias does not need backward computation.
I0628 19:16:23.420454 32034 net.cpp:324] label_data_1_split does not need backward computation.
I0628 19:16:23.420457 32034 net.cpp:324] data does not need backward computation.
I0628 19:16:23.420459 32034 net.cpp:366] This network produces output accuracy/top1
I0628 19:16:23.420462 32034 net.cpp:366] This network produces output accuracy/top5
I0628 19:16:23.420464 32034 net.cpp:366] This network produces output loss
I0628 19:16:23.420490 32034 net.cpp:388] Top memory (TEST) required for data: 137625600 diff: 91750408
I0628 19:16:23.420492 32034 net.cpp:391] Bottom memory (TEST) required for data: 137625600 diff: 137625600
I0628 19:16:23.420495 32034 net.cpp:394] Shared (in-place) memory (TEST) by data: 91750400 diff: 91750400
I0628 19:16:23.420497 32034 net.cpp:397] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0628 19:16:23.420498 32034 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0628 19:16:23.420501 32034 net.cpp:406] Network initialization done.
I0628 19:16:23.420552 32034 solver.cpp:56] Solver scaffolding done.
I0628 19:16:23.424154 32034 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0628 19:16:23.428874 32034 net.cpp:1087] Copying source layer data Type:Data #blobs=0
I0628 19:16:23.428899 32034 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0628 19:16:23.428936 32034 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0628 19:16:23.428953 32034 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429221 32034 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0628 19:16:23.429229 32034 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0628 19:16:23.429240 32034 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429402 32034 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0628 19:16:23.429407 32034 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0628 19:16:23.429411 32034 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.429430 32034 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429600 32034 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.429606 32034 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.429621 32034 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429775 32034 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.429781 32034 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0628 19:16:23.429785 32034 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.429827 32034 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429970 32034 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.429976 32034 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.430016 32034 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.430148 32034 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.430155 32034 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0628 19:16:23.430157 32034 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.430279 32034 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.430418 32034 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.430424 32034 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.430490 32034 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.430629 32034 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.430634 32034 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0628 19:16:23.430637 32034 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.431035 32034 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.431182 32034 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.431188 32034 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.431370 32034 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.431514 32034 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.431519 32034 net.cpp:1087] Copying source layer pool5 Type:Pooling #blobs=0
I0628 19:16:23.431522 32034 net.cpp:1087] Copying source layer fc10 Type:InnerProduct #blobs=2
I0628 19:16:23.431535 32034 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0628 19:16:23.435065 32034 net.cpp:1087] Copying source layer data Type:Data #blobs=0
I0628 19:16:23.435081 32034 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0628 19:16:23.435104 32034 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0628 19:16:23.435115 32034 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435307 32034 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0628 19:16:23.435312 32034 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0628 19:16:23.435319 32034 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435436 32034 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0628 19:16:23.435439 32034 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0628 19:16:23.435441 32034 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.435456 32034 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435582 32034 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.435586 32034 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.435598 32034 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435719 32034 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.435722 32034 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0628 19:16:23.435724 32034 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.435761 32034 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435866 32034 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.435870 32034 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.435891 32034 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435991 32034 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.436002 32034 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0628 19:16:23.436004 32034 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.436115 32034 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.436214 32034 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.436218 32034 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.436271 32034 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.436372 32034 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.436377 32034 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0628 19:16:23.436379 32034 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.436739 32034 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.436846 32034 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.436849 32034 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.437006 32034 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.437120 32034 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.437125 32034 net.cpp:1087] Copying source layer pool5 Type:Pooling #blobs=0
I0628 19:16:23.437129 32034 net.cpp:1087] Copying source layer fc10 Type:InnerProduct #blobs=2
I0628 19:16:23.437137 32034 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0628 19:16:23.437203 32034 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0628 19:16:23.437207 32034 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0628 19:16:23.437209 32034 parallel.cpp:59] Starting Optimization
I0628 19:16:23.437211 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:23.437237 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.437836 32082 device_alternate.hpp:116] NVML initialized on thread 140590727554816
I0628 19:16:23.449897 32082 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0628 19:16:23.449942 32083 device_alternate.hpp:116] NVML initialized on thread 140590719162112
I0628 19:16:23.451162 32083 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0628 19:16:23.455222 32083 solver.cpp:42] Solver data type: FLOAT
I0628 19:16:23.455693 32083 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:23.455699 32083 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:23.455724 32083 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 32
I0628 19:16:23.455732 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.456372 32084 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0628 19:16:23.457113 32083 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:23.457176 32083 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:23.457181 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.736929 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 8.25G, req 0G)
I0628 19:16:23.744374 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 0  (limit 8.23G, req 0G)
I0628 19:16:23.757426 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.2G, req 0G)
I0628 19:16:23.764397 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0628 19:16:23.777598 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.17G, req 0.02G)
I0628 19:16:23.783074 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.02G)
I0628 19:16:23.805760 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.14G, req 0.02G)
I0628 19:16:23.815860 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.12G, req 0.02G)
I0628 19:16:23.862053 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0.02G)
I0628 19:16:23.882817 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 5 5  (limit 8.08G, req 0.02G)
I0628 19:16:23.884151 32083 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/test.prototxt
I0628 19:16:23.884270 32083 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:23.884275 32083 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:23.884292 32083 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 25
I0628 19:16:23.884299 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.884932 32086 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0628 19:16:23.885007 32083 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.885066 32083 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.885069 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.886028 32087 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.886034 32087 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.886831 32087 data_layer.cpp:110] [1] Parser threads: 1
I0628 19:16:23.886838 32087 data_layer.cpp:112] [1] Transformer threads: 1
I0628 19:16:23.976263 32083 solver.cpp:56] Solver scaffolding done.
I0628 19:16:23.992076 32083 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0628 19:16:23.992076 32082 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0628 19:16:24.092077 32083 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:24.092100 32082 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:24.094200 32083 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:24.094259 32083 solver.cpp:474] Solving jacintonet11v2_train
I0628 19:16:24.094264 32083 solver.cpp:475] Learning Rate Policy: poly
I0628 19:16:24.103812 32082 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:24.103890 32082 solver.cpp:474] Solving jacintonet11v2_train
I0628 19:16:24.103895 32082 solver.cpp:475] Learning Rate Policy: poly
I0628 19:16:24.108479 32082 solver.cpp:268] Starting Optimization on GPU 0
I0628 19:16:24.108484 32083 solver.cpp:268] Starting Optimization on GPU 1
I0628 19:16:24.108516 32082 solver.cpp:545] Iteration 0, Testing net (#0)
I0628 19:16:24.108552 32089 device_alternate.hpp:116] NVML initialized on thread 140590326802176
I0628 19:16:24.108570 32089 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0628 19:16:24.109127 32088 device_alternate.hpp:116] NVML initialized on thread 140590335194880
I0628 19:16:24.109138 32088 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0628 19:16:24.156026 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.92
I0628 19:16:24.156042 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 1
I0628 19:16:24.156047 32082 solver.cpp:630]     Test net output #2: loss = 0.230969 (* 1 = 0.230969 loss)
I0628 19:16:24.156050 32082 solver.cpp:295] [MultiGPU] Initial Test completed
I0628 19:16:24.156061 32082 blocking_queue.cpp:40] Data layer prefetch queue empty
I0628 19:16:24.164827 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.76G, req 0.02G)
I0628 19:16:24.165887 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.69G, req 0.01G)
I0628 19:16:24.170982 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.76G, req 0.02G)
I0628 19:16:24.171963 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.68G, req 0.01G)
I0628 19:16:24.180423 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 7.74G, req 0.02G)
I0628 19:16:24.181746 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.67G, req 0.01G)
I0628 19:16:24.186123 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0.02G)
I0628 19:16:24.187412 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.65G, req 0.01G)
I0628 19:16:24.194908 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.71G, req 0.02G)
I0628 19:16:24.197582 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.64G, req 0.02G)
I0628 19:16:24.199347 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.7G, req 0.02G)
I0628 19:16:24.202584 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.63G, req 0.02G)
I0628 19:16:24.214046 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.02G)
I0628 19:16:24.218883 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.61G, req 0.02G)
I0628 19:16:24.220908 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.67G, req 0.02G)
I0628 19:16:24.226111 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.6G, req 0.02G)
I0628 19:16:24.241638 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.65G, req 0.02G)
I0628 19:16:24.247148 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.57G, req 0.02G)
I0628 19:16:24.248780 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.64G, req 0.02G)
I0628 19:16:24.254791 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.56G, req 0.02G)
I0628 19:16:24.271425 32085 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:24.271440 32085 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:24.271509 32085 data_layer.cpp:110] [1] Parser threads: 1
I0628 19:16:24.271517 32085 data_layer.cpp:112] [1] Transformer threads: 1
I0628 19:16:24.276278 32051 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:24.276290 32051 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:24.276361 32051 data_layer.cpp:110] [0] Parser threads: 1
I0628 19:16:24.276368 32051 data_layer.cpp:112] [0] Transformer threads: 1
I0628 19:16:24.276680 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.02
I0628 19:16:24.282459 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 0  (limit 7.56G, req 0.02G)
I0628 19:16:24.287362 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.295169 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 7.56G, req 0.02G)
I0628 19:16:24.299315 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.305445 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.56G, req 0.02G)
I0628 19:16:24.308485 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.320910 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.327947 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.344398 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.56G, req 0.02G)
I0628 19:16:24.350427 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.56G, req 0.02G)
I0628 19:16:24.436935 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:24.440271 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:24.440646 32082 solver.cpp:354] Iteration 0 (0.284538 s), loss = 0.00127412
I0628 19:16:24.440663 32082 solver.cpp:371]     Train net output #0: loss = 0.00127412 (* 1 = 0.00127412 loss)
I0628 19:16:24.440670 32082 sgd_solver.cpp:137] Iteration 0, lr = 0.01, m = 0.9
I0628 19:16:24.446297 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.451243 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.458339 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.463274 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.470731 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.45G, req 0.02G)
I0628 19:16:24.474318 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.492055 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.45G, req 0.02G)
I0628 19:16:24.501844 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.524276 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.533179 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.45G, req 0.02G)
I0628 19:16:24.546957 32082 solver.cpp:354] Iteration 1 (0.106255 s), loss = 0.00123006
I0628 19:16:24.546990 32082 solver.cpp:371]     Train net output #0: loss = 0.00123006 (* 1 = 0.00123006 loss)
I0628 19:16:24.555524 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.68G/1 1 0 0  (limit 6.9G, req 0.02G)
I0628 19:16:24.557387 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.68G/1 1 0 0  (limit 6.79G, req 0.02G)
I0628 19:16:24.563413 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.35G/2 1 1 3  (limit 6.22G, req 0.02G)
I0628 19:16:24.564997 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.35G/2 1 1 3  (limit 6.11G, req 0.02G)
I0628 19:16:24.578878 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.35G/1 6 4 3  (limit 6.22G, req 0.02G)
I0628 19:16:24.581668 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.35G/1 6 4 3  (limit 6.11G, req 0.02G)
I0628 19:16:24.586210 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.35G/2 6 4 3  (limit 6.22G, req 0.02G)
I0628 19:16:24.589797 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.35G/2 6 4 3  (limit 6.11G, req 0.02G)
I0628 19:16:24.597297 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.35G/1 6 4 5  (limit 6.22G, req 0.02G)
I0628 19:16:24.601249 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.35G/1 6 4 5  (limit 6.11G, req 0.02G)
I0628 19:16:24.602174 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.35G/2 6 4 0  (limit 6.22G, req 0.02G)
I0628 19:16:24.606168 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.35G/2 6 4 0  (limit 6.11G, req 0.02G)
I0628 19:16:24.630295 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.35G/1 6 4 5  (limit 6.22G, req 0.03G)
I0628 19:16:24.635841 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.35G/1 6 4 5  (limit 6.11G, req 0.03G)
I0628 19:16:24.640148 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.35G/2 6 4 3  (limit 6.22G, req 0.03G)
I0628 19:16:24.644145 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.35G/2 6 4 3  (limit 6.11G, req 0.03G)
I0628 19:16:24.681107 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.35G/1 7 5 5  (limit 6.22G, req 0.03G)
I0628 19:16:24.687614 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.35G/1 7 5 5  (limit 6.11G, req 0.03G)
I0628 19:16:24.692947 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.35G/2 7 5 5  (limit 6.22G, req 0.03G)
I0628 19:16:24.696846 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.35G/2 7 5 5  (limit 6.11G, req 0.03G)
I0628 19:16:24.708863 32082 solver.cpp:349] Iteration 2 (6.17939 iter/s, 0.161828s/100 iter), loss = 0.000986893
I0628 19:16:24.708884 32082 solver.cpp:371]     Train net output #0: loss = 0.000986893 (* 1 = 0.000986893 loss)
I0628 19:16:24.743780 32083 cudnn_conv_layer.cpp:283] [1] Layer 'conv1a' reallocating workspace: 1.35G -> 0.07G
I0628 19:16:24.743875 32082 cudnn_conv_layer.cpp:283] [0] Layer 'conv1a' reallocating workspace: 1.35G -> 0.07G
I0628 19:16:26.401309 32082 solver.cpp:349] Iteration 100 (57.9145 iter/s, 1.69215s/100 iter), loss = 0.000659005
I0628 19:16:26.401331 32082 solver.cpp:371]     Train net output #0: loss = 0.000659005 (* 1 = 0.000659005 loss)
I0628 19:16:26.401335 32082 sgd_solver.cpp:137] Iteration 100, lr = 0.00998437, m = 0.9
I0628 19:16:28.120751 32082 solver.cpp:349] Iteration 200 (58.1687 iter/s, 1.71914s/100 iter), loss = 0.00237289
I0628 19:16:28.120779 32082 solver.cpp:371]     Train net output #0: loss = 0.00237289 (* 1 = 0.00237289 loss)
I0628 19:16:28.120784 32082 sgd_solver.cpp:137] Iteration 200, lr = 0.00996875, m = 0.9
I0628 19:16:29.838142 32082 solver.cpp:349] Iteration 300 (58.2384 iter/s, 1.71708s/100 iter), loss = 0.00174145
I0628 19:16:29.838169 32082 solver.cpp:371]     Train net output #0: loss = 0.00174145 (* 1 = 0.00174145 loss)
I0628 19:16:29.838176 32082 sgd_solver.cpp:137] Iteration 300, lr = 0.00995312, m = 0.9
I0628 19:16:31.555893 32082 solver.cpp:349] Iteration 400 (58.2262 iter/s, 1.71744s/100 iter), loss = 0.00140638
I0628 19:16:31.555919 32082 solver.cpp:371]     Train net output #0: loss = 0.00140638 (* 1 = 0.00140638 loss)
I0628 19:16:31.555925 32082 sgd_solver.cpp:137] Iteration 400, lr = 0.0099375, m = 0.9
I0628 19:16:33.273975 32082 solver.cpp:349] Iteration 500 (58.2149 iter/s, 1.71777s/100 iter), loss = 0.000838197
I0628 19:16:33.273998 32082 solver.cpp:371]     Train net output #0: loss = 0.000838196 (* 1 = 0.000838196 loss)
I0628 19:16:33.274003 32082 sgd_solver.cpp:137] Iteration 500, lr = 0.00992187, m = 0.9
I0628 19:16:34.993888 32082 solver.cpp:349] Iteration 600 (58.1528 iter/s, 1.71961s/100 iter), loss = 0.00101999
I0628 19:16:34.993916 32082 solver.cpp:371]     Train net output #0: loss = 0.00101999 (* 1 = 0.00101999 loss)
I0628 19:16:34.993921 32082 sgd_solver.cpp:137] Iteration 600, lr = 0.00990625, m = 0.9
I0628 19:16:36.715065 32082 solver.cpp:349] Iteration 700 (58.1103 iter/s, 1.72087s/100 iter), loss = 0.00143992
I0628 19:16:36.715090 32082 solver.cpp:371]     Train net output #0: loss = 0.00143992 (* 1 = 0.00143992 loss)
I0628 19:16:36.715113 32082 sgd_solver.cpp:137] Iteration 700, lr = 0.00989062, m = 0.9
I0628 19:16:38.054155 32050 data_reader.cpp:262] Starting prefetch of epoch 1
I0628 19:16:38.432346 32082 solver.cpp:349] Iteration 800 (58.2427 iter/s, 1.71695s/100 iter), loss = 0.000767142
I0628 19:16:38.432371 32082 solver.cpp:371]     Train net output #0: loss = 0.000767141 (* 1 = 0.000767141 loss)
I0628 19:16:38.432377 32082 sgd_solver.cpp:137] Iteration 800, lr = 0.009875, m = 0.9
I0628 19:16:40.153707 32082 solver.cpp:349] Iteration 900 (58.1041 iter/s, 1.72105s/100 iter), loss = 0.000887107
I0628 19:16:40.153733 32082 solver.cpp:371]     Train net output #0: loss = 0.000887106 (* 1 = 0.000887106 loss)
I0628 19:16:40.153739 32082 sgd_solver.cpp:137] Iteration 900, lr = 0.00985937, m = 0.9
I0628 19:16:41.860955 32082 solver.cpp:401] Sparsity after update:
I0628 19:16:41.862035 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:16:41.862043 32082 net.cpp:2170] conv1a_param_0(0.01) 
I0628 19:16:41.862056 32082 net.cpp:2170] conv1b_param_0(0.02) 
I0628 19:16:41.862062 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:16:41.862066 32082 net.cpp:2170] res2a_branch2a_param_0(0.02) 
I0628 19:16:41.862069 32082 net.cpp:2170] res2a_branch2b_param_0(0.02) 
I0628 19:16:41.862074 32082 net.cpp:2170] res3a_branch2a_param_0(0.02) 
I0628 19:16:41.862078 32082 net.cpp:2170] res3a_branch2b_param_0(0.02) 
I0628 19:16:41.862083 32082 net.cpp:2170] res4a_branch2a_param_0(0.02) 
I0628 19:16:41.862087 32082 net.cpp:2170] res4a_branch2b_param_0(0.02) 
I0628 19:16:41.862092 32082 net.cpp:2170] res5a_branch2a_param_0(0.02) 
I0628 19:16:41.862097 32082 net.cpp:2170] res5a_branch2b_param_0(0.02) 
I0628 19:16:41.862100 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (47060/2.3599e+06) 0.0199
I0628 19:16:41.862112 32082 solver.cpp:545] Iteration 1000, Testing net (#0)
I0628 19:16:42.860646 32080 data_reader.cpp:262] Starting prefetch of epoch 1
I0628 19:16:42.882050 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9168
I0628 19:16:42.882063 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:16:42.882068 32082 solver.cpp:630]     Test net output #2: loss = 0.299202 (* 1 = 0.299202 loss)
I0628 19:16:42.882081 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01981s
I0628 19:16:42.899294 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.04
I0628 19:16:43.024837 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:43.026312 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:43.026671 32082 solver.cpp:349] Iteration 1000 (34.8131 iter/s, 2.87248s/100 iter), loss = 0.00151083
I0628 19:16:43.026690 32082 solver.cpp:371]     Train net output #0: loss = 0.00151083 (* 1 = 0.00151083 loss)
I0628 19:16:43.026695 32082 sgd_solver.cpp:137] Iteration 1000, lr = 0.00984375, m = 0.9
I0628 19:16:44.751830 32082 solver.cpp:349] Iteration 1100 (57.9758 iter/s, 1.72486s/100 iter), loss = 0.00245544
I0628 19:16:44.751857 32082 solver.cpp:371]     Train net output #0: loss = 0.00245544 (* 1 = 0.00245544 loss)
I0628 19:16:44.751863 32082 sgd_solver.cpp:137] Iteration 1100, lr = 0.00982813, m = 0.9
I0628 19:16:46.470062 32082 solver.cpp:349] Iteration 1200 (58.2099 iter/s, 1.71792s/100 iter), loss = 0.00142794
I0628 19:16:46.470088 32082 solver.cpp:371]     Train net output #0: loss = 0.00142794 (* 1 = 0.00142794 loss)
I0628 19:16:46.470093 32082 sgd_solver.cpp:137] Iteration 1200, lr = 0.0098125, m = 0.9
I0628 19:16:48.191891 32082 solver.cpp:349] Iteration 1300 (58.0882 iter/s, 1.72152s/100 iter), loss = 0.00138737
I0628 19:16:48.191913 32082 solver.cpp:371]     Train net output #0: loss = 0.00138737 (* 1 = 0.00138737 loss)
I0628 19:16:48.191917 32082 sgd_solver.cpp:137] Iteration 1300, lr = 0.00979687, m = 0.9
I0628 19:16:49.909384 32082 solver.cpp:349] Iteration 1400 (58.2347 iter/s, 1.71719s/100 iter), loss = 0.00105391
I0628 19:16:49.909409 32082 solver.cpp:371]     Train net output #0: loss = 0.00105391 (* 1 = 0.00105391 loss)
I0628 19:16:49.909430 32082 sgd_solver.cpp:137] Iteration 1400, lr = 0.00978125, m = 0.9
I0628 19:16:51.629667 32082 solver.cpp:349] Iteration 1500 (58.141 iter/s, 1.71996s/100 iter), loss = 0.000944691
I0628 19:16:51.629690 32082 solver.cpp:371]     Train net output #0: loss = 0.00094469 (* 1 = 0.00094469 loss)
I0628 19:16:51.629696 32082 sgd_solver.cpp:137] Iteration 1500, lr = 0.00976562, m = 0.9
I0628 19:16:52.644906 32050 data_reader.cpp:262] Starting prefetch of epoch 2
I0628 19:16:53.350920 32082 solver.cpp:349] Iteration 1600 (58.1077 iter/s, 1.72094s/100 iter), loss = 0.00200254
I0628 19:16:53.351018 32082 solver.cpp:371]     Train net output #0: loss = 0.00200254 (* 1 = 0.00200254 loss)
I0628 19:16:53.351025 32082 sgd_solver.cpp:137] Iteration 1600, lr = 0.00975, m = 0.9
I0628 19:16:55.076735 32082 solver.cpp:349] Iteration 1700 (57.9582 iter/s, 1.72538s/100 iter), loss = 0.00134221
I0628 19:16:55.076757 32082 solver.cpp:371]     Train net output #0: loss = 0.00134221 (* 1 = 0.00134221 loss)
I0628 19:16:55.076761 32082 sgd_solver.cpp:137] Iteration 1700, lr = 0.00973437, m = 0.9
I0628 19:16:56.795634 32082 solver.cpp:349] Iteration 1800 (58.2067 iter/s, 1.71802s/100 iter), loss = 0.00154625
I0628 19:16:56.795660 32082 solver.cpp:371]     Train net output #0: loss = 0.00154625 (* 1 = 0.00154625 loss)
I0628 19:16:56.795665 32082 sgd_solver.cpp:137] Iteration 1800, lr = 0.00971875, m = 0.9
I0628 19:16:58.517601 32082 solver.cpp:349] Iteration 1900 (58.103 iter/s, 1.72108s/100 iter), loss = 0.000624003
I0628 19:16:58.517623 32082 solver.cpp:371]     Train net output #0: loss = 0.000624001 (* 1 = 0.000624001 loss)
I0628 19:16:58.517628 32082 sgd_solver.cpp:137] Iteration 1900, lr = 0.00970312, m = 0.9
I0628 19:17:00.221585 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:00.222738 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:00.222748 32082 net.cpp:2170] conv1a_param_0(0.02) 
I0628 19:17:00.222754 32082 net.cpp:2170] conv1b_param_0(0.0399) 
I0628 19:17:00.222755 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:00.222765 32082 net.cpp:2170] res2a_branch2a_param_0(0.04) 
I0628 19:17:00.222767 32082 net.cpp:2170] res2a_branch2b_param_0(0.0399) 
I0628 19:17:00.222769 32082 net.cpp:2170] res3a_branch2a_param_0(0.04) 
I0628 19:17:00.222771 32082 net.cpp:2170] res3a_branch2b_param_0(0.04) 
I0628 19:17:00.222772 32082 net.cpp:2170] res4a_branch2a_param_0(0.04) 
I0628 19:17:00.222774 32082 net.cpp:2170] res4a_branch2b_param_0(0.04) 
I0628 19:17:00.222776 32082 net.cpp:2170] res5a_branch2a_param_0(0.04) 
I0628 19:17:00.222779 32082 net.cpp:2170] res5a_branch2b_param_0(0.04) 
I0628 19:17:00.222780 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (94130/2.3599e+06) 0.0399
I0628 19:17:00.222789 32082 solver.cpp:545] Iteration 2000, Testing net (#0)
I0628 19:17:01.223199 32080 data_reader.cpp:262] Starting prefetch of epoch 2
I0628 19:17:01.243849 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9172
I0628 19:17:01.243862 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:17:01.243867 32082 solver.cpp:630]     Test net output #2: loss = 0.299738 (* 1 = 0.299738 loss)
I0628 19:17:01.243881 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02059s
I0628 19:17:01.261152 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.06
I0628 19:17:01.408627 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:01.410100 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:01.410457 32082 solver.cpp:349] Iteration 2000 (34.5851 iter/s, 2.89141s/100 iter), loss = 0.00200341
I0628 19:17:01.410476 32082 solver.cpp:371]     Train net output #0: loss = 0.00200341 (* 1 = 0.00200341 loss)
I0628 19:17:01.410485 32082 sgd_solver.cpp:137] Iteration 2000, lr = 0.0096875, m = 0.9
I0628 19:17:03.129894 32082 solver.cpp:349] Iteration 2100 (58.1881 iter/s, 1.71856s/100 iter), loss = 0.000749538
I0628 19:17:03.129920 32082 solver.cpp:371]     Train net output #0: loss = 0.000749537 (* 1 = 0.000749537 loss)
I0628 19:17:03.129925 32082 sgd_solver.cpp:137] Iteration 2100, lr = 0.00967188, m = 0.9
I0628 19:17:04.851223 32082 solver.cpp:349] Iteration 2200 (58.1241 iter/s, 1.72046s/100 iter), loss = 0.000698627
I0628 19:17:04.851248 32082 solver.cpp:371]     Train net output #0: loss = 0.000698626 (* 1 = 0.000698626 loss)
I0628 19:17:04.851254 32082 sgd_solver.cpp:137] Iteration 2200, lr = 0.00965625, m = 0.9
I0628 19:17:06.580906 32082 solver.cpp:349] Iteration 2300 (57.8433 iter/s, 1.72881s/100 iter), loss = 0.00200384
I0628 19:17:06.580934 32082 solver.cpp:371]     Train net output #0: loss = 0.00200384 (* 1 = 0.00200384 loss)
I0628 19:17:06.580955 32082 sgd_solver.cpp:137] Iteration 2300, lr = 0.00964062, m = 0.9
I0628 19:17:07.268123 32050 data_reader.cpp:262] Starting prefetch of epoch 3
I0628 19:17:08.316471 32082 solver.cpp:349] Iteration 2400 (57.6477 iter/s, 1.73467s/100 iter), loss = 0.0009961
I0628 19:17:08.316495 32082 solver.cpp:371]     Train net output #0: loss = 0.000996098 (* 1 = 0.000996098 loss)
I0628 19:17:08.316499 32082 sgd_solver.cpp:137] Iteration 2400, lr = 0.009625, m = 0.9
I0628 19:17:10.035967 32082 solver.cpp:349] Iteration 2500 (58.1857 iter/s, 1.71864s/100 iter), loss = 0.00135639
I0628 19:17:10.035993 32082 solver.cpp:371]     Train net output #0: loss = 0.00135639 (* 1 = 0.00135639 loss)
I0628 19:17:10.036000 32082 sgd_solver.cpp:137] Iteration 2500, lr = 0.00960938, m = 0.9
I0628 19:17:11.754714 32082 solver.cpp:349] Iteration 2600 (58.2111 iter/s, 1.71789s/100 iter), loss = 0.00163494
I0628 19:17:11.754740 32082 solver.cpp:371]     Train net output #0: loss = 0.00163493 (* 1 = 0.00163493 loss)
I0628 19:17:11.754746 32082 sgd_solver.cpp:137] Iteration 2600, lr = 0.00959375, m = 0.9
I0628 19:17:13.476811 32082 solver.cpp:349] Iteration 2700 (58.0977 iter/s, 1.72124s/100 iter), loss = 0.000746667
I0628 19:17:13.476833 32082 solver.cpp:371]     Train net output #0: loss = 0.000746666 (* 1 = 0.000746666 loss)
I0628 19:17:13.476837 32082 sgd_solver.cpp:137] Iteration 2700, lr = 0.00957812, m = 0.9
I0628 19:17:15.201035 32082 solver.cpp:349] Iteration 2800 (58.0256 iter/s, 1.72338s/100 iter), loss = 0.00188829
I0628 19:17:15.201062 32082 solver.cpp:371]     Train net output #0: loss = 0.00188829 (* 1 = 0.00188829 loss)
I0628 19:17:15.201068 32082 sgd_solver.cpp:137] Iteration 2800, lr = 0.0095625, m = 0.9
I0628 19:17:16.922005 32082 solver.cpp:349] Iteration 2900 (58.1356 iter/s, 1.72012s/100 iter), loss = 0.00103684
I0628 19:17:16.922030 32082 solver.cpp:371]     Train net output #0: loss = 0.00103684 (* 1 = 0.00103684 loss)
I0628 19:17:16.922036 32082 sgd_solver.cpp:137] Iteration 2900, lr = 0.00954687, m = 0.9
I0628 19:17:18.632292 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:18.633371 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:18.633379 32082 net.cpp:2170] conv1a_param_0(0.03) 
I0628 19:17:18.633388 32082 net.cpp:2170] conv1b_param_0(0.0599) 
I0628 19:17:18.633393 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:18.633396 32082 net.cpp:2170] res2a_branch2a_param_0(0.06) 
I0628 19:17:18.633401 32082 net.cpp:2170] res2a_branch2b_param_0(0.0599) 
I0628 19:17:18.633405 32082 net.cpp:2170] res3a_branch2a_param_0(0.06) 
I0628 19:17:18.633409 32082 net.cpp:2170] res3a_branch2b_param_0(0.06) 
I0628 19:17:18.633414 32082 net.cpp:2170] res4a_branch2a_param_0(0.06) 
I0628 19:17:18.633419 32082 net.cpp:2170] res4a_branch2b_param_0(0.06) 
I0628 19:17:18.633424 32082 net.cpp:2170] res5a_branch2a_param_0(0.06) 
I0628 19:17:18.633426 32082 net.cpp:2170] res5a_branch2b_param_0(0.06) 
I0628 19:17:18.633430 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (141202/2.3599e+06) 0.0598
I0628 19:17:18.633440 32082 solver.cpp:545] Iteration 3000, Testing net (#0)
I0628 19:17:19.631693 32080 data_reader.cpp:262] Starting prefetch of epoch 3
I0628 19:17:19.654022 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9158
I0628 19:17:19.654034 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:17:19.654039 32082 solver.cpp:630]     Test net output #2: loss = 0.299341 (* 1 = 0.299341 loss)
I0628 19:17:19.654053 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02013s
I0628 19:17:19.671375 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.08
I0628 19:17:19.833674 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:19.835153 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:19.835510 32082 solver.cpp:349] Iteration 3000 (34.3394 iter/s, 2.91211s/100 iter), loss = 0.000913233
I0628 19:17:19.835526 32082 solver.cpp:371]     Train net output #0: loss = 0.000913232 (* 1 = 0.000913232 loss)
I0628 19:17:19.835546 32082 sgd_solver.cpp:137] Iteration 3000, lr = 0.00953125, m = 0.9
I0628 19:17:21.557530 32082 solver.cpp:349] Iteration 3100 (58.0999 iter/s, 1.72117s/100 iter), loss = 0.00112214
I0628 19:17:21.557557 32082 solver.cpp:371]     Train net output #0: loss = 0.00112214 (* 1 = 0.00112214 loss)
I0628 19:17:21.557562 32082 sgd_solver.cpp:137] Iteration 3100, lr = 0.00951563, m = 0.9
I0628 19:17:21.919602 32050 data_reader.cpp:262] Starting prefetch of epoch 4
I0628 19:17:23.277473 32082 solver.cpp:349] Iteration 3200 (58.1698 iter/s, 1.71911s/100 iter), loss = 0.00111836
I0628 19:17:23.277501 32082 solver.cpp:371]     Train net output #0: loss = 0.00111835 (* 1 = 0.00111835 loss)
I0628 19:17:23.277508 32082 sgd_solver.cpp:137] Iteration 3200, lr = 0.0095, m = 0.9
I0628 19:17:24.996561 32082 solver.cpp:349] Iteration 3300 (58.1988 iter/s, 1.71825s/100 iter), loss = 0.000879399
I0628 19:17:24.996649 32082 solver.cpp:371]     Train net output #0: loss = 0.000879398 (* 1 = 0.000879398 loss)
I0628 19:17:24.996664 32082 sgd_solver.cpp:137] Iteration 3300, lr = 0.00948437, m = 0.9
I0628 19:17:26.714577 32082 solver.cpp:349] Iteration 3400 (58.2375 iter/s, 1.71711s/100 iter), loss = 0.000845185
I0628 19:17:26.714601 32082 solver.cpp:371]     Train net output #0: loss = 0.000845184 (* 1 = 0.000845184 loss)
I0628 19:17:26.714607 32082 sgd_solver.cpp:137] Iteration 3400, lr = 0.00946875, m = 0.9
I0628 19:17:28.436924 32082 solver.cpp:349] Iteration 3500 (58.0882 iter/s, 1.72152s/100 iter), loss = 0.00106532
I0628 19:17:28.436946 32082 solver.cpp:371]     Train net output #0: loss = 0.00106532 (* 1 = 0.00106532 loss)
I0628 19:17:28.436951 32082 sgd_solver.cpp:137] Iteration 3500, lr = 0.00945312, m = 0.9
I0628 19:17:30.154984 32082 solver.cpp:349] Iteration 3600 (58.2329 iter/s, 1.71724s/100 iter), loss = 0.00163842
I0628 19:17:30.155011 32082 solver.cpp:371]     Train net output #0: loss = 0.00163842 (* 1 = 0.00163842 loss)
I0628 19:17:30.155017 32082 sgd_solver.cpp:137] Iteration 3600, lr = 0.0094375, m = 0.9
I0628 19:17:31.874857 32082 solver.cpp:349] Iteration 3700 (58.1717 iter/s, 1.71905s/100 iter), loss = 0.00162913
I0628 19:17:31.874881 32082 solver.cpp:371]     Train net output #0: loss = 0.00162913 (* 1 = 0.00162913 loss)
I0628 19:17:31.874884 32082 sgd_solver.cpp:137] Iteration 3700, lr = 0.00942187, m = 0.9
I0628 19:17:33.596530 32082 solver.cpp:349] Iteration 3800 (58.1105 iter/s, 1.72086s/100 iter), loss = 0.00116374
I0628 19:17:33.596556 32082 solver.cpp:371]     Train net output #0: loss = 0.00116374 (* 1 = 0.00116374 loss)
I0628 19:17:33.596562 32082 sgd_solver.cpp:137] Iteration 3800, lr = 0.00940625, m = 0.9
I0628 19:17:35.319780 32082 solver.cpp:349] Iteration 3900 (58.0575 iter/s, 1.72243s/100 iter), loss = 0.00173968
I0628 19:17:35.319802 32082 solver.cpp:371]     Train net output #0: loss = 0.00173968 (* 1 = 0.00173968 loss)
I0628 19:17:35.319806 32082 sgd_solver.cpp:137] Iteration 3900, lr = 0.00939062, m = 0.9
I0628 19:17:35.373925 32050 data_reader.cpp:262] Starting prefetch of epoch 5
I0628 19:17:37.025796 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:37.026919 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:37.026927 32082 net.cpp:2170] conv1a_param_0(0.04) 
I0628 19:17:37.026934 32082 net.cpp:2170] conv1b_param_0(0.0799) 
I0628 19:17:37.026937 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:37.026942 32082 net.cpp:2170] res2a_branch2a_param_0(0.08) 
I0628 19:17:37.026945 32082 net.cpp:2170] res2a_branch2b_param_0(0.08) 
I0628 19:17:37.026948 32082 net.cpp:2170] res3a_branch2a_param_0(0.08) 
I0628 19:17:37.026953 32082 net.cpp:2170] res3a_branch2b_param_0(0.08) 
I0628 19:17:37.026957 32082 net.cpp:2170] res4a_branch2a_param_0(0.08) 
I0628 19:17:37.026962 32082 net.cpp:2170] res4a_branch2b_param_0(0.08) 
I0628 19:17:37.026965 32082 net.cpp:2170] res5a_branch2a_param_0(0.08) 
I0628 19:17:37.026969 32082 net.cpp:2170] res5a_branch2b_param_0(0.08) 
I0628 19:17:37.026973 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (188271/2.3599e+06) 0.0798
I0628 19:17:37.026985 32082 solver.cpp:545] Iteration 4000, Testing net (#0)
I0628 19:17:38.025363 32080 data_reader.cpp:262] Starting prefetch of epoch 4
I0628 19:17:38.047377 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:17:38.047390 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:17:38.047395 32082 solver.cpp:630]     Test net output #2: loss = 0.299908 (* 1 = 0.299908 loss)
I0628 19:17:38.047410 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01997s
I0628 19:17:38.064651 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.1
I0628 19:17:38.254528 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:38.255998 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:38.256356 32082 solver.cpp:349] Iteration 4000 (34.0689 iter/s, 2.93523s/100 iter), loss = 0.00120807
I0628 19:17:38.256384 32082 solver.cpp:371]     Train net output #0: loss = 0.00120807 (* 1 = 0.00120807 loss)
I0628 19:17:38.256391 32082 sgd_solver.cpp:137] Iteration 4000, lr = 0.009375, m = 0.9
I0628 19:17:39.980475 32082 solver.cpp:349] Iteration 4100 (58.028 iter/s, 1.72331s/100 iter), loss = 0.000839306
I0628 19:17:39.980501 32082 solver.cpp:371]     Train net output #0: loss = 0.000839304 (* 1 = 0.000839304 loss)
I0628 19:17:39.980507 32082 sgd_solver.cpp:137] Iteration 4100, lr = 0.00935937, m = 0.9
I0628 19:17:41.698829 32082 solver.cpp:349] Iteration 4200 (58.2225 iter/s, 1.71755s/100 iter), loss = 0.000959956
I0628 19:17:41.698856 32082 solver.cpp:371]     Train net output #0: loss = 0.000959954 (* 1 = 0.000959954 loss)
I0628 19:17:41.698863 32082 sgd_solver.cpp:137] Iteration 4200, lr = 0.00934375, m = 0.9
I0628 19:17:43.419827 32082 solver.cpp:349] Iteration 4300 (58.1329 iter/s, 1.7202s/100 iter), loss = 0.00184238
I0628 19:17:43.419849 32082 solver.cpp:371]     Train net output #0: loss = 0.00184238 (* 1 = 0.00184238 loss)
I0628 19:17:43.419853 32082 sgd_solver.cpp:137] Iteration 4300, lr = 0.00932813, m = 0.9
I0628 19:17:45.140874 32082 solver.cpp:349] Iteration 4400 (58.131 iter/s, 1.72025s/100 iter), loss = 0.000961925
I0628 19:17:45.140898 32082 solver.cpp:371]     Train net output #0: loss = 0.000961923 (* 1 = 0.000961923 loss)
I0628 19:17:45.140905 32082 sgd_solver.cpp:137] Iteration 4400, lr = 0.0093125, m = 0.9
I0628 19:17:46.864351 32082 solver.cpp:349] Iteration 4500 (58.0491 iter/s, 1.72268s/100 iter), loss = 0.00203632
I0628 19:17:46.864378 32082 solver.cpp:371]     Train net output #0: loss = 0.00203632 (* 1 = 0.00203632 loss)
I0628 19:17:46.864385 32082 sgd_solver.cpp:137] Iteration 4500, lr = 0.00929687, m = 0.9
I0628 19:17:48.587512 32082 solver.cpp:349] Iteration 4600 (58.0598 iter/s, 1.72236s/100 iter), loss = 0.00190979
I0628 19:17:48.587539 32082 solver.cpp:371]     Train net output #0: loss = 0.00190979 (* 1 = 0.00190979 loss)
I0628 19:17:48.587545 32082 sgd_solver.cpp:137] Iteration 4600, lr = 0.00928125, m = 0.9
I0628 19:17:50.032589 32050 data_reader.cpp:262] Starting prefetch of epoch 6
I0628 19:17:50.307451 32082 solver.cpp:349] Iteration 4700 (58.1684 iter/s, 1.71915s/100 iter), loss = 0.000908224
I0628 19:17:50.307479 32082 solver.cpp:371]     Train net output #0: loss = 0.000908222 (* 1 = 0.000908222 loss)
I0628 19:17:50.307485 32082 sgd_solver.cpp:137] Iteration 4700, lr = 0.00926562, m = 0.9
I0628 19:17:52.026530 32082 solver.cpp:349] Iteration 4800 (58.1974 iter/s, 1.71829s/100 iter), loss = 0.00193136
I0628 19:17:52.026556 32082 solver.cpp:371]     Train net output #0: loss = 0.00193135 (* 1 = 0.00193135 loss)
I0628 19:17:52.026561 32082 sgd_solver.cpp:137] Iteration 4800, lr = 0.00925, m = 0.9
I0628 19:17:53.750279 32082 solver.cpp:349] Iteration 4900 (58.0396 iter/s, 1.72296s/100 iter), loss = 0.000980431
I0628 19:17:53.750305 32082 solver.cpp:371]     Train net output #0: loss = 0.000980429 (* 1 = 0.000980429 loss)
I0628 19:17:53.750311 32082 sgd_solver.cpp:137] Iteration 4900, lr = 0.00923437, m = 0.9
I0628 19:17:55.459545 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:55.460713 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:55.460721 32082 net.cpp:2170] conv1a_param_0(0.0496) 
I0628 19:17:55.460729 32082 net.cpp:2170] conv1b_param_0(0.0998) 
I0628 19:17:55.460731 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:55.460733 32082 net.cpp:2170] res2a_branch2a_param_0(0.1) 
I0628 19:17:55.460736 32082 net.cpp:2170] res2a_branch2b_param_0(0.0999) 
I0628 19:17:55.460737 32082 net.cpp:2170] res3a_branch2a_param_0(0.1) 
I0628 19:17:55.460741 32082 net.cpp:2170] res3a_branch2b_param_0(0.1) 
I0628 19:17:55.460742 32082 net.cpp:2170] res4a_branch2a_param_0(0.1) 
I0628 19:17:55.460743 32082 net.cpp:2170] res4a_branch2b_param_0(0.1) 
I0628 19:17:55.460747 32082 net.cpp:2170] res5a_branch2a_param_0(0.1) 
I0628 19:17:55.460748 32082 net.cpp:2170] res5a_branch2b_param_0(0.1) 
I0628 19:17:55.460749 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (235344/2.3599e+06) 0.0997
I0628 19:17:55.460757 32082 solver.cpp:545] Iteration 5000, Testing net (#0)
I0628 19:17:56.459312 32080 data_reader.cpp:262] Starting prefetch of epoch 5
I0628 19:17:56.479486 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9168
I0628 19:17:56.479498 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:17:56.479503 32082 solver.cpp:630]     Test net output #2: loss = 0.299623 (* 1 = 0.299623 loss)
I0628 19:17:56.479517 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01832s
I0628 19:17:56.496773 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.12
I0628 19:17:56.688431 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:56.689913 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:56.690274 32082 solver.cpp:349] Iteration 5000 (34.0287 iter/s, 2.93869s/100 iter), loss = 0.000907788
I0628 19:17:56.690291 32082 solver.cpp:371]     Train net output #0: loss = 0.000907785 (* 1 = 0.000907785 loss)
I0628 19:17:56.690296 32082 sgd_solver.cpp:137] Iteration 5000, lr = 0.00921875, m = 0.9
I0628 19:17:58.414589 32082 solver.cpp:349] Iteration 5100 (58.0199 iter/s, 1.72355s/100 iter), loss = 0.0018882
I0628 19:17:58.414609 32082 solver.cpp:371]     Train net output #0: loss = 0.0018882 (* 1 = 0.0018882 loss)
I0628 19:17:58.414613 32082 sgd_solver.cpp:137] Iteration 5100, lr = 0.00920312, m = 0.9
I0628 19:18:00.136987 32082 solver.cpp:349] Iteration 5200 (58.0844 iter/s, 1.72163s/100 iter), loss = 0.00126935
I0628 19:18:00.137009 32082 solver.cpp:371]     Train net output #0: loss = 0.00126935 (* 1 = 0.00126935 loss)
I0628 19:18:00.137014 32082 sgd_solver.cpp:137] Iteration 5200, lr = 0.0091875, m = 0.9
I0628 19:18:01.862867 32082 solver.cpp:349] Iteration 5300 (57.9673 iter/s, 1.72511s/100 iter), loss = 0.00183895
I0628 19:18:01.862888 32082 solver.cpp:371]     Train net output #0: loss = 0.00183895 (* 1 = 0.00183895 loss)
I0628 19:18:01.862893 32082 sgd_solver.cpp:137] Iteration 5300, lr = 0.00917188, m = 0.9
I0628 19:18:03.586518 32082 solver.cpp:349] Iteration 5400 (58.0422 iter/s, 1.72288s/100 iter), loss = 0.000883252
I0628 19:18:03.586544 32082 solver.cpp:371]     Train net output #0: loss = 0.000883251 (* 1 = 0.000883251 loss)
I0628 19:18:03.586549 32082 sgd_solver.cpp:137] Iteration 5400, lr = 0.00915625, m = 0.9
I0628 19:18:04.702170 32050 data_reader.cpp:262] Starting prefetch of epoch 7
I0628 19:18:05.303620 32082 solver.cpp:349] Iteration 5500 (58.2636 iter/s, 1.71634s/100 iter), loss = 0.00131171
I0628 19:18:05.303647 32082 solver.cpp:371]     Train net output #0: loss = 0.00131171 (* 1 = 0.00131171 loss)
I0628 19:18:05.303653 32082 sgd_solver.cpp:137] Iteration 5500, lr = 0.00914062, m = 0.9
I0628 19:18:07.025302 32082 solver.cpp:349] Iteration 5600 (58.1086 iter/s, 1.72091s/100 iter), loss = 0.00159786
I0628 19:18:07.025326 32082 solver.cpp:371]     Train net output #0: loss = 0.00159785 (* 1 = 0.00159785 loss)
I0628 19:18:07.025333 32082 sgd_solver.cpp:137] Iteration 5600, lr = 0.009125, m = 0.9
I0628 19:18:08.745064 32082 solver.cpp:349] Iteration 5700 (58.1734 iter/s, 1.719s/100 iter), loss = 0.00153627
I0628 19:18:08.745112 32082 solver.cpp:371]     Train net output #0: loss = 0.00153627 (* 1 = 0.00153627 loss)
I0628 19:18:08.745120 32082 sgd_solver.cpp:137] Iteration 5700, lr = 0.00910938, m = 0.9
I0628 19:18:10.467553 32082 solver.cpp:349] Iteration 5800 (58.082 iter/s, 1.7217s/100 iter), loss = 0.00149291
I0628 19:18:10.467578 32082 solver.cpp:371]     Train net output #0: loss = 0.0014929 (* 1 = 0.0014929 loss)
I0628 19:18:10.467584 32082 sgd_solver.cpp:137] Iteration 5800, lr = 0.00909375, m = 0.9
I0628 19:18:12.189560 32082 solver.cpp:349] Iteration 5900 (58.0973 iter/s, 1.72125s/100 iter), loss = 0.00114046
I0628 19:18:12.189587 32082 solver.cpp:371]     Train net output #0: loss = 0.00114045 (* 1 = 0.00114045 loss)
I0628 19:18:12.189592 32082 sgd_solver.cpp:137] Iteration 5900, lr = 0.00907812, m = 0.9
I0628 19:18:13.893934 32082 solver.cpp:401] Sparsity after update:
I0628 19:18:13.895041 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:18:13.895051 32082 net.cpp:2170] conv1a_param_0(0.0596) 
I0628 19:18:13.895056 32082 net.cpp:2170] conv1b_param_0(0.12) 
I0628 19:18:13.895058 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:18:13.895061 32082 net.cpp:2170] res2a_branch2a_param_0(0.12) 
I0628 19:18:13.895062 32082 net.cpp:2170] res2a_branch2b_param_0(0.12) 
I0628 19:18:13.895066 32082 net.cpp:2170] res3a_branch2a_param_0(0.12) 
I0628 19:18:13.895067 32082 net.cpp:2170] res3a_branch2b_param_0(0.12) 
I0628 19:18:13.895071 32082 net.cpp:2170] res4a_branch2a_param_0(0.12) 
I0628 19:18:13.895072 32082 net.cpp:2170] res4a_branch2b_param_0(0.12) 
I0628 19:18:13.895074 32082 net.cpp:2170] res5a_branch2a_param_0(0.12) 
I0628 19:18:13.895076 32082 net.cpp:2170] res5a_branch2b_param_0(0.12) 
I0628 19:18:13.895078 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (282417/2.3599e+06) 0.12
I0628 19:18:13.895087 32082 solver.cpp:545] Iteration 6000, Testing net (#0)
I0628 19:18:14.895823 32080 data_reader.cpp:262] Starting prefetch of epoch 6
I0628 19:18:14.916054 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9162
I0628 19:18:14.916066 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:18:14.916074 32082 solver.cpp:630]     Test net output #2: loss = 0.297778 (* 1 = 0.297778 loss)
I0628 19:18:14.916090 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02058s
I0628 19:18:14.933434 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.14
I0628 19:18:15.158463 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:18:15.159934 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:18:15.160293 32082 solver.cpp:349] Iteration 6000 (33.6761 iter/s, 2.96947s/100 iter), loss = 0.00198984
I0628 19:18:15.160313 32082 solver.cpp:371]     Train net output #0: loss = 0.00198984 (* 1 = 0.00198984 loss)
I0628 19:18:15.160322 32082 sgd_solver.cpp:137] Iteration 6000, lr = 0.0090625, m = 0.9
I0628 19:18:16.883004 32082 solver.cpp:349] Iteration 6100 (58.0733 iter/s, 1.72196s/100 iter), loss = 0.00134146
I0628 19:18:16.883026 32082 solver.cpp:371]     Train net output #0: loss = 0.00134146 (* 1 = 0.00134146 loss)
I0628 19:18:16.883031 32082 sgd_solver.cpp:137] Iteration 6100, lr = 0.00904687, m = 0.9
I0628 19:18:18.604333 32082 solver.cpp:349] Iteration 6200 (58.1196 iter/s, 1.72059s/100 iter), loss = 0.00151677
I0628 19:18:18.604357 32082 solver.cpp:371]     Train net output #0: loss = 0.00151677 (* 1 = 0.00151677 loss)
I0628 19:18:18.604360 32082 sgd_solver.cpp:137] Iteration 6200, lr = 0.00903125, m = 0.9
I0628 19:18:19.395951 32050 data_reader.cpp:262] Starting prefetch of epoch 8
I0628 19:18:20.325747 32082 solver.cpp:349] Iteration 6300 (58.1167 iter/s, 1.72068s/100 iter), loss = 0.000867034
I0628 19:18:20.325773 32082 solver.cpp:371]     Train net output #0: loss = 0.000867032 (* 1 = 0.000867032 loss)
I0628 19:18:20.325778 32082 sgd_solver.cpp:137] Iteration 6300, lr = 0.00901563, m = 0.9
I0628 19:18:22.046751 32082 solver.cpp:349] Iteration 6400 (58.1307 iter/s, 1.72026s/100 iter), loss = 0.00102577
I0628 19:18:22.046794 32082 solver.cpp:371]     Train net output #0: loss = 0.00102576 (* 1 = 0.00102576 loss)
I0628 19:18:22.046800 32082 sgd_solver.cpp:137] Iteration 6400, lr = 0.009, m = 0.9
I0628 19:18:23.770651 32082 solver.cpp:349] Iteration 6500 (58.0335 iter/s, 1.72314s/100 iter), loss = 0.00104501
I0628 19:18:23.770678 32082 solver.cpp:371]     Train net output #0: loss = 0.00104501 (* 1 = 0.00104501 loss)
I0628 19:18:23.770684 32082 sgd_solver.cpp:137] Iteration 6500, lr = 0.00898437, m = 0.9
I0628 19:18:25.491950 32082 solver.cpp:349] Iteration 6600 (58.1206 iter/s, 1.72056s/100 iter), loss = 0.00224834
I0628 19:18:25.492019 32082 solver.cpp:371]     Train net output #0: loss = 0.00224834 (* 1 = 0.00224834 loss)
I0628 19:18:25.492027 32082 sgd_solver.cpp:137] Iteration 6600, lr = 0.00896875, m = 0.9
I0628 19:18:27.212291 32082 solver.cpp:349] Iteration 6700 (58.1544 iter/s, 1.71956s/100 iter), loss = 0.000830301
I0628 19:18:27.212319 32082 solver.cpp:371]     Train net output #0: loss = 0.000830299 (* 1 = 0.000830299 loss)
I0628 19:18:27.212326 32082 sgd_solver.cpp:137] Iteration 6700, lr = 0.00895312, m = 0.9
I0628 19:18:28.932799 32082 solver.cpp:349] Iteration 6800 (58.1472 iter/s, 1.71977s/100 iter), loss = 0.000736836
I0628 19:18:28.932822 32082 solver.cpp:371]     Train net output #0: loss = 0.000736834 (* 1 = 0.000736834 loss)
I0628 19:18:28.932828 32082 sgd_solver.cpp:137] Iteration 6800, lr = 0.0089375, m = 0.9
I0628 19:18:30.652338 32082 solver.cpp:349] Iteration 6900 (58.1798 iter/s, 1.71881s/100 iter), loss = 0.000978627
I0628 19:18:30.652364 32082 solver.cpp:371]     Train net output #0: loss = 0.000978625 (* 1 = 0.000978625 loss)
I0628 19:18:30.652370 32082 sgd_solver.cpp:137] Iteration 6900, lr = 0.00892187, m = 0.9
I0628 19:18:32.356318 32082 solver.cpp:401] Sparsity after update:
I0628 19:18:32.357453 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:18:32.357461 32082 net.cpp:2170] conv1a_param_0(0.0696) 
I0628 19:18:32.357468 32082 net.cpp:2170] conv1b_param_0(0.14) 
I0628 19:18:32.357472 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:18:32.357476 32082 net.cpp:2170] res2a_branch2a_param_0(0.14) 
I0628 19:18:32.357481 32082 net.cpp:2170] res2a_branch2b_param_0(0.14) 
I0628 19:18:32.357484 32082 net.cpp:2170] res3a_branch2a_param_0(0.14) 
I0628 19:18:32.357488 32082 net.cpp:2170] res3a_branch2b_param_0(0.14) 
I0628 19:18:32.357492 32082 net.cpp:2170] res4a_branch2a_param_0(0.14) 
I0628 19:18:32.357496 32082 net.cpp:2170] res4a_branch2b_param_0(0.14) 
I0628 19:18:32.357501 32082 net.cpp:2170] res5a_branch2a_param_0(0.14) 
I0628 19:18:32.357504 32082 net.cpp:2170] res5a_branch2b_param_0(0.14) 
I0628 19:18:32.357508 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (329487/2.3599e+06) 0.14
I0628 19:18:32.357520 32082 solver.cpp:545] Iteration 7000, Testing net (#0)
I0628 19:18:33.357944 32080 data_reader.cpp:262] Starting prefetch of epoch 7
I0628 19:18:33.378206 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9164
I0628 19:18:33.378219 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:18:33.378224 32082 solver.cpp:630]     Test net output #2: loss = 0.29925 (* 1 = 0.29925 loss)
I0628 19:18:33.378237 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02031s
I0628 19:18:33.395573 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.16
I0628 19:18:33.631631 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:18:33.633095 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:18:33.633455 32082 solver.cpp:349] Iteration 7000 (33.5583 iter/s, 2.97989s/100 iter), loss = 0.000955429
I0628 19:18:33.633472 32082 solver.cpp:371]     Train net output #0: loss = 0.000955427 (* 1 = 0.000955427 loss)
I0628 19:18:33.633478 32082 sgd_solver.cpp:137] Iteration 7000, lr = 0.00890625, m = 0.9
I0628 19:18:34.114519 32050 data_reader.cpp:262] Starting prefetch of epoch 9
I0628 19:18:35.355132 32082 solver.cpp:349] Iteration 7100 (58.107 iter/s, 1.72096s/100 iter), loss = 0.000786382
I0628 19:18:35.355159 32082 solver.cpp:371]     Train net output #0: loss = 0.00078638 (* 1 = 0.00078638 loss)
I0628 19:18:35.355165 32082 sgd_solver.cpp:137] Iteration 7100, lr = 0.00889063, m = 0.9
I0628 19:18:37.076681 32082 solver.cpp:349] Iteration 7200 (58.1117 iter/s, 1.72082s/100 iter), loss = 0.000819024
I0628 19:18:37.076707 32082 solver.cpp:371]     Train net output #0: loss = 0.000819022 (* 1 = 0.000819022 loss)
I0628 19:18:37.076714 32082 sgd_solver.cpp:137] Iteration 7200, lr = 0.008875, m = 0.9
I0628 19:18:38.804913 32082 solver.cpp:349] Iteration 7300 (57.8868 iter/s, 1.72751s/100 iter), loss = 0.00110974
I0628 19:18:38.804952 32082 solver.cpp:371]     Train net output #0: loss = 0.00110974 (* 1 = 0.00110974 loss)
I0628 19:18:38.804956 32082 sgd_solver.cpp:137] Iteration 7300, lr = 0.00885937, m = 0.9
I0628 19:18:40.528326 32082 solver.cpp:349] Iteration 7400 (58.0489 iter/s, 1.72269s/100 iter), loss = 0.00114124
I0628 19:18:40.528349 32082 solver.cpp:371]     Train net output #0: loss = 0.00114124 (* 1 = 0.00114124 loss)
I0628 19:18:40.528355 32082 sgd_solver.cpp:137] Iteration 7400, lr = 0.00884375, m = 0.9
I0628 19:18:42.247586 32082 solver.cpp:349] Iteration 7500 (58.1885 iter/s, 1.71855s/100 iter), loss = 0.0010139
I0628 19:18:42.247611 32082 solver.cpp:371]     Train net output #0: loss = 0.0010139 (* 1 = 0.0010139 loss)
I0628 19:18:42.247615 32082 sgd_solver.cpp:137] Iteration 7500, lr = 0.00882812, m = 0.9
I0628 19:18:43.968003 32082 solver.cpp:349] Iteration 7600 (58.1494 iter/s, 1.71971s/100 iter), loss = 0.00130514
I0628 19:18:43.968029 32082 solver.cpp:371]     Train net output #0: loss = 0.00130513 (* 1 = 0.00130513 loss)
I0628 19:18:43.968035 32082 sgd_solver.cpp:137] Iteration 7600, lr = 0.0088125, m = 0.9
I0628 19:18:45.688783 32082 solver.cpp:349] Iteration 7700 (58.1371 iter/s, 1.72007s/100 iter), loss = 0.0018141
I0628 19:18:45.688805 32082 solver.cpp:371]     Train net output #0: loss = 0.0018141 (* 1 = 0.0018141 loss)
I0628 19:18:45.688810 32082 sgd_solver.cpp:137] Iteration 7700, lr = 0.00879687, m = 0.9
I0628 19:18:47.417119 32082 solver.cpp:349] Iteration 7800 (57.8827 iter/s, 1.72763s/100 iter), loss = 0.00176653
I0628 19:18:47.417146 32082 solver.cpp:371]     Train net output #0: loss = 0.00176653 (* 1 = 0.00176653 loss)
I0628 19:18:47.417152 32082 sgd_solver.cpp:137] Iteration 7800, lr = 0.00878125, m = 0.9
I0628 19:18:47.574573 32050 data_reader.cpp:262] Starting prefetch of epoch 10
I0628 19:18:49.138737 32082 solver.cpp:349] Iteration 7900 (58.1087 iter/s, 1.72091s/100 iter), loss = 0.000712459
I0628 19:18:49.138763 32082 solver.cpp:371]     Train net output #0: loss = 0.000712456 (* 1 = 0.000712456 loss)
I0628 19:18:49.138768 32082 sgd_solver.cpp:137] Iteration 7900, lr = 0.00876562, m = 0.9
I0628 19:18:50.846983 32082 solver.cpp:401] Sparsity after update:
I0628 19:18:50.848057 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:18:50.848064 32082 net.cpp:2170] conv1a_param_0(0.0796) 
I0628 19:18:50.848071 32082 net.cpp:2170] conv1b_param_0(0.16) 
I0628 19:18:50.848074 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:18:50.848076 32082 net.cpp:2170] res2a_branch2a_param_0(0.16) 
I0628 19:18:50.848078 32082 net.cpp:2170] res2a_branch2b_param_0(0.16) 
I0628 19:18:50.848080 32082 net.cpp:2170] res3a_branch2a_param_0(0.16) 
I0628 19:18:50.848083 32082 net.cpp:2170] res3a_branch2b_param_0(0.16) 
I0628 19:18:50.848084 32082 net.cpp:2170] res4a_branch2a_param_0(0.16) 
I0628 19:18:50.848085 32082 net.cpp:2170] res4a_branch2b_param_0(0.16) 
I0628 19:18:50.848088 32082 net.cpp:2170] res5a_branch2a_param_0(0.16) 
I0628 19:18:50.848090 32082 net.cpp:2170] res5a_branch2b_param_0(0.16) 
I0628 19:18:50.848098 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (376561/2.3599e+06) 0.16
I0628 19:18:50.848105 32082 solver.cpp:545] Iteration 8000, Testing net (#0)
I0628 19:18:51.846923 32080 data_reader.cpp:262] Starting prefetch of epoch 8
I0628 19:18:51.867439 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9174
I0628 19:18:51.867451 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:18:51.867456 32082 solver.cpp:630]     Test net output #2: loss = 0.298238 (* 1 = 0.298238 loss)
I0628 19:18:51.867470 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01897s
I0628 19:18:51.884699 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.18
I0628 19:18:52.142195 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:18:52.143671 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:18:52.144024 32082 solver.cpp:349] Iteration 8000 (33.2878 iter/s, 3.0041s/100 iter), loss = 0.00093761
I0628 19:18:52.144052 32082 solver.cpp:371]     Train net output #0: loss = 0.000937607 (* 1 = 0.000937607 loss)
I0628 19:18:52.144057 32082 sgd_solver.cpp:137] Iteration 8000, lr = 0.00875, m = 0.9
I0628 19:18:53.865178 32082 solver.cpp:349] Iteration 8100 (58.1241 iter/s, 1.72046s/100 iter), loss = 0.000754828
I0628 19:18:53.865203 32082 solver.cpp:371]     Train net output #0: loss = 0.000754826 (* 1 = 0.000754826 loss)
I0628 19:18:53.865209 32082 sgd_solver.cpp:137] Iteration 8100, lr = 0.00873438, m = 0.9
I0628 19:18:55.582878 32082 solver.cpp:349] Iteration 8200 (58.2409 iter/s, 1.71701s/100 iter), loss = 0.0015611
I0628 19:18:55.582962 32082 solver.cpp:371]     Train net output #0: loss = 0.0015611 (* 1 = 0.0015611 loss)
I0628 19:18:55.582968 32082 sgd_solver.cpp:137] Iteration 8200, lr = 0.00871875, m = 0.9
I0628 19:18:57.303390 32082 solver.cpp:349] Iteration 8300 (58.1477 iter/s, 1.71976s/100 iter), loss = 0.000840607
I0628 19:18:57.303413 32082 solver.cpp:371]     Train net output #0: loss = 0.000840604 (* 1 = 0.000840604 loss)
I0628 19:18:57.303418 32082 sgd_solver.cpp:137] Iteration 8300, lr = 0.00870312, m = 0.9
I0628 19:18:59.023788 32082 solver.cpp:349] Iteration 8400 (58.1493 iter/s, 1.71971s/100 iter), loss = 0.00125962
I0628 19:18:59.023811 32082 solver.cpp:371]     Train net output #0: loss = 0.00125962 (* 1 = 0.00125962 loss)
I0628 19:18:59.023818 32082 sgd_solver.cpp:137] Iteration 8400, lr = 0.0086875, m = 0.9
I0628 19:19:00.742475 32082 solver.cpp:349] Iteration 8500 (58.2073 iter/s, 1.718s/100 iter), loss = 0.00143807
I0628 19:19:00.742523 32082 solver.cpp:371]     Train net output #0: loss = 0.00143807 (* 1 = 0.00143807 loss)
I0628 19:19:00.742532 32082 sgd_solver.cpp:137] Iteration 8500, lr = 0.00867188, m = 0.9
I0628 19:19:02.288385 32050 data_reader.cpp:262] Starting prefetch of epoch 11
I0628 19:19:02.460824 32082 solver.cpp:349] Iteration 8600 (58.2195 iter/s, 1.71764s/100 iter), loss = 0.00103549
I0628 19:19:02.460847 32082 solver.cpp:371]     Train net output #0: loss = 0.00103549 (* 1 = 0.00103549 loss)
I0628 19:19:02.460853 32082 sgd_solver.cpp:137] Iteration 8600, lr = 0.00865625, m = 0.9
I0628 19:19:04.179535 32082 solver.cpp:349] Iteration 8700 (58.2063 iter/s, 1.71803s/100 iter), loss = 0.00161488
I0628 19:19:04.179563 32082 solver.cpp:371]     Train net output #0: loss = 0.00161488 (* 1 = 0.00161488 loss)
I0628 19:19:04.179569 32082 sgd_solver.cpp:137] Iteration 8700, lr = 0.00864062, m = 0.9
I0628 19:19:05.901384 32082 solver.cpp:349] Iteration 8800 (58.1002 iter/s, 1.72117s/100 iter), loss = 0.00142391
I0628 19:19:05.901407 32082 solver.cpp:371]     Train net output #0: loss = 0.00142391 (* 1 = 0.00142391 loss)
I0628 19:19:05.901412 32082 sgd_solver.cpp:137] Iteration 8800, lr = 0.008625, m = 0.9
I0628 19:19:07.621522 32082 solver.cpp:349] Iteration 8900 (58.1577 iter/s, 1.71946s/100 iter), loss = 0.000880841
I0628 19:19:07.621552 32082 solver.cpp:371]     Train net output #0: loss = 0.000880838 (* 1 = 0.000880838 loss)
I0628 19:19:07.621559 32082 sgd_solver.cpp:137] Iteration 8900, lr = 0.00860937, m = 0.9
I0628 19:19:09.324519 32082 solver.cpp:401] Sparsity after update:
I0628 19:19:09.325896 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:19:09.325903 32082 net.cpp:2170] conv1a_param_0(0.0896) 
I0628 19:19:09.325912 32082 net.cpp:2170] conv1b_param_0(0.18) 
I0628 19:19:09.325917 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:19:09.325920 32082 net.cpp:2170] res2a_branch2a_param_0(0.18) 
I0628 19:19:09.325925 32082 net.cpp:2170] res2a_branch2b_param_0(0.18) 
I0628 19:19:09.325928 32082 net.cpp:2170] res3a_branch2a_param_0(0.18) 
I0628 19:19:09.325932 32082 net.cpp:2170] res3a_branch2b_param_0(0.18) 
I0628 19:19:09.325937 32082 net.cpp:2170] res4a_branch2a_param_0(0.18) 
I0628 19:19:09.325940 32082 net.cpp:2170] res4a_branch2b_param_0(0.18) 
I0628 19:19:09.325944 32082 net.cpp:2170] res5a_branch2a_param_0(0.18) 
I0628 19:19:09.325948 32082 net.cpp:2170] res5a_branch2b_param_0(0.18) 
I0628 19:19:09.325953 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (423633/2.3599e+06) 0.18
I0628 19:19:09.325963 32082 solver.cpp:545] Iteration 9000, Testing net (#0)
I0628 19:19:10.323429 32080 data_reader.cpp:262] Starting prefetch of epoch 9
I0628 19:19:10.344887 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9178
I0628 19:19:10.344899 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:19:10.344907 32082 solver.cpp:630]     Test net output #2: loss = 0.299471 (* 1 = 0.299471 loss)
I0628 19:19:10.344923 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01858s
I0628 19:19:10.362179 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.2
I0628 19:19:10.634727 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:19:10.636188 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:19:10.636544 32082 solver.cpp:349] Iteration 9000 (33.18 iter/s, 3.01387s/100 iter), loss = 0.00136166
I0628 19:19:10.636564 32082 solver.cpp:371]     Train net output #0: loss = 0.00136165 (* 1 = 0.00136165 loss)
I0628 19:19:10.636572 32082 sgd_solver.cpp:137] Iteration 9000, lr = 0.00859375, m = 0.9
I0628 19:19:12.358628 32082 solver.cpp:349] Iteration 9100 (58.0919 iter/s, 1.72141s/100 iter), loss = 0.000980862
I0628 19:19:12.358656 32082 solver.cpp:371]     Train net output #0: loss = 0.000980859 (* 1 = 0.000980859 loss)
I0628 19:19:12.358662 32082 sgd_solver.cpp:137] Iteration 9100, lr = 0.00857813, m = 0.9
I0628 19:19:14.080646 32082 solver.cpp:349] Iteration 9200 (58.0943 iter/s, 1.72134s/100 iter), loss = 0.000576059
I0628 19:19:14.080672 32082 solver.cpp:371]     Train net output #0: loss = 0.000576056 (* 1 = 0.000576056 loss)
I0628 19:19:14.080677 32082 sgd_solver.cpp:137] Iteration 9200, lr = 0.0085625, m = 0.9
I0628 19:19:15.801635 32082 solver.cpp:349] Iteration 9300 (58.1288 iter/s, 1.72032s/100 iter), loss = 0.002242
I0628 19:19:15.801661 32082 solver.cpp:371]     Train net output #0: loss = 0.002242 (* 1 = 0.002242 loss)
I0628 19:19:15.801667 32082 sgd_solver.cpp:137] Iteration 9300, lr = 0.00854687, m = 0.9
I0628 19:19:17.021692 32050 data_reader.cpp:262] Starting prefetch of epoch 12
I0628 19:19:17.520140 32082 solver.cpp:349] Iteration 9400 (58.2128 iter/s, 1.71784s/100 iter), loss = 0.00107085
I0628 19:19:17.520165 32082 solver.cpp:371]     Train net output #0: loss = 0.00107085 (* 1 = 0.00107085 loss)
I0628 19:19:17.520171 32082 sgd_solver.cpp:137] Iteration 9400, lr = 0.00853125, m = 0.9
I0628 19:19:19.243333 32082 solver.cpp:349] Iteration 9500 (58.0542 iter/s, 1.72253s/100 iter), loss = 0.00131783
I0628 19:19:19.243355 32082 solver.cpp:371]     Train net output #0: loss = 0.00131783 (* 1 = 0.00131783 loss)
I0628 19:19:19.243358 32082 sgd_solver.cpp:137] Iteration 9500, lr = 0.00851563, m = 0.9
I0628 19:19:20.968773 32082 solver.cpp:349] Iteration 9600 (57.9784 iter/s, 1.72478s/100 iter), loss = 0.00106778
I0628 19:19:20.968799 32082 solver.cpp:371]     Train net output #0: loss = 0.00106778 (* 1 = 0.00106778 loss)
I0628 19:19:20.968806 32082 sgd_solver.cpp:137] Iteration 9600, lr = 0.0085, m = 0.9
I0628 19:19:22.689841 32082 solver.cpp:349] Iteration 9700 (58.1259 iter/s, 1.7204s/100 iter), loss = 0.000689816
I0628 19:19:22.689862 32082 solver.cpp:371]     Train net output #0: loss = 0.000689813 (* 1 = 0.000689813 loss)
I0628 19:19:22.689867 32082 sgd_solver.cpp:137] Iteration 9700, lr = 0.00848437, m = 0.9
I0628 19:19:24.408540 32082 solver.cpp:349] Iteration 9800 (58.2056 iter/s, 1.71805s/100 iter), loss = 0.000979654
I0628 19:19:24.408563 32082 solver.cpp:371]     Train net output #0: loss = 0.000979651 (* 1 = 0.000979651 loss)
I0628 19:19:24.408567 32082 sgd_solver.cpp:137] Iteration 9800, lr = 0.00846875, m = 0.9
I0628 19:19:26.126735 32082 solver.cpp:349] Iteration 9900 (58.2227 iter/s, 1.71754s/100 iter), loss = 0.002617
I0628 19:19:26.126906 32082 solver.cpp:371]     Train net output #0: loss = 0.002617 (* 1 = 0.002617 loss)
I0628 19:19:26.126914 32082 sgd_solver.cpp:137] Iteration 9900, lr = 0.00845312, m = 0.9
I0628 19:19:27.831676 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0628 19:19:27.843545 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0628 19:19:27.846946 32082 solver.cpp:401] Sparsity after update:
I0628 19:19:27.848012 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:19:27.848021 32082 net.cpp:2170] conv1a_param_0(0.0996) 
I0628 19:19:27.848029 32082 net.cpp:2170] conv1b_param_0(0.2) 
I0628 19:19:27.848034 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:19:27.848038 32082 net.cpp:2170] res2a_branch2a_param_0(0.2) 
I0628 19:19:27.848042 32082 net.cpp:2170] res2a_branch2b_param_0(0.2) 
I0628 19:19:27.848047 32082 net.cpp:2170] res3a_branch2a_param_0(0.2) 
I0628 19:19:27.848052 32082 net.cpp:2170] res3a_branch2b_param_0(0.2) 
I0628 19:19:27.848057 32082 net.cpp:2170] res4a_branch2a_param_0(0.2) 
I0628 19:19:27.848060 32082 net.cpp:2170] res4a_branch2b_param_0(0.2) 
I0628 19:19:27.848064 32082 net.cpp:2170] res5a_branch2a_param_0(0.2) 
I0628 19:19:27.848069 32082 net.cpp:2170] res5a_branch2b_param_0(0.2) 
I0628 19:19:27.848073 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (470708/2.3599e+06) 0.199
I0628 19:19:27.848085 32082 solver.cpp:545] Iteration 10000, Testing net (#0)
I0628 19:19:28.848776 32080 data_reader.cpp:262] Starting prefetch of epoch 10
I0628 19:19:28.868971 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:19:28.868983 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:19:28.868988 32082 solver.cpp:630]     Test net output #2: loss = 0.297735 (* 1 = 0.297735 loss)
I0628 19:19:28.869000 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02055s
I0628 19:19:28.886283 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.22
I0628 19:19:29.187264 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:19:29.188745 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:19:29.189101 32082 solver.cpp:349] Iteration 10000 (32.6681 iter/s, 3.06109s/100 iter), loss = 0.000613237
I0628 19:19:29.189121 32082 solver.cpp:371]     Train net output #0: loss = 0.000613234 (* 1 = 0.000613234 loss)
I0628 19:19:29.189126 32082 sgd_solver.cpp:137] Iteration 10000, lr = 0.0084375, m = 0.9
I0628 19:19:30.911495 32082 solver.cpp:349] Iteration 10100 (58.0805 iter/s, 1.72175s/100 iter), loss = 0.0014592
I0628 19:19:30.911523 32082 solver.cpp:371]     Train net output #0: loss = 0.0014592 (* 1 = 0.0014592 loss)
I0628 19:19:30.911530 32082 sgd_solver.cpp:137] Iteration 10100, lr = 0.00842187, m = 0.9
I0628 19:19:31.823320 32050 data_reader.cpp:262] Starting prefetch of epoch 13
I0628 19:19:32.632098 32082 solver.cpp:349] Iteration 10200 (58.1413 iter/s, 1.71995s/100 iter), loss = 0.000780241
I0628 19:19:32.632122 32082 solver.cpp:371]     Train net output #0: loss = 0.000780238 (* 1 = 0.000780238 loss)
I0628 19:19:32.632128 32082 sgd_solver.cpp:137] Iteration 10200, lr = 0.00840625, m = 0.9
I0628 19:19:34.349612 32082 solver.cpp:349] Iteration 10300 (58.2456 iter/s, 1.71687s/100 iter), loss = 0.000969737
I0628 19:19:34.349638 32082 solver.cpp:371]     Train net output #0: loss = 0.000969734 (* 1 = 0.000969734 loss)
I0628 19:19:34.349644 32082 sgd_solver.cpp:137] Iteration 10300, lr = 0.00839063, m = 0.9
I0628 19:19:36.072269 32082 solver.cpp:349] Iteration 10400 (58.0717 iter/s, 1.72201s/100 iter), loss = 0.00253104
I0628 19:19:36.072294 32082 solver.cpp:371]     Train net output #0: loss = 0.00253104 (* 1 = 0.00253104 loss)
I0628 19:19:36.072301 32082 sgd_solver.cpp:137] Iteration 10400, lr = 0.008375, m = 0.9
I0628 19:19:37.795069 32082 solver.cpp:349] Iteration 10500 (58.0668 iter/s, 1.72215s/100 iter), loss = 0.00152859
I0628 19:19:37.795109 32082 solver.cpp:371]     Train net output #0: loss = 0.00152858 (* 1 = 0.00152858 loss)
I0628 19:19:37.795115 32082 sgd_solver.cpp:137] Iteration 10500, lr = 0.00835937, m = 0.9
I0628 19:19:39.516350 32082 solver.cpp:349] Iteration 10600 (58.1185 iter/s, 1.72062s/100 iter), loss = 0.00187302
I0628 19:19:39.516372 32082 solver.cpp:371]     Train net output #0: loss = 0.00187301 (* 1 = 0.00187301 loss)
I0628 19:19:39.516378 32082 sgd_solver.cpp:137] Iteration 10600, lr = 0.00834375, m = 0.9
I0628 19:19:41.234783 32082 solver.cpp:349] Iteration 10700 (58.2141 iter/s, 1.7178s/100 iter), loss = 0.000964664
I0628 19:19:41.234812 32082 solver.cpp:371]     Train net output #0: loss = 0.000964661 (* 1 = 0.000964661 loss)
I0628 19:19:41.234817 32082 sgd_solver.cpp:137] Iteration 10700, lr = 0.00832812, m = 0.9
I0628 19:19:42.953799 32082 solver.cpp:349] Iteration 10800 (58.1945 iter/s, 1.71837s/100 iter), loss = 0.00154089
I0628 19:19:42.953824 32082 solver.cpp:371]     Train net output #0: loss = 0.00154089 (* 1 = 0.00154089 loss)
I0628 19:19:42.953830 32082 sgd_solver.cpp:137] Iteration 10800, lr = 0.0083125, m = 0.9
I0628 19:19:44.675196 32082 solver.cpp:349] Iteration 10900 (58.1139 iter/s, 1.72076s/100 iter), loss = 0.00134306
I0628 19:19:44.675218 32082 solver.cpp:371]     Train net output #0: loss = 0.00134306 (* 1 = 0.00134306 loss)
I0628 19:19:44.675225 32082 sgd_solver.cpp:137] Iteration 10900, lr = 0.00829687, m = 0.9
I0628 19:19:45.258944 32050 data_reader.cpp:262] Starting prefetch of epoch 14
I0628 19:19:46.377391 32082 solver.cpp:401] Sparsity after update:
I0628 19:19:46.378463 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:19:46.378470 32082 net.cpp:2170] conv1a_param_0(0.11) 
I0628 19:19:46.378479 32082 net.cpp:2170] conv1b_param_0(0.22) 
I0628 19:19:46.378484 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:19:46.378487 32082 net.cpp:2170] res2a_branch2a_param_0(0.22) 
I0628 19:19:46.378492 32082 net.cpp:2170] res2a_branch2b_param_0(0.22) 
I0628 19:19:46.378496 32082 net.cpp:2170] res3a_branch2a_param_0(0.22) 
I0628 19:19:46.378501 32082 net.cpp:2170] res3a_branch2b_param_0(0.22) 
I0628 19:19:46.378505 32082 net.cpp:2170] res4a_branch2a_param_0(0.22) 
I0628 19:19:46.378510 32082 net.cpp:2170] res4a_branch2b_param_0(0.22) 
I0628 19:19:46.378515 32082 net.cpp:2170] res5a_branch2a_param_0(0.22) 
I0628 19:19:46.378518 32082 net.cpp:2170] res5a_branch2b_param_0(0.22) 
I0628 19:19:46.378523 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (517781/2.3599e+06) 0.219
I0628 19:19:46.378535 32082 solver.cpp:545] Iteration 11000, Testing net (#0)
I0628 19:19:47.378394 32080 data_reader.cpp:262] Starting prefetch of epoch 11
I0628 19:19:47.399004 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:19:47.399019 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:19:47.399024 32082 solver.cpp:630]     Test net output #2: loss = 0.298564 (* 1 = 0.298564 loss)
I0628 19:19:47.399037 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02015s
I0628 19:19:47.416344 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.24
I0628 19:19:47.737447 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:19:47.738919 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:19:47.739274 32082 solver.cpp:349] Iteration 11000 (32.6479 iter/s, 3.06299s/100 iter), loss = 0.00173886
I0628 19:19:47.739291 32082 solver.cpp:371]     Train net output #0: loss = 0.00173885 (* 1 = 0.00173885 loss)
I0628 19:19:47.739297 32082 sgd_solver.cpp:137] Iteration 11000, lr = 0.00828125, m = 0.9
I0628 19:19:49.458783 32082 solver.cpp:349] Iteration 11100 (58.1772 iter/s, 1.71889s/100 iter), loss = 0.000907674
I0628 19:19:49.458806 32082 solver.cpp:371]     Train net output #0: loss = 0.000907671 (* 1 = 0.000907671 loss)
I0628 19:19:49.458812 32082 sgd_solver.cpp:137] Iteration 11100, lr = 0.00826562, m = 0.9
I0628 19:19:51.180548 32082 solver.cpp:349] Iteration 11200 (58.1017 iter/s, 1.72112s/100 iter), loss = 0.00130449
I0628 19:19:51.180574 32082 solver.cpp:371]     Train net output #0: loss = 0.00130449 (* 1 = 0.00130449 loss)
I0628 19:19:51.180580 32082 sgd_solver.cpp:137] Iteration 11200, lr = 0.00825, m = 0.9
I0628 19:19:52.902299 32082 solver.cpp:349] Iteration 11300 (58.1017 iter/s, 1.72112s/100 iter), loss = 0.000824856
I0628 19:19:52.902326 32082 solver.cpp:371]     Train net output #0: loss = 0.000824853 (* 1 = 0.000824853 loss)
I0628 19:19:52.902333 32082 sgd_solver.cpp:137] Iteration 11300, lr = 0.00823438, m = 0.9
I0628 19:19:54.625238 32082 solver.cpp:349] Iteration 11400 (58.0616 iter/s, 1.72231s/100 iter), loss = 0.00111397
I0628 19:19:54.625259 32082 solver.cpp:371]     Train net output #0: loss = 0.00111397 (* 1 = 0.00111397 loss)
I0628 19:19:54.625263 32082 sgd_solver.cpp:137] Iteration 11400, lr = 0.00821875, m = 0.9
I0628 19:19:56.346091 32082 solver.cpp:349] Iteration 11500 (58.1316 iter/s, 1.72024s/100 iter), loss = 0.00137205
I0628 19:19:56.346154 32082 solver.cpp:371]     Train net output #0: loss = 0.00137205 (* 1 = 0.00137205 loss)
I0628 19:19:56.346161 32082 sgd_solver.cpp:137] Iteration 11500, lr = 0.00820312, m = 0.9
I0628 19:19:58.069077 32082 solver.cpp:349] Iteration 11600 (58.0612 iter/s, 1.72232s/100 iter), loss = 0.0014088
I0628 19:19:58.069123 32082 solver.cpp:371]     Train net output #0: loss = 0.0014088 (* 1 = 0.0014088 loss)
I0628 19:19:58.069136 32082 sgd_solver.cpp:137] Iteration 11600, lr = 0.0081875, m = 0.9
I0628 19:19:59.787880 32082 solver.cpp:349] Iteration 11700 (58.2023 iter/s, 1.71815s/100 iter), loss = 0.00168468
I0628 19:19:59.787906 32082 solver.cpp:371]     Train net output #0: loss = 0.00168468 (* 1 = 0.00168468 loss)
I0628 19:19:59.787912 32082 sgd_solver.cpp:137] Iteration 11700, lr = 0.00817188, m = 0.9
I0628 19:20:00.045784 32050 data_reader.cpp:262] Starting prefetch of epoch 15
I0628 19:20:01.506086 32082 solver.cpp:349] Iteration 11800 (58.2215 iter/s, 1.71758s/100 iter), loss = 0.00105685
I0628 19:20:01.506144 32082 solver.cpp:371]     Train net output #0: loss = 0.00105685 (* 1 = 0.00105685 loss)
I0628 19:20:01.506153 32082 sgd_solver.cpp:137] Iteration 11800, lr = 0.00815625, m = 0.9
I0628 19:20:03.226028 32082 solver.cpp:349] Iteration 11900 (58.1638 iter/s, 1.71928s/100 iter), loss = 0.00101841
I0628 19:20:03.226054 32082 solver.cpp:371]     Train net output #0: loss = 0.0010184 (* 1 = 0.0010184 loss)
I0628 19:20:03.226060 32082 sgd_solver.cpp:137] Iteration 11900, lr = 0.00814062, m = 0.9
I0628 19:20:04.932199 32082 solver.cpp:401] Sparsity after update:
I0628 19:20:04.933286 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:20:04.933295 32082 net.cpp:2170] conv1a_param_0(0.12) 
I0628 19:20:04.933300 32082 net.cpp:2170] conv1b_param_0(0.24) 
I0628 19:20:04.933301 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:20:04.933303 32082 net.cpp:2170] res2a_branch2a_param_0(0.24) 
I0628 19:20:04.933305 32082 net.cpp:2170] res2a_branch2b_param_0(0.24) 
I0628 19:20:04.933308 32082 net.cpp:2170] res3a_branch2a_param_0(0.24) 
I0628 19:20:04.933310 32082 net.cpp:2170] res3a_branch2b_param_0(0.24) 
I0628 19:20:04.933312 32082 net.cpp:2170] res4a_branch2a_param_0(0.24) 
I0628 19:20:04.933315 32082 net.cpp:2170] res4a_branch2b_param_0(0.24) 
I0628 19:20:04.933316 32082 net.cpp:2170] res5a_branch2a_param_0(0.24) 
I0628 19:20:04.933317 32082 net.cpp:2170] res5a_branch2b_param_0(0.24) 
I0628 19:20:04.933320 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (564843/2.3599e+06) 0.239
I0628 19:20:04.933328 32082 solver.cpp:545] Iteration 12000, Testing net (#0)
I0628 19:20:05.931288 32080 data_reader.cpp:262] Starting prefetch of epoch 12
I0628 19:20:05.954638 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:20:05.954651 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:20:05.954656 32082 solver.cpp:630]     Test net output #2: loss = 0.297628 (* 1 = 0.297628 loss)
I0628 19:20:05.954670 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.021s
I0628 19:20:05.971977 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.26
I0628 19:20:06.299933 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:20:06.301401 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:20:06.301760 32082 solver.cpp:349] Iteration 12000 (32.5238 iter/s, 3.07467s/100 iter), loss = 0.00180383
I0628 19:20:06.301779 32082 solver.cpp:371]     Train net output #0: loss = 0.00180382 (* 1 = 0.00180382 loss)
I0628 19:20:06.301785 32082 sgd_solver.cpp:137] Iteration 12000, lr = 0.008125, m = 0.9
I0628 19:20:08.025883 32082 solver.cpp:349] Iteration 12100 (58.021 iter/s, 1.72351s/100 iter), loss = 0.00162371
I0628 19:20:08.025907 32082 solver.cpp:371]     Train net output #0: loss = 0.0016237 (* 1 = 0.0016237 loss)
I0628 19:20:08.025913 32082 sgd_solver.cpp:137] Iteration 12100, lr = 0.00810937, m = 0.9
I0628 19:20:09.747210 32082 solver.cpp:349] Iteration 12200 (58.1154 iter/s, 1.72071s/100 iter), loss = 0.00101637
I0628 19:20:09.747251 32082 solver.cpp:371]     Train net output #0: loss = 0.00101637 (* 1 = 0.00101637 loss)
I0628 19:20:09.747256 32082 sgd_solver.cpp:137] Iteration 12200, lr = 0.00809375, m = 0.9
I0628 19:20:11.465023 32082 solver.cpp:349] Iteration 12300 (58.2348 iter/s, 1.71719s/100 iter), loss = 0.00271192
I0628 19:20:11.465049 32082 solver.cpp:371]     Train net output #0: loss = 0.00271192 (* 1 = 0.00271192 loss)
I0628 19:20:11.465054 32082 sgd_solver.cpp:137] Iteration 12300, lr = 0.00807813, m = 0.9
I0628 19:20:13.189365 32082 solver.cpp:349] Iteration 12400 (58.0137 iter/s, 1.72373s/100 iter), loss = 0.000604328
I0628 19:20:13.189393 32082 solver.cpp:371]     Train net output #0: loss = 0.000604324 (* 1 = 0.000604324 loss)
I0628 19:20:13.189399 32082 sgd_solver.cpp:137] Iteration 12400, lr = 0.0080625, m = 0.9
I0628 19:20:14.840718 32050 data_reader.cpp:262] Starting prefetch of epoch 16
I0628 19:20:14.909363 32082 solver.cpp:349] Iteration 12500 (58.1602 iter/s, 1.71939s/100 iter), loss = 0.00110945
I0628 19:20:14.909390 32082 solver.cpp:371]     Train net output #0: loss = 0.00110944 (* 1 = 0.00110944 loss)
I0628 19:20:14.909396 32082 sgd_solver.cpp:137] Iteration 12500, lr = 0.00804687, m = 0.9
I0628 19:20:16.630746 32082 solver.cpp:349] Iteration 12600 (58.1134 iter/s, 1.72077s/100 iter), loss = 0.000995209
I0628 19:20:16.630769 32082 solver.cpp:371]     Train net output #0: loss = 0.000995206 (* 1 = 0.000995206 loss)
I0628 19:20:16.630775 32082 sgd_solver.cpp:137] Iteration 12600, lr = 0.00803125, m = 0.9
I0628 19:20:18.350015 32082 solver.cpp:349] Iteration 12700 (58.1846 iter/s, 1.71867s/100 iter), loss = 0.00212596
I0628 19:20:18.350040 32082 solver.cpp:371]     Train net output #0: loss = 0.00212596 (* 1 = 0.00212596 loss)
I0628 19:20:18.350046 32082 sgd_solver.cpp:137] Iteration 12700, lr = 0.00801562, m = 0.9
I0628 19:20:20.068261 32082 solver.cpp:349] Iteration 12800 (58.2193 iter/s, 1.71764s/100 iter), loss = 0.00214719
I0628 19:20:20.068287 32082 solver.cpp:371]     Train net output #0: loss = 0.00214718 (* 1 = 0.00214718 loss)
I0628 19:20:20.068294 32082 sgd_solver.cpp:137] Iteration 12800, lr = 0.008, m = 0.9
I0628 19:20:21.790199 32082 solver.cpp:349] Iteration 12900 (58.0945 iter/s, 1.72133s/100 iter), loss = 0.00180843
I0628 19:20:21.790225 32082 solver.cpp:371]     Train net output #0: loss = 0.00180843 (* 1 = 0.00180843 loss)
I0628 19:20:21.790230 32082 sgd_solver.cpp:137] Iteration 12900, lr = 0.00798437, m = 0.9
I0628 19:20:23.490283 32082 solver.cpp:401] Sparsity after update:
I0628 19:20:23.491392 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:20:23.491400 32082 net.cpp:2170] conv1a_param_0(0.13) 
I0628 19:20:23.491406 32082 net.cpp:2170] conv1b_param_0(0.26) 
I0628 19:20:23.491408 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:20:23.491411 32082 net.cpp:2170] res2a_branch2a_param_0(0.26) 
I0628 19:20:23.491413 32082 net.cpp:2170] res2a_branch2b_param_0(0.26) 
I0628 19:20:23.491415 32082 net.cpp:2170] res3a_branch2a_param_0(0.26) 
I0628 19:20:23.491417 32082 net.cpp:2170] res3a_branch2b_param_0(0.26) 
I0628 19:20:23.491420 32082 net.cpp:2170] res4a_branch2a_param_0(0.26) 
I0628 19:20:23.491421 32082 net.cpp:2170] res4a_branch2b_param_0(0.26) 
I0628 19:20:23.491423 32082 net.cpp:2170] res5a_branch2a_param_0(0.26) 
I0628 19:20:23.491425 32082 net.cpp:2170] res5a_branch2b_param_0(0.26) 
I0628 19:20:23.491427 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (611917/2.3599e+06) 0.259
I0628 19:20:23.491435 32082 solver.cpp:545] Iteration 13000, Testing net (#0)
I0628 19:20:24.491652 32080 data_reader.cpp:262] Starting prefetch of epoch 13
I0628 19:20:24.511811 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:20:24.511824 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:20:24.511829 32082 solver.cpp:630]     Test net output #2: loss = 0.297317 (* 1 = 0.297317 loss)
I0628 19:20:24.511842 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02008s
I0628 19:20:24.529108 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.28
I0628 19:20:24.891427 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:20:24.892896 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:20:24.893254 32082 solver.cpp:349] Iteration 13000 (32.2371 iter/s, 3.10201s/100 iter), loss = 0.000840855
I0628 19:20:24.893271 32082 solver.cpp:371]     Train net output #0: loss = 0.000840851 (* 1 = 0.000840851 loss)
I0628 19:20:24.893280 32082 sgd_solver.cpp:137] Iteration 13000, lr = 0.00796875, m = 0.9
I0628 19:20:26.619350 32082 solver.cpp:349] Iteration 13100 (57.9542 iter/s, 1.7255s/100 iter), loss = 0.00142433
I0628 19:20:26.619418 32082 solver.cpp:371]     Train net output #0: loss = 0.00142432 (* 1 = 0.00142432 loss)
I0628 19:20:26.619426 32082 sgd_solver.cpp:137] Iteration 13100, lr = 0.00795313, m = 0.9
I0628 19:20:28.341728 32082 solver.cpp:349] Iteration 13200 (58.0811 iter/s, 1.72173s/100 iter), loss = 0.00166087
I0628 19:20:28.341753 32082 solver.cpp:371]     Train net output #0: loss = 0.00166087 (* 1 = 0.00166087 loss)
I0628 19:20:28.341759 32082 sgd_solver.cpp:137] Iteration 13200, lr = 0.0079375, m = 0.9
I0628 19:20:29.684077 32050 data_reader.cpp:262] Starting prefetch of epoch 17
I0628 19:20:30.062101 32082 solver.cpp:349] Iteration 13300 (58.1471 iter/s, 1.71978s/100 iter), loss = 0.000912278
I0628 19:20:30.062125 32082 solver.cpp:371]     Train net output #0: loss = 0.000912274 (* 1 = 0.000912274 loss)
I0628 19:20:30.062131 32082 sgd_solver.cpp:137] Iteration 13300, lr = 0.00792187, m = 0.9
I0628 19:20:31.781476 32082 solver.cpp:349] Iteration 13400 (58.1807 iter/s, 1.71878s/100 iter), loss = 0.00118087
I0628 19:20:31.781500 32082 solver.cpp:371]     Train net output #0: loss = 0.00118086 (* 1 = 0.00118086 loss)
I0628 19:20:31.781507 32082 sgd_solver.cpp:137] Iteration 13400, lr = 0.00790625, m = 0.9
I0628 19:20:33.503947 32082 solver.cpp:349] Iteration 13500 (58.076 iter/s, 1.72188s/100 iter), loss = 0.00181193
I0628 19:20:33.503973 32082 solver.cpp:371]     Train net output #0: loss = 0.00181193 (* 1 = 0.00181193 loss)
I0628 19:20:33.503978 32082 sgd_solver.cpp:137] Iteration 13500, lr = 0.00789062, m = 0.9
I0628 19:20:35.224673 32082 solver.cpp:349] Iteration 13600 (58.1349 iter/s, 1.72014s/100 iter), loss = 0.002209
I0628 19:20:35.224695 32082 solver.cpp:371]     Train net output #0: loss = 0.002209 (* 1 = 0.002209 loss)
I0628 19:20:35.224699 32082 sgd_solver.cpp:137] Iteration 13600, lr = 0.007875, m = 0.9
I0628 19:20:36.947293 32082 solver.cpp:349] Iteration 13700 (58.0707 iter/s, 1.72204s/100 iter), loss = 0.00297785
I0628 19:20:36.947314 32082 solver.cpp:371]     Train net output #0: loss = 0.00297785 (* 1 = 0.00297785 loss)
I0628 19:20:36.947319 32082 sgd_solver.cpp:137] Iteration 13700, lr = 0.00785937, m = 0.9
I0628 19:20:38.674298 32082 solver.cpp:349] Iteration 13800 (57.9232 iter/s, 1.72642s/100 iter), loss = 0.00194699
I0628 19:20:38.674324 32082 solver.cpp:371]     Train net output #0: loss = 0.00194698 (* 1 = 0.00194698 loss)
I0628 19:20:38.674330 32082 sgd_solver.cpp:137] Iteration 13800, lr = 0.00784375, m = 0.9
I0628 19:20:40.393911 32082 solver.cpp:349] Iteration 13900 (58.1724 iter/s, 1.71903s/100 iter), loss = 0.00165088
I0628 19:20:40.393939 32082 solver.cpp:371]     Train net output #0: loss = 0.00165087 (* 1 = 0.00165087 loss)
I0628 19:20:40.393944 32082 sgd_solver.cpp:137] Iteration 13900, lr = 0.00782812, m = 0.9
I0628 19:20:42.096611 32082 solver.cpp:401] Sparsity after update:
I0628 19:20:42.097736 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:20:42.097745 32082 net.cpp:2170] conv1a_param_0(0.14) 
I0628 19:20:42.097751 32082 net.cpp:2170] conv1b_param_0(0.28) 
I0628 19:20:42.097754 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:20:42.097755 32082 net.cpp:2170] res2a_branch2a_param_0(0.28) 
I0628 19:20:42.097757 32082 net.cpp:2170] res2a_branch2b_param_0(0.28) 
I0628 19:20:42.097759 32082 net.cpp:2170] res3a_branch2a_param_0(0.28) 
I0628 19:20:42.097761 32082 net.cpp:2170] res3a_branch2b_param_0(0.28) 
I0628 19:20:42.097762 32082 net.cpp:2170] res4a_branch2a_param_0(0.28) 
I0628 19:20:42.097764 32082 net.cpp:2170] res4a_branch2b_param_0(0.28) 
I0628 19:20:42.097766 32082 net.cpp:2170] res5a_branch2a_param_0(0.28) 
I0628 19:20:42.097769 32082 net.cpp:2170] res5a_branch2b_param_0(0.28) 
I0628 19:20:42.097770 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (658990/2.3599e+06) 0.279
I0628 19:20:42.097776 32082 solver.cpp:545] Iteration 14000, Testing net (#0)
I0628 19:20:43.097185 32080 data_reader.cpp:262] Starting prefetch of epoch 14
I0628 19:20:43.121727 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:20:43.121740 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:20:43.121760 32082 solver.cpp:630]     Test net output #2: loss = 0.297584 (* 1 = 0.297584 loss)
I0628 19:20:43.121775 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02367s
I0628 19:20:43.139058 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.3
I0628 19:20:43.508545 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:20:43.510035 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:20:43.510392 32082 solver.cpp:349] Iteration 14000 (32.098 iter/s, 3.11546s/100 iter), loss = 0.00085541
I0628 19:20:43.510411 32082 solver.cpp:371]     Train net output #0: loss = 0.000855407 (* 1 = 0.000855407 loss)
I0628 19:20:43.510417 32082 sgd_solver.cpp:137] Iteration 14000, lr = 0.0078125, m = 0.9
I0628 19:20:44.524830 32050 data_reader.cpp:262] Starting prefetch of epoch 18
I0628 19:20:45.233043 32082 solver.cpp:349] Iteration 14100 (58.0695 iter/s, 1.72208s/100 iter), loss = 0.00177509
I0628 19:20:45.233067 32082 solver.cpp:371]     Train net output #0: loss = 0.00177508 (* 1 = 0.00177508 loss)
I0628 19:20:45.233074 32082 sgd_solver.cpp:137] Iteration 14100, lr = 0.00779688, m = 0.9
I0628 19:20:46.954504 32082 solver.cpp:349] Iteration 14200 (58.1098 iter/s, 1.72088s/100 iter), loss = 0.00186602
I0628 19:20:46.954530 32082 solver.cpp:371]     Train net output #0: loss = 0.00186602 (* 1 = 0.00186602 loss)
I0628 19:20:46.954535 32082 sgd_solver.cpp:137] Iteration 14200, lr = 0.00778125, m = 0.9
I0628 19:20:48.674077 32082 solver.cpp:349] Iteration 14300 (58.1735 iter/s, 1.71899s/100 iter), loss = 0.00137069
I0628 19:20:48.674101 32082 solver.cpp:371]     Train net output #0: loss = 0.00137069 (* 1 = 0.00137069 loss)
I0628 19:20:48.674106 32082 sgd_solver.cpp:137] Iteration 14300, lr = 0.00776563, m = 0.9
I0628 19:20:50.399368 32082 solver.cpp:349] Iteration 14400 (57.9806 iter/s, 1.72471s/100 iter), loss = 0.000982747
I0628 19:20:50.399389 32082 solver.cpp:371]     Train net output #0: loss = 0.000982743 (* 1 = 0.000982743 loss)
I0628 19:20:50.399394 32082 sgd_solver.cpp:137] Iteration 14400, lr = 0.00775, m = 0.9
I0628 19:20:52.116974 32082 solver.cpp:349] Iteration 14500 (58.2398 iter/s, 1.71704s/100 iter), loss = 0.00206265
I0628 19:20:52.117000 32082 solver.cpp:371]     Train net output #0: loss = 0.00206265 (* 1 = 0.00206265 loss)
I0628 19:20:52.117005 32082 sgd_solver.cpp:137] Iteration 14500, lr = 0.00773437, m = 0.9
I0628 19:20:53.840986 32082 solver.cpp:349] Iteration 14600 (58.0236 iter/s, 1.72344s/100 iter), loss = 0.000885526
I0628 19:20:53.841011 32082 solver.cpp:371]     Train net output #0: loss = 0.000885522 (* 1 = 0.000885522 loss)
I0628 19:20:53.841015 32082 sgd_solver.cpp:137] Iteration 14600, lr = 0.00771875, m = 0.9
I0628 19:20:55.558038 32082 solver.cpp:349] Iteration 14700 (58.2587 iter/s, 1.71648s/100 iter), loss = 0.00121897
I0628 19:20:55.558065 32082 solver.cpp:371]     Train net output #0: loss = 0.00121896 (* 1 = 0.00121896 loss)
I0628 19:20:55.558071 32082 sgd_solver.cpp:137] Iteration 14700, lr = 0.00770312, m = 0.9
I0628 19:20:57.283500 32082 solver.cpp:349] Iteration 14800 (57.9748 iter/s, 1.72489s/100 iter), loss = 0.00199677
I0628 19:20:57.283588 32082 solver.cpp:371]     Train net output #0: loss = 0.00199677 (* 1 = 0.00199677 loss)
I0628 19:20:57.283596 32082 sgd_solver.cpp:137] Iteration 14800, lr = 0.0076875, m = 0.9
I0628 19:20:57.974362 32050 data_reader.cpp:262] Starting prefetch of epoch 19
I0628 19:20:59.005138 32082 solver.cpp:349] Iteration 14900 (58.1056 iter/s, 1.721s/100 iter), loss = 0.000870252
I0628 19:20:59.005161 32082 solver.cpp:371]     Train net output #0: loss = 0.000870248 (* 1 = 0.000870248 loss)
I0628 19:20:59.005164 32082 sgd_solver.cpp:137] Iteration 14900, lr = 0.00767187, m = 0.9
I0628 19:21:00.714540 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:00.715612 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:00.715621 32082 net.cpp:2170] conv1a_param_0(0.15) 
I0628 19:21:00.715627 32082 net.cpp:2170] conv1b_param_0(0.3) 
I0628 19:21:00.715631 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:00.715639 32082 net.cpp:2170] res2a_branch2a_param_0(0.3) 
I0628 19:21:00.715642 32082 net.cpp:2170] res2a_branch2b_param_0(0.3) 
I0628 19:21:00.715644 32082 net.cpp:2170] res3a_branch2a_param_0(0.3) 
I0628 19:21:00.715646 32082 net.cpp:2170] res3a_branch2b_param_0(0.3) 
I0628 19:21:00.715648 32082 net.cpp:2170] res4a_branch2a_param_0(0.3) 
I0628 19:21:00.715651 32082 net.cpp:2170] res4a_branch2b_param_0(0.3) 
I0628 19:21:00.715654 32082 net.cpp:2170] res5a_branch2a_param_0(0.3) 
I0628 19:21:00.715657 32082 net.cpp:2170] res5a_branch2b_param_0(0.3) 
I0628 19:21:00.715662 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (706067/2.3599e+06) 0.299
I0628 19:21:00.715670 32082 solver.cpp:545] Iteration 15000, Testing net (#0)
I0628 19:21:01.712234 32080 data_reader.cpp:262] Starting prefetch of epoch 15
I0628 19:21:01.735826 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9172
I0628 19:21:01.735837 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:21:01.735842 32082 solver.cpp:630]     Test net output #2: loss = 0.296975 (* 1 = 0.296975 loss)
I0628 19:21:01.735857 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01987s
I0628 19:21:01.753106 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.32
I0628 19:21:02.145108 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:02.146592 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:02.146950 32082 solver.cpp:349] Iteration 15000 (31.8388 iter/s, 3.14082s/100 iter), loss = 0.00143987
I0628 19:21:02.146968 32082 solver.cpp:371]     Train net output #0: loss = 0.00143987 (* 1 = 0.00143987 loss)
I0628 19:21:02.146973 32082 sgd_solver.cpp:137] Iteration 15000, lr = 0.00765625, m = 0.9
I0628 19:21:03.867398 32082 solver.cpp:349] Iteration 15100 (58.1432 iter/s, 1.71989s/100 iter), loss = 0.00275538
I0628 19:21:03.867418 32082 solver.cpp:371]     Train net output #0: loss = 0.00275538 (* 1 = 0.00275538 loss)
I0628 19:21:03.867421 32082 sgd_solver.cpp:137] Iteration 15100, lr = 0.00764062, m = 0.9
I0628 19:21:05.586139 32082 solver.cpp:349] Iteration 15200 (58.2009 iter/s, 1.71819s/100 iter), loss = 0.000795216
I0628 19:21:05.586163 32082 solver.cpp:371]     Train net output #0: loss = 0.000795213 (* 1 = 0.000795213 loss)
I0628 19:21:05.586169 32082 sgd_solver.cpp:137] Iteration 15200, lr = 0.007625, m = 0.9
I0628 19:21:07.306494 32082 solver.cpp:349] Iteration 15300 (58.1466 iter/s, 1.71979s/100 iter), loss = 0.00182629
I0628 19:21:07.306520 32082 solver.cpp:371]     Train net output #0: loss = 0.00182629 (* 1 = 0.00182629 loss)
I0628 19:21:07.306526 32082 sgd_solver.cpp:137] Iteration 15300, lr = 0.00760937, m = 0.9
I0628 19:21:09.026090 32082 solver.cpp:349] Iteration 15400 (58.1722 iter/s, 1.71903s/100 iter), loss = 0.00124094
I0628 19:21:09.026115 32082 solver.cpp:371]     Train net output #0: loss = 0.00124094 (* 1 = 0.00124094 loss)
I0628 19:21:09.026121 32082 sgd_solver.cpp:137] Iteration 15400, lr = 0.00759375, m = 0.9
I0628 19:21:10.747356 32082 solver.cpp:349] Iteration 15500 (58.1157 iter/s, 1.72071s/100 iter), loss = 0.0026744
I0628 19:21:10.747397 32082 solver.cpp:371]     Train net output #0: loss = 0.0026744 (* 1 = 0.0026744 loss)
I0628 19:21:10.747403 32082 sgd_solver.cpp:137] Iteration 15500, lr = 0.00757812, m = 0.9
I0628 19:21:12.463558 32082 solver.cpp:349] Iteration 15600 (58.2877 iter/s, 1.71563s/100 iter), loss = 0.00154356
I0628 19:21:12.463583 32082 solver.cpp:371]     Train net output #0: loss = 0.00154356 (* 1 = 0.00154356 loss)
I0628 19:21:12.463589 32082 sgd_solver.cpp:137] Iteration 15600, lr = 0.0075625, m = 0.9
I0628 19:21:12.827086 32050 data_reader.cpp:262] Starting prefetch of epoch 20
I0628 19:21:14.183971 32082 solver.cpp:349] Iteration 15700 (58.1445 iter/s, 1.71985s/100 iter), loss = 0.00130775
I0628 19:21:14.183997 32082 solver.cpp:371]     Train net output #0: loss = 0.00130774 (* 1 = 0.00130774 loss)
I0628 19:21:14.184005 32082 sgd_solver.cpp:137] Iteration 15700, lr = 0.00754687, m = 0.9
I0628 19:21:15.903528 32082 solver.cpp:349] Iteration 15800 (58.1733 iter/s, 1.719s/100 iter), loss = 0.00106151
I0628 19:21:15.903551 32082 solver.cpp:371]     Train net output #0: loss = 0.00106151 (* 1 = 0.00106151 loss)
I0628 19:21:15.903555 32082 sgd_solver.cpp:137] Iteration 15800, lr = 0.00753125, m = 0.9
I0628 19:21:17.630542 32082 solver.cpp:349] Iteration 15900 (57.9219 iter/s, 1.72646s/100 iter), loss = 0.00107149
I0628 19:21:17.630566 32082 solver.cpp:371]     Train net output #0: loss = 0.00107149 (* 1 = 0.00107149 loss)
I0628 19:21:17.630573 32082 sgd_solver.cpp:137] Iteration 15900, lr = 0.00751562, m = 0.9
I0628 19:21:19.334621 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:19.335716 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:19.335726 32082 net.cpp:2170] conv1a_param_0(0.16) 
I0628 19:21:19.335733 32082 net.cpp:2170] conv1b_param_0(0.32) 
I0628 19:21:19.335738 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:19.335742 32082 net.cpp:2170] res2a_branch2a_param_0(0.32) 
I0628 19:21:19.335747 32082 net.cpp:2170] res2a_branch2b_param_0(0.32) 
I0628 19:21:19.335750 32082 net.cpp:2170] res3a_branch2a_param_0(0.32) 
I0628 19:21:19.335755 32082 net.cpp:2170] res3a_branch2b_param_0(0.32) 
I0628 19:21:19.335759 32082 net.cpp:2170] res4a_branch2a_param_0(0.32) 
I0628 19:21:19.335764 32082 net.cpp:2170] res4a_branch2b_param_0(0.32) 
I0628 19:21:19.335768 32082 net.cpp:2170] res5a_branch2a_param_0(0.32) 
I0628 19:21:19.335772 32082 net.cpp:2170] res5a_branch2b_param_0(0.32) 
I0628 19:21:19.335777 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (753133/2.3599e+06) 0.319
I0628 19:21:19.335788 32082 solver.cpp:545] Iteration 16000, Testing net (#0)
I0628 19:21:20.332100 32080 data_reader.cpp:262] Starting prefetch of epoch 16
I0628 19:21:20.355062 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.917199
I0628 19:21:20.355074 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:21:20.355080 32082 solver.cpp:630]     Test net output #2: loss = 0.299341 (* 1 = 0.299341 loss)
I0628 19:21:20.355098 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01901s
I0628 19:21:20.372404 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.34
I0628 19:21:20.779772 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:20.781250 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:20.781608 32082 solver.cpp:349] Iteration 16000 (31.7451 iter/s, 3.15009s/100 iter), loss = 0.00156999
I0628 19:21:20.781625 32082 solver.cpp:371]     Train net output #0: loss = 0.00156999 (* 1 = 0.00156999 loss)
I0628 19:21:20.781631 32082 sgd_solver.cpp:137] Iteration 16000, lr = 0.0075, m = 0.9
I0628 19:21:22.503127 32082 solver.cpp:349] Iteration 16100 (58.1065 iter/s, 1.72098s/100 iter), loss = 0.00238028
I0628 19:21:22.503151 32082 solver.cpp:371]     Train net output #0: loss = 0.00238028 (* 1 = 0.00238028 loss)
I0628 19:21:22.503155 32082 sgd_solver.cpp:137] Iteration 16100, lr = 0.00748438, m = 0.9
I0628 19:21:24.224289 32082 solver.cpp:349] Iteration 16200 (58.1187 iter/s, 1.72062s/100 iter), loss = 0.00130366
I0628 19:21:24.224334 32082 solver.cpp:371]     Train net output #0: loss = 0.00130366 (* 1 = 0.00130366 loss)
I0628 19:21:24.224339 32082 sgd_solver.cpp:137] Iteration 16200, lr = 0.00746875, m = 0.9
I0628 19:21:25.945211 32082 solver.cpp:349] Iteration 16300 (58.1276 iter/s, 1.72035s/100 iter), loss = 0.00132841
I0628 19:21:25.945233 32082 solver.cpp:371]     Train net output #0: loss = 0.00132841 (* 1 = 0.00132841 loss)
I0628 19:21:25.945238 32082 sgd_solver.cpp:137] Iteration 16300, lr = 0.00745312, m = 0.9
I0628 19:21:27.668085 32082 solver.cpp:349] Iteration 16400 (58.0608 iter/s, 1.72233s/100 iter), loss = 0.00167868
I0628 19:21:27.668157 32082 solver.cpp:371]     Train net output #0: loss = 0.00167867 (* 1 = 0.00167867 loss)
I0628 19:21:27.668165 32082 sgd_solver.cpp:137] Iteration 16400, lr = 0.0074375, m = 0.9
I0628 19:21:27.719925 32050 data_reader.cpp:262] Starting prefetch of epoch 21
I0628 19:21:29.387053 32082 solver.cpp:349] Iteration 16500 (58.1947 iter/s, 1.71837s/100 iter), loss = 0.00176654
I0628 19:21:29.387079 32082 solver.cpp:371]     Train net output #0: loss = 0.00176654 (* 1 = 0.00176654 loss)
I0628 19:21:29.387084 32082 sgd_solver.cpp:137] Iteration 16500, lr = 0.00742187, m = 0.9
I0628 19:21:31.105729 32082 solver.cpp:349] Iteration 16600 (58.2027 iter/s, 1.71813s/100 iter), loss = 0.000747273
I0628 19:21:31.105752 32082 solver.cpp:371]     Train net output #0: loss = 0.000747269 (* 1 = 0.000747269 loss)
I0628 19:21:31.105756 32082 sgd_solver.cpp:137] Iteration 16600, lr = 0.00740625, m = 0.9
I0628 19:21:32.826076 32082 solver.cpp:349] Iteration 16700 (58.1461 iter/s, 1.71981s/100 iter), loss = 0.00120975
I0628 19:21:32.826102 32082 solver.cpp:371]     Train net output #0: loss = 0.00120974 (* 1 = 0.00120974 loss)
I0628 19:21:32.826107 32082 sgd_solver.cpp:137] Iteration 16700, lr = 0.00739062, m = 0.9
I0628 19:21:34.549110 32082 solver.cpp:349] Iteration 16800 (58.0555 iter/s, 1.72249s/100 iter), loss = 0.00288433
I0628 19:21:34.549137 32082 solver.cpp:371]     Train net output #0: loss = 0.00288433 (* 1 = 0.00288433 loss)
I0628 19:21:34.549144 32082 sgd_solver.cpp:137] Iteration 16800, lr = 0.007375, m = 0.9
I0628 19:21:36.270468 32082 solver.cpp:349] Iteration 16900 (58.112 iter/s, 1.72081s/100 iter), loss = 0.00170128
I0628 19:21:36.270495 32082 solver.cpp:371]     Train net output #0: loss = 0.00170128 (* 1 = 0.00170128 loss)
I0628 19:21:36.270501 32082 sgd_solver.cpp:137] Iteration 16900, lr = 0.00735937, m = 0.9
I0628 19:21:37.969740 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:37.970813 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:37.970823 32082 net.cpp:2170] conv1a_param_0(0.17) 
I0628 19:21:37.970829 32082 net.cpp:2170] conv1b_param_0(0.34) 
I0628 19:21:37.970831 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:37.970834 32082 net.cpp:2170] res2a_branch2a_param_0(0.34) 
I0628 19:21:37.970837 32082 net.cpp:2170] res2a_branch2b_param_0(0.34) 
I0628 19:21:37.970839 32082 net.cpp:2170] res3a_branch2a_param_0(0.34) 
I0628 19:21:37.970841 32082 net.cpp:2170] res3a_branch2b_param_0(0.34) 
I0628 19:21:37.970844 32082 net.cpp:2170] res4a_branch2a_param_0(0.34) 
I0628 19:21:37.970845 32082 net.cpp:2170] res4a_branch2b_param_0(0.34) 
I0628 19:21:37.970847 32082 net.cpp:2170] res5a_branch2a_param_0(0.34) 
I0628 19:21:37.970849 32082 net.cpp:2170] res5a_branch2b_param_0(0.34) 
I0628 19:21:37.970851 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (800206/2.3599e+06) 0.339
I0628 19:21:37.970859 32082 solver.cpp:545] Iteration 17000, Testing net (#0)
I0628 19:21:38.971030 32080 data_reader.cpp:262] Starting prefetch of epoch 17
I0628 19:21:38.991153 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9188
I0628 19:21:38.991165 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:21:38.991169 32082 solver.cpp:630]     Test net output #2: loss = 0.298422 (* 1 = 0.298422 loss)
I0628 19:21:38.991183 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02003s
I0628 19:21:39.008482 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.36
I0628 19:21:39.438681 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:39.440184 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:39.440562 32082 solver.cpp:349] Iteration 17000 (31.5543 iter/s, 3.16914s/100 iter), loss = 0.00145359
I0628 19:21:39.440585 32082 solver.cpp:371]     Train net output #0: loss = 0.00145359 (* 1 = 0.00145359 loss)
I0628 19:21:39.440603 32082 sgd_solver.cpp:137] Iteration 17000, lr = 0.00734375, m = 0.9
I0628 19:21:41.161120 32082 solver.cpp:349] Iteration 17100 (58.1392 iter/s, 1.72001s/100 iter), loss = 0.00232401
I0628 19:21:41.161159 32082 solver.cpp:371]     Train net output #0: loss = 0.00232401 (* 1 = 0.00232401 loss)
I0628 19:21:41.161166 32082 sgd_solver.cpp:137] Iteration 17100, lr = 0.00732813, m = 0.9
I0628 19:21:42.605798 32050 data_reader.cpp:262] Starting prefetch of epoch 22
I0628 19:21:42.880795 32082 solver.cpp:349] Iteration 17200 (58.1694 iter/s, 1.71912s/100 iter), loss = 0.00116345
I0628 19:21:42.880821 32082 solver.cpp:371]     Train net output #0: loss = 0.00116345 (* 1 = 0.00116345 loss)
I0628 19:21:42.880828 32082 sgd_solver.cpp:137] Iteration 17200, lr = 0.0073125, m = 0.9
I0628 19:21:44.607269 32082 solver.cpp:349] Iteration 17300 (57.9397 iter/s, 1.72593s/100 iter), loss = 0.00166173
I0628 19:21:44.607291 32082 solver.cpp:371]     Train net output #0: loss = 0.00166173 (* 1 = 0.00166173 loss)
I0628 19:21:44.607295 32082 sgd_solver.cpp:137] Iteration 17300, lr = 0.00729688, m = 0.9
I0628 19:21:46.327725 32082 solver.cpp:349] Iteration 17400 (58.142 iter/s, 1.71993s/100 iter), loss = 0.00112594
I0628 19:21:46.327747 32082 solver.cpp:371]     Train net output #0: loss = 0.00112594 (* 1 = 0.00112594 loss)
I0628 19:21:46.327751 32082 sgd_solver.cpp:137] Iteration 17400, lr = 0.00728125, m = 0.9
I0628 19:21:48.047842 32082 solver.cpp:349] Iteration 17500 (58.1534 iter/s, 1.71959s/100 iter), loss = 0.00133067
I0628 19:21:48.047868 32082 solver.cpp:371]     Train net output #0: loss = 0.00133066 (* 1 = 0.00133066 loss)
I0628 19:21:48.047873 32082 sgd_solver.cpp:137] Iteration 17500, lr = 0.00726563, m = 0.9
I0628 19:21:49.770807 32082 solver.cpp:349] Iteration 17600 (58.0575 iter/s, 1.72243s/100 iter), loss = 0.00323801
I0628 19:21:49.770833 32082 solver.cpp:371]     Train net output #0: loss = 0.003238 (* 1 = 0.003238 loss)
I0628 19:21:49.770839 32082 sgd_solver.cpp:137] Iteration 17600, lr = 0.00725, m = 0.9
I0628 19:21:51.490188 32082 solver.cpp:349] Iteration 17700 (58.1785 iter/s, 1.71885s/100 iter), loss = 0.00175501
I0628 19:21:51.490214 32082 solver.cpp:371]     Train net output #0: loss = 0.00175501 (* 1 = 0.00175501 loss)
I0628 19:21:51.490221 32082 sgd_solver.cpp:137] Iteration 17700, lr = 0.00723437, m = 0.9
I0628 19:21:53.210973 32082 solver.cpp:349] Iteration 17800 (58.131 iter/s, 1.72025s/100 iter), loss = 0.00158824
I0628 19:21:53.210994 32082 solver.cpp:371]     Train net output #0: loss = 0.00158824 (* 1 = 0.00158824 loss)
I0628 19:21:53.210999 32082 sgd_solver.cpp:137] Iteration 17800, lr = 0.00721875, m = 0.9
I0628 19:21:54.931502 32082 solver.cpp:349] Iteration 17900 (58.1393 iter/s, 1.72001s/100 iter), loss = 0.00123675
I0628 19:21:54.931529 32082 solver.cpp:371]     Train net output #0: loss = 0.00123674 (* 1 = 0.00123674 loss)
I0628 19:21:54.931535 32082 sgd_solver.cpp:137] Iteration 17900, lr = 0.00720312, m = 0.9
I0628 19:21:56.050997 32050 data_reader.cpp:262] Starting prefetch of epoch 23
I0628 19:21:56.634529 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:56.635679 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:56.635687 32082 net.cpp:2170] conv1a_param_0(0.18) 
I0628 19:21:56.635694 32082 net.cpp:2170] conv1b_param_0(0.36) 
I0628 19:21:56.635699 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:56.635700 32082 net.cpp:2170] res2a_branch2a_param_0(0.36) 
I0628 19:21:56.635702 32082 net.cpp:2170] res2a_branch2b_param_0(0.36) 
I0628 19:21:56.635705 32082 net.cpp:2170] res3a_branch2a_param_0(0.36) 
I0628 19:21:56.635706 32082 net.cpp:2170] res3a_branch2b_param_0(0.36) 
I0628 19:21:56.635709 32082 net.cpp:2170] res4a_branch2a_param_0(0.36) 
I0628 19:21:56.635710 32082 net.cpp:2170] res4a_branch2b_param_0(0.36) 
I0628 19:21:56.635713 32082 net.cpp:2170] res5a_branch2a_param_0(0.36) 
I0628 19:21:56.635715 32082 net.cpp:2170] res5a_branch2b_param_0(0.36) 
I0628 19:21:56.635717 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (847279/2.3599e+06) 0.359
I0628 19:21:56.635725 32082 solver.cpp:545] Iteration 18000, Testing net (#0)
I0628 19:21:57.638653 32080 data_reader.cpp:262] Starting prefetch of epoch 18
I0628 19:21:57.659021 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9188
I0628 19:21:57.659054 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:21:57.659059 32082 solver.cpp:630]     Test net output #2: loss = 0.297917 (* 1 = 0.297917 loss)
I0628 19:21:57.659072 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02306s
I0628 19:21:57.676357 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.38
I0628 19:21:58.129647 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:58.131119 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:58.131477 32082 solver.cpp:349] Iteration 18000 (31.2594 iter/s, 3.19903s/100 iter), loss = 0.00127674
I0628 19:21:58.131496 32082 solver.cpp:371]     Train net output #0: loss = 0.00127674 (* 1 = 0.00127674 loss)
I0628 19:21:58.131501 32082 sgd_solver.cpp:137] Iteration 18000, lr = 0.0071875, m = 0.9
I0628 19:21:59.854558 32082 solver.cpp:349] Iteration 18100 (58.0531 iter/s, 1.72256s/100 iter), loss = 0.00228488
I0628 19:21:59.854584 32082 solver.cpp:371]     Train net output #0: loss = 0.00228487 (* 1 = 0.00228487 loss)
I0628 19:21:59.854590 32082 sgd_solver.cpp:137] Iteration 18100, lr = 0.00717187, m = 0.9
I0628 19:22:01.572015 32082 solver.cpp:349] Iteration 18200 (58.2435 iter/s, 1.71693s/100 iter), loss = 0.000693781
I0628 19:22:01.572041 32082 solver.cpp:371]     Train net output #0: loss = 0.000693776 (* 1 = 0.000693776 loss)
I0628 19:22:01.572046 32082 sgd_solver.cpp:137] Iteration 18200, lr = 0.00715625, m = 0.9
I0628 19:22:03.292608 32082 solver.cpp:349] Iteration 18300 (58.1373 iter/s, 1.72007s/100 iter), loss = 0.00129666
I0628 19:22:03.292635 32082 solver.cpp:371]     Train net output #0: loss = 0.00129666 (* 1 = 0.00129666 loss)
I0628 19:22:03.292641 32082 sgd_solver.cpp:137] Iteration 18300, lr = 0.00714062, m = 0.9
I0628 19:22:05.012122 32082 solver.cpp:349] Iteration 18400 (58.1737 iter/s, 1.71899s/100 iter), loss = 0.00102875
I0628 19:22:05.012148 32082 solver.cpp:371]     Train net output #0: loss = 0.00102875 (* 1 = 0.00102875 loss)
I0628 19:22:05.012154 32082 sgd_solver.cpp:137] Iteration 18400, lr = 0.007125, m = 0.9
I0628 19:22:06.732118 32082 solver.cpp:349] Iteration 18500 (58.1573 iter/s, 1.71947s/100 iter), loss = 0.00305622
I0628 19:22:06.732141 32082 solver.cpp:371]     Train net output #0: loss = 0.00305621 (* 1 = 0.00305621 loss)
I0628 19:22:06.732146 32082 sgd_solver.cpp:137] Iteration 18500, lr = 0.00710937, m = 0.9
I0628 19:22:08.455132 32082 solver.cpp:349] Iteration 18600 (58.0552 iter/s, 1.7225s/100 iter), loss = 0.000981332
I0628 19:22:08.455155 32082 solver.cpp:371]     Train net output #0: loss = 0.000981327 (* 1 = 0.000981327 loss)
I0628 19:22:08.455158 32082 sgd_solver.cpp:137] Iteration 18600, lr = 0.00709375, m = 0.9
I0628 19:22:10.175518 32082 solver.cpp:349] Iteration 18700 (58.1439 iter/s, 1.71987s/100 iter), loss = 0.00153814
I0628 19:22:10.175544 32082 solver.cpp:371]     Train net output #0: loss = 0.00153814 (* 1 = 0.00153814 loss)
I0628 19:22:10.175550 32082 sgd_solver.cpp:137] Iteration 18700, lr = 0.00707812, m = 0.9
I0628 19:22:10.967597 32050 data_reader.cpp:262] Starting prefetch of epoch 24
I0628 19:22:11.894840 32082 solver.cpp:349] Iteration 18800 (58.18 iter/s, 1.7188s/100 iter), loss = 0.00173058
I0628 19:22:11.894861 32082 solver.cpp:371]     Train net output #0: loss = 0.00173057 (* 1 = 0.00173057 loss)
I0628 19:22:11.894865 32082 sgd_solver.cpp:137] Iteration 18800, lr = 0.0070625, m = 0.9
I0628 19:22:13.615367 32082 solver.cpp:349] Iteration 18900 (58.1389 iter/s, 1.72002s/100 iter), loss = 0.00184476
I0628 19:22:13.615391 32082 solver.cpp:371]     Train net output #0: loss = 0.00184476 (* 1 = 0.00184476 loss)
I0628 19:22:13.615394 32082 sgd_solver.cpp:137] Iteration 18900, lr = 0.00704687, m = 0.9
I0628 19:22:15.321308 32082 solver.cpp:401] Sparsity after update:
I0628 19:22:15.322412 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:22:15.322419 32082 net.cpp:2170] conv1a_param_0(0.19) 
I0628 19:22:15.322425 32082 net.cpp:2170] conv1b_param_0(0.38) 
I0628 19:22:15.322428 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:22:15.322430 32082 net.cpp:2170] res2a_branch2a_param_0(0.38) 
I0628 19:22:15.322432 32082 net.cpp:2170] res2a_branch2b_param_0(0.38) 
I0628 19:22:15.322433 32082 net.cpp:2170] res3a_branch2a_param_0(0.38) 
I0628 19:22:15.322435 32082 net.cpp:2170] res3a_branch2b_param_0(0.38) 
I0628 19:22:15.322438 32082 net.cpp:2170] res4a_branch2a_param_0(0.38) 
I0628 19:22:15.322449 32082 net.cpp:2170] res4a_branch2b_param_0(0.38) 
I0628 19:22:15.322451 32082 net.cpp:2170] res5a_branch2a_param_0(0.38) 
I0628 19:22:15.322453 32082 net.cpp:2170] res5a_branch2b_param_0(0.38) 
I0628 19:22:15.322455 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (894353/2.3599e+06) 0.379
I0628 19:22:15.322463 32082 solver.cpp:545] Iteration 19000, Testing net (#0)
I0628 19:22:16.325639 32080 data_reader.cpp:262] Starting prefetch of epoch 19
I0628 19:22:16.345877 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:22:16.345890 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:22:16.345894 32082 solver.cpp:630]     Test net output #2: loss = 0.297331 (* 1 = 0.297331 loss)
I0628 19:22:16.345908 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02316s
I0628 19:22:16.363210 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.4
I0628 19:22:16.836954 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:22:16.838429 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:22:16.838789 32082 solver.cpp:349] Iteration 19000 (31.0318 iter/s, 3.2225s/100 iter), loss = 0.00127647
I0628 19:22:16.838809 32082 solver.cpp:371]     Train net output #0: loss = 0.00127646 (* 1 = 0.00127646 loss)
I0628 19:22:16.838817 32082 sgd_solver.cpp:137] Iteration 19000, lr = 0.00703125, m = 0.9
I0628 19:22:18.559056 32082 solver.cpp:349] Iteration 19100 (58.1479 iter/s, 1.71975s/100 iter), loss = 0.00159837
I0628 19:22:18.559077 32082 solver.cpp:371]     Train net output #0: loss = 0.00159836 (* 1 = 0.00159836 loss)
I0628 19:22:18.559083 32082 sgd_solver.cpp:137] Iteration 19100, lr = 0.00701563, m = 0.9
I0628 19:22:20.285709 32082 solver.cpp:349] Iteration 19200 (57.9327 iter/s, 1.72614s/100 iter), loss = 0.000955108
I0628 19:22:20.285732 32082 solver.cpp:371]     Train net output #0: loss = 0.000955104 (* 1 = 0.000955104 loss)
I0628 19:22:20.285737 32082 sgd_solver.cpp:137] Iteration 19200, lr = 0.007, m = 0.9
I0628 19:22:22.006700 32082 solver.cpp:349] Iteration 19300 (58.1232 iter/s, 1.72048s/100 iter), loss = 0.0011965
I0628 19:22:22.006722 32082 solver.cpp:371]     Train net output #0: loss = 0.00119649 (* 1 = 0.00119649 loss)
I0628 19:22:22.006726 32082 sgd_solver.cpp:137] Iteration 19300, lr = 0.00698437, m = 0.9
I0628 19:22:23.729660 32082 solver.cpp:349] Iteration 19400 (58.0567 iter/s, 1.72245s/100 iter), loss = 0.0013953
I0628 19:22:23.729686 32082 solver.cpp:371]     Train net output #0: loss = 0.0013953 (* 1 = 0.0013953 loss)
I0628 19:22:23.729691 32082 sgd_solver.cpp:137] Iteration 19400, lr = 0.00696875, m = 0.9
I0628 19:22:25.451745 32082 solver.cpp:349] Iteration 19500 (58.0864 iter/s, 1.72157s/100 iter), loss = 0.00127724
I0628 19:22:25.451771 32082 solver.cpp:371]     Train net output #0: loss = 0.00127723 (* 1 = 0.00127723 loss)
I0628 19:22:25.451776 32082 sgd_solver.cpp:137] Iteration 19500, lr = 0.00695312, m = 0.9
I0628 19:22:25.933691 32050 data_reader.cpp:262] Starting prefetch of epoch 25
I0628 19:22:27.172125 32082 solver.cpp:349] Iteration 19600 (58.1439 iter/s, 1.71987s/100 iter), loss = 0.00126146
I0628 19:22:27.172150 32082 solver.cpp:371]     Train net output #0: loss = 0.00126146 (* 1 = 0.00126146 loss)
I0628 19:22:27.172155 32082 sgd_solver.cpp:137] Iteration 19600, lr = 0.0069375, m = 0.9
I0628 19:22:28.893153 32082 solver.cpp:349] Iteration 19700 (58.122 iter/s, 1.72052s/100 iter), loss = 0.00206663
I0628 19:22:28.893230 32082 solver.cpp:371]     Train net output #0: loss = 0.00206662 (* 1 = 0.00206662 loss)
I0628 19:22:28.893235 32082 sgd_solver.cpp:137] Iteration 19700, lr = 0.00692187, m = 0.9
I0628 19:22:30.612263 32082 solver.cpp:349] Iteration 19800 (58.1885 iter/s, 1.71855s/100 iter), loss = 0.00336939
I0628 19:22:30.612289 32082 solver.cpp:371]     Train net output #0: loss = 0.00336938 (* 1 = 0.00336938 loss)
I0628 19:22:30.612296 32082 sgd_solver.cpp:137] Iteration 19800, lr = 0.00690625, m = 0.9
I0628 19:22:32.335132 32082 solver.cpp:349] Iteration 19900 (58.0599 iter/s, 1.72236s/100 iter), loss = 0.00151099
I0628 19:22:32.335155 32082 solver.cpp:371]     Train net output #0: loss = 0.00151099 (* 1 = 0.00151099 loss)
I0628 19:22:32.335160 32082 sgd_solver.cpp:137] Iteration 19900, lr = 0.00689062, m = 0.9
I0628 19:22:34.043210 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0628 19:22:34.051419 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0628 19:22:34.054909 32082 solver.cpp:401] Sparsity after update:
I0628 19:22:34.055934 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:22:34.055941 32082 net.cpp:2170] conv1a_param_0(0.2) 
I0628 19:22:34.055948 32082 net.cpp:2170] conv1b_param_0(0.4) 
I0628 19:22:34.055950 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:22:34.055953 32082 net.cpp:2170] res2a_branch2a_param_0(0.4) 
I0628 19:22:34.055955 32082 net.cpp:2170] res2a_branch2b_param_0(0.4) 
I0628 19:22:34.055958 32082 net.cpp:2170] res3a_branch2a_param_0(0.4) 
I0628 19:22:34.055959 32082 net.cpp:2170] res3a_branch2b_param_0(0.4) 
I0628 19:22:34.055961 32082 net.cpp:2170] res4a_branch2a_param_0(0.4) 
I0628 19:22:34.055963 32082 net.cpp:2170] res4a_branch2b_param_0(0.4) 
I0628 19:22:34.055965 32082 net.cpp:2170] res5a_branch2a_param_0(0.4) 
I0628 19:22:34.055968 32082 net.cpp:2170] res5a_branch2b_param_0(0.4) 
I0628 19:22:34.055970 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (941426/2.3599e+06) 0.399
I0628 19:22:34.055979 32082 solver.cpp:545] Iteration 20000, Testing net (#0)
I0628 19:22:35.055912 32080 data_reader.cpp:262] Starting prefetch of epoch 20
I0628 19:22:35.076084 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:22:35.076095 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:22:35.076099 32082 solver.cpp:630]     Test net output #2: loss = 0.298101 (* 1 = 0.298101 loss)
I0628 19:22:35.076113 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01986s
I0628 19:22:35.093415 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.42
I0628 19:22:35.579188 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:22:35.580651 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:22:35.581009 32082 solver.cpp:349] Iteration 20000 (30.8169 iter/s, 3.24497s/100 iter), loss = 0.00138409
I0628 19:22:35.581028 32082 solver.cpp:371]     Train net output #0: loss = 0.00138408 (* 1 = 0.00138408 loss)
I0628 19:22:35.581033 32082 sgd_solver.cpp:137] Iteration 20000, lr = 0.006875, m = 0.9
I0628 19:22:37.302196 32082 solver.cpp:349] Iteration 20100 (58.1162 iter/s, 1.72069s/100 iter), loss = 0.00123437
I0628 19:22:37.302222 32082 solver.cpp:371]     Train net output #0: loss = 0.00123437 (* 1 = 0.00123437 loss)
I0628 19:22:37.302228 32082 sgd_solver.cpp:137] Iteration 20100, lr = 0.00685938, m = 0.9
I0628 19:22:39.025413 32082 solver.cpp:349] Iteration 20200 (58.0481 iter/s, 1.72271s/100 iter), loss = 0.00327714
I0628 19:22:39.025435 32082 solver.cpp:371]     Train net output #0: loss = 0.00327714 (* 1 = 0.00327714 loss)
I0628 19:22:39.025441 32082 sgd_solver.cpp:137] Iteration 20200, lr = 0.00684375, m = 0.9
I0628 19:22:40.745877 32082 solver.cpp:349] Iteration 20300 (58.1408 iter/s, 1.71996s/100 iter), loss = 0.00313952
I0628 19:22:40.745916 32082 solver.cpp:371]     Train net output #0: loss = 0.00313952 (* 1 = 0.00313952 loss)
I0628 19:22:40.745923 32082 sgd_solver.cpp:137] Iteration 20300, lr = 0.00682813, m = 0.9
I0628 19:22:40.900382 32050 data_reader.cpp:262] Starting prefetch of epoch 26
I0628 19:22:42.471127 32082 solver.cpp:349] Iteration 20400 (57.98 iter/s, 1.72473s/100 iter), loss = 0.00106967
I0628 19:22:42.471151 32082 solver.cpp:371]     Train net output #0: loss = 0.00106966 (* 1 = 0.00106966 loss)
I0628 19:22:42.471155 32082 sgd_solver.cpp:137] Iteration 20400, lr = 0.0068125, m = 0.9
I0628 19:22:44.193503 32082 solver.cpp:349] Iteration 20500 (58.0761 iter/s, 1.72188s/100 iter), loss = 0.00188611
I0628 19:22:44.193531 32082 solver.cpp:371]     Train net output #0: loss = 0.00188611 (* 1 = 0.00188611 loss)
I0628 19:22:44.193536 32082 sgd_solver.cpp:137] Iteration 20500, lr = 0.00679688, m = 0.9
I0628 19:22:45.910614 32082 solver.cpp:349] Iteration 20600 (58.2544 iter/s, 1.71661s/100 iter), loss = 0.00155148
I0628 19:22:45.910640 32082 solver.cpp:371]     Train net output #0: loss = 0.00155148 (* 1 = 0.00155148 loss)
I0628 19:22:45.910645 32082 sgd_solver.cpp:137] Iteration 20600, lr = 0.00678125, m = 0.9
I0628 19:22:47.631467 32082 solver.cpp:349] Iteration 20700 (58.1276 iter/s, 1.72035s/100 iter), loss = 0.00201201
I0628 19:22:47.631494 32082 solver.cpp:371]     Train net output #0: loss = 0.00201201 (* 1 = 0.00201201 loss)
I0628 19:22:47.631500 32082 sgd_solver.cpp:137] Iteration 20700, lr = 0.00676562, m = 0.9
I0628 19:22:49.353922 32082 solver.cpp:349] Iteration 20800 (58.0736 iter/s, 1.72195s/100 iter), loss = 0.00121136
I0628 19:22:49.353948 32082 solver.cpp:371]     Train net output #0: loss = 0.00121136 (* 1 = 0.00121136 loss)
I0628 19:22:49.353955 32082 sgd_solver.cpp:137] Iteration 20800, lr = 0.00675, m = 0.9
I0628 19:22:51.075182 32082 solver.cpp:349] Iteration 20900 (58.1138 iter/s, 1.72076s/100 iter), loss = 0.00136441
I0628 19:22:51.075206 32082 solver.cpp:371]     Train net output #0: loss = 0.00136441 (* 1 = 0.00136441 loss)
I0628 19:22:51.075212 32082 sgd_solver.cpp:137] Iteration 20900, lr = 0.00673437, m = 0.9
I0628 19:22:52.779145 32082 solver.cpp:401] Sparsity after update:
I0628 19:22:52.780202 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:22:52.780210 32082 net.cpp:2170] conv1a_param_0(0.21) 
I0628 19:22:52.780216 32082 net.cpp:2170] conv1b_param_0(0.42) 
I0628 19:22:52.780218 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:22:52.780220 32082 net.cpp:2170] res2a_branch2a_param_0(0.42) 
I0628 19:22:52.780223 32082 net.cpp:2170] res2a_branch2b_param_0(0.42) 
I0628 19:22:52.780225 32082 net.cpp:2170] res3a_branch2a_param_0(0.42) 
I0628 19:22:52.780227 32082 net.cpp:2170] res3a_branch2b_param_0(0.42) 
I0628 19:22:52.780230 32082 net.cpp:2170] res4a_branch2a_param_0(0.42) 
I0628 19:22:52.780231 32082 net.cpp:2170] res4a_branch2b_param_0(0.42) 
I0628 19:22:52.780233 32082 net.cpp:2170] res5a_branch2a_param_0(0.42) 
I0628 19:22:52.780236 32082 net.cpp:2170] res5a_branch2b_param_0(0.42) 
I0628 19:22:52.780238 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (988492/2.3599e+06) 0.419
I0628 19:22:52.780246 32082 solver.cpp:545] Iteration 21000, Testing net (#0)
I0628 19:22:53.780596 32080 data_reader.cpp:262] Starting prefetch of epoch 21
I0628 19:22:53.801076 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9184
I0628 19:22:53.801090 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:22:53.801095 32082 solver.cpp:630]     Test net output #2: loss = 0.299408 (* 1 = 0.299408 loss)
I0628 19:22:53.801108 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02059s
I0628 19:22:53.818778 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.44
I0628 19:22:54.331900 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:22:54.333371 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:22:54.333734 32082 solver.cpp:349] Iteration 21000 (30.6969 iter/s, 3.25766s/100 iter), loss = 0.00173958
I0628 19:22:54.333762 32082 solver.cpp:371]     Train net output #0: loss = 0.00173958 (* 1 = 0.00173958 loss)
I0628 19:22:54.333767 32082 sgd_solver.cpp:137] Iteration 21000, lr = 0.00671875, m = 0.9
I0628 19:22:55.884397 32050 data_reader.cpp:262] Starting prefetch of epoch 27
I0628 19:22:56.056288 32082 solver.cpp:349] Iteration 21100 (58.07 iter/s, 1.72206s/100 iter), loss = 0.00127419
I0628 19:22:56.056311 32082 solver.cpp:371]     Train net output #0: loss = 0.00127419 (* 1 = 0.00127419 loss)
I0628 19:22:56.056315 32082 sgd_solver.cpp:137] Iteration 21100, lr = 0.00670313, m = 0.9
I0628 19:22:57.776768 32082 solver.cpp:349] Iteration 21200 (58.1398 iter/s, 1.71999s/100 iter), loss = 0.00314684
I0628 19:22:57.776789 32082 solver.cpp:371]     Train net output #0: loss = 0.00314684 (* 1 = 0.00314684 loss)
I0628 19:22:57.776794 32082 sgd_solver.cpp:137] Iteration 21200, lr = 0.0066875, m = 0.9
I0628 19:22:59.493788 32082 solver.cpp:349] Iteration 21300 (58.2569 iter/s, 1.71653s/100 iter), loss = 0.0014739
I0628 19:22:59.493862 32082 solver.cpp:371]     Train net output #0: loss = 0.0014739 (* 1 = 0.0014739 loss)
I0628 19:22:59.493868 32082 sgd_solver.cpp:137] Iteration 21300, lr = 0.00667187, m = 0.9
I0628 19:23:01.216753 32082 solver.cpp:349] Iteration 21400 (58.0577 iter/s, 1.72242s/100 iter), loss = 0.00122436
I0628 19:23:01.216778 32082 solver.cpp:371]     Train net output #0: loss = 0.00122436 (* 1 = 0.00122436 loss)
I0628 19:23:01.216784 32082 sgd_solver.cpp:137] Iteration 21400, lr = 0.00665625, m = 0.9
I0628 19:23:02.935565 32082 solver.cpp:349] Iteration 21500 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00208552
I0628 19:23:02.935591 32082 solver.cpp:371]     Train net output #0: loss = 0.00208552 (* 1 = 0.00208552 loss)
I0628 19:23:02.935596 32082 sgd_solver.cpp:137] Iteration 21500, lr = 0.00664062, m = 0.9
I0628 19:23:04.653000 32082 solver.cpp:349] Iteration 21600 (58.243 iter/s, 1.71694s/100 iter), loss = 0.00211512
I0628 19:23:04.653028 32082 solver.cpp:371]     Train net output #0: loss = 0.00211512 (* 1 = 0.00211512 loss)
I0628 19:23:04.653033 32082 sgd_solver.cpp:137] Iteration 21600, lr = 0.006625, m = 0.9
I0628 19:23:06.371117 32082 solver.cpp:349] Iteration 21700 (58.2199 iter/s, 1.71763s/100 iter), loss = 0.000734824
I0628 19:23:06.371141 32082 solver.cpp:371]     Train net output #0: loss = 0.00073482 (* 1 = 0.00073482 loss)
I0628 19:23:06.371147 32082 sgd_solver.cpp:137] Iteration 21700, lr = 0.00660937, m = 0.9
I0628 19:23:08.089682 32082 solver.cpp:349] Iteration 21800 (58.2046 iter/s, 1.71808s/100 iter), loss = 0.00353842
I0628 19:23:08.089709 32082 solver.cpp:371]     Train net output #0: loss = 0.00353842 (* 1 = 0.00353842 loss)
I0628 19:23:08.089715 32082 sgd_solver.cpp:137] Iteration 21800, lr = 0.00659375, m = 0.9
I0628 19:23:09.310729 32050 data_reader.cpp:262] Starting prefetch of epoch 28
I0628 19:23:09.813076 32082 solver.cpp:349] Iteration 21900 (58.0415 iter/s, 1.7229s/100 iter), loss = 0.00208937
I0628 19:23:09.813098 32082 solver.cpp:371]     Train net output #0: loss = 0.00208937 (* 1 = 0.00208937 loss)
I0628 19:23:09.813102 32082 sgd_solver.cpp:137] Iteration 21900, lr = 0.00657812, m = 0.9
I0628 19:23:11.513779 32082 solver.cpp:401] Sparsity after update:
I0628 19:23:11.514871 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:23:11.514879 32082 net.cpp:2170] conv1a_param_0(0.22) 
I0628 19:23:11.514885 32082 net.cpp:2170] conv1b_param_0(0.44) 
I0628 19:23:11.514888 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:23:11.514890 32082 net.cpp:2170] res2a_branch2a_param_0(0.44) 
I0628 19:23:11.514892 32082 net.cpp:2170] res2a_branch2b_param_0(0.44) 
I0628 19:23:11.514894 32082 net.cpp:2170] res3a_branch2a_param_0(0.44) 
I0628 19:23:11.514896 32082 net.cpp:2170] res3a_branch2b_param_0(0.44) 
I0628 19:23:11.514899 32082 net.cpp:2170] res4a_branch2a_param_0(0.44) 
I0628 19:23:11.514900 32082 net.cpp:2170] res4a_branch2b_param_0(0.44) 
I0628 19:23:11.514902 32082 net.cpp:2170] res5a_branch2a_param_0(0.44) 
I0628 19:23:11.514904 32082 net.cpp:2170] res5a_branch2b_param_0(0.44) 
I0628 19:23:11.514906 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.03556e+06/2.3599e+06) 0.439
I0628 19:23:11.514914 32082 solver.cpp:545] Iteration 22000, Testing net (#0)
I0628 19:23:12.512642 32080 data_reader.cpp:262] Starting prefetch of epoch 22
I0628 19:23:12.535820 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9174
I0628 19:23:12.535833 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:23:12.535840 32082 solver.cpp:630]     Test net output #2: loss = 0.301279 (* 1 = 0.301279 loss)
I0628 19:23:12.535858 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02068s
I0628 19:23:12.553115 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.46
I0628 19:23:13.083266 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:23:13.084722 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:23:13.085078 32082 solver.cpp:349] Iteration 22000 (30.5705 iter/s, 3.27113s/100 iter), loss = 0.00241128
I0628 19:23:13.085103 32082 solver.cpp:371]     Train net output #0: loss = 0.00241128 (* 1 = 0.00241128 loss)
I0628 19:23:13.085108 32082 sgd_solver.cpp:137] Iteration 22000, lr = 0.0065625, m = 0.9
I0628 19:23:14.805814 32082 solver.cpp:349] Iteration 22100 (58.1309 iter/s, 1.72026s/100 iter), loss = 0.00162286
I0628 19:23:14.805835 32082 solver.cpp:371]     Train net output #0: loss = 0.00162286 (* 1 = 0.00162286 loss)
I0628 19:23:14.805840 32082 sgd_solver.cpp:137] Iteration 22100, lr = 0.00654687, m = 0.9
I0628 19:23:16.526540 32082 solver.cpp:349] Iteration 22200 (58.1311 iter/s, 1.72025s/100 iter), loss = 0.00185903
I0628 19:23:16.526561 32082 solver.cpp:371]     Train net output #0: loss = 0.00185903 (* 1 = 0.00185903 loss)
I0628 19:23:16.526564 32082 sgd_solver.cpp:137] Iteration 22200, lr = 0.00653125, m = 0.9
I0628 19:23:18.252411 32082 solver.cpp:349] Iteration 22300 (57.9578 iter/s, 1.72539s/100 iter), loss = 0.00149192
I0628 19:23:18.252434 32082 solver.cpp:371]     Train net output #0: loss = 0.00149192 (* 1 = 0.00149192 loss)
I0628 19:23:18.252439 32082 sgd_solver.cpp:137] Iteration 22300, lr = 0.00651562, m = 0.9
I0628 19:23:19.969911 32082 solver.cpp:349] Iteration 22400 (58.2403 iter/s, 1.71702s/100 iter), loss = 0.0029787
I0628 19:23:19.969934 32082 solver.cpp:371]     Train net output #0: loss = 0.00297869 (* 1 = 0.00297869 loss)
I0628 19:23:19.969940 32082 sgd_solver.cpp:137] Iteration 22400, lr = 0.0065, m = 0.9
I0628 19:23:21.694207 32082 solver.cpp:349] Iteration 22500 (58.0109 iter/s, 1.72381s/100 iter), loss = 0.00134489
I0628 19:23:21.694232 32082 solver.cpp:371]     Train net output #0: loss = 0.00134488 (* 1 = 0.00134488 loss)
I0628 19:23:21.694238 32082 sgd_solver.cpp:137] Iteration 22500, lr = 0.00648437, m = 0.9
I0628 19:23:23.417632 32082 solver.cpp:349] Iteration 22600 (58.0403 iter/s, 1.72294s/100 iter), loss = 0.00138944
I0628 19:23:23.417659 32082 solver.cpp:371]     Train net output #0: loss = 0.00138943 (* 1 = 0.00138943 loss)
I0628 19:23:23.417665 32082 sgd_solver.cpp:137] Iteration 22600, lr = 0.00646875, m = 0.9
I0628 19:23:24.327414 32050 data_reader.cpp:262] Starting prefetch of epoch 29
I0628 19:23:25.139910 32082 solver.cpp:349] Iteration 22700 (58.0789 iter/s, 1.72179s/100 iter), loss = 0.00114649
I0628 19:23:25.139935 32082 solver.cpp:371]     Train net output #0: loss = 0.00114648 (* 1 = 0.00114648 loss)
I0628 19:23:25.139941 32082 sgd_solver.cpp:137] Iteration 22700, lr = 0.00645312, m = 0.9
I0628 19:23:26.862645 32082 solver.cpp:349] Iteration 22800 (58.0635 iter/s, 1.72225s/100 iter), loss = 0.00198938
I0628 19:23:26.862673 32082 solver.cpp:371]     Train net output #0: loss = 0.00198938 (* 1 = 0.00198938 loss)
I0628 19:23:26.862679 32082 sgd_solver.cpp:137] Iteration 22800, lr = 0.0064375, m = 0.9
I0628 19:23:28.581779 32082 solver.cpp:349] Iteration 22900 (58.1851 iter/s, 1.71865s/100 iter), loss = 0.00340618
I0628 19:23:28.581802 32082 solver.cpp:371]     Train net output #0: loss = 0.00340617 (* 1 = 0.00340617 loss)
I0628 19:23:28.581807 32082 sgd_solver.cpp:137] Iteration 22900, lr = 0.00642187, m = 0.9
I0628 19:23:30.285939 32082 solver.cpp:401] Sparsity after update:
I0628 19:23:30.287101 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:23:30.287108 32082 net.cpp:2170] conv1a_param_0(0.23) 
I0628 19:23:30.287118 32082 net.cpp:2170] conv1b_param_0(0.46) 
I0628 19:23:30.287123 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:23:30.287127 32082 net.cpp:2170] res2a_branch2a_param_0(0.46) 
I0628 19:23:30.287132 32082 net.cpp:2170] res2a_branch2b_param_0(0.46) 
I0628 19:23:30.287134 32082 net.cpp:2170] res3a_branch2a_param_0(0.46) 
I0628 19:23:30.287138 32082 net.cpp:2170] res3a_branch2b_param_0(0.46) 
I0628 19:23:30.287143 32082 net.cpp:2170] res4a_branch2a_param_0(0.46) 
I0628 19:23:30.287147 32082 net.cpp:2170] res4a_branch2b_param_0(0.46) 
I0628 19:23:30.287150 32082 net.cpp:2170] res5a_branch2a_param_0(0.46) 
I0628 19:23:30.287154 32082 net.cpp:2170] res5a_branch2b_param_0(0.46) 
I0628 19:23:30.287158 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.3599e+06) 0.459
I0628 19:23:30.287168 32082 solver.cpp:545] Iteration 23000, Testing net (#0)
I0628 19:23:31.284610 32080 data_reader.cpp:262] Starting prefetch of epoch 23
I0628 19:23:31.307639 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9186
I0628 19:23:31.307652 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:23:31.307659 32082 solver.cpp:630]     Test net output #2: loss = 0.302122 (* 1 = 0.302122 loss)
I0628 19:23:31.307675 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02015s
I0628 19:23:31.324957 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.48
I0628 19:23:31.873148 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:23:31.874608 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:23:31.874966 32082 solver.cpp:349] Iteration 23000 (30.3764 iter/s, 3.29203s/100 iter), loss = 0.00149413
I0628 19:23:31.874985 32082 solver.cpp:371]     Train net output #0: loss = 0.00149412 (* 1 = 0.00149412 loss)
I0628 19:23:31.874994 32082 sgd_solver.cpp:137] Iteration 23000, lr = 0.00640625, m = 0.9
I0628 19:23:33.594794 32082 solver.cpp:349] Iteration 23100 (58.1671 iter/s, 1.71918s/100 iter), loss = 0.0021158
I0628 19:23:33.594822 32082 solver.cpp:371]     Train net output #0: loss = 0.00211579 (* 1 = 0.00211579 loss)
I0628 19:23:33.594828 32082 sgd_solver.cpp:137] Iteration 23100, lr = 0.00639063, m = 0.9
I0628 19:23:35.316073 32082 solver.cpp:349] Iteration 23200 (58.1182 iter/s, 1.72063s/100 iter), loss = 0.00132608
I0628 19:23:35.316102 32082 solver.cpp:371]     Train net output #0: loss = 0.00132607 (* 1 = 0.00132607 loss)
I0628 19:23:35.316107 32082 sgd_solver.cpp:137] Iteration 23200, lr = 0.006375, m = 0.9
I0628 19:23:37.045815 32082 solver.cpp:349] Iteration 23300 (57.8338 iter/s, 1.72909s/100 iter), loss = 0.0022326
I0628 19:23:37.045837 32082 solver.cpp:371]     Train net output #0: loss = 0.0022326 (* 1 = 0.0022326 loss)
I0628 19:23:37.045842 32082 sgd_solver.cpp:137] Iteration 23300, lr = 0.00635938, m = 0.9
I0628 19:23:38.769493 32082 solver.cpp:349] Iteration 23400 (58.0369 iter/s, 1.72304s/100 iter), loss = 0.00292
I0628 19:23:38.769516 32082 solver.cpp:371]     Train net output #0: loss = 0.00292 (* 1 = 0.00292 loss)
I0628 19:23:38.769521 32082 sgd_solver.cpp:137] Iteration 23400, lr = 0.00634375, m = 0.9
I0628 19:23:39.355033 32050 data_reader.cpp:262] Starting prefetch of epoch 30
I0628 19:23:40.489640 32082 solver.cpp:349] Iteration 23500 (58.1561 iter/s, 1.71951s/100 iter), loss = 0.00406069
I0628 19:23:40.489666 32082 solver.cpp:371]     Train net output #0: loss = 0.00406069 (* 1 = 0.00406069 loss)
I0628 19:23:40.489672 32082 sgd_solver.cpp:137] Iteration 23500, lr = 0.00632813, m = 0.9
I0628 19:23:42.211535 32082 solver.cpp:349] Iteration 23600 (58.0971 iter/s, 1.72126s/100 iter), loss = 0.00136618
I0628 19:23:42.211557 32082 solver.cpp:371]     Train net output #0: loss = 0.00136618 (* 1 = 0.00136618 loss)
I0628 19:23:42.211561 32082 sgd_solver.cpp:137] Iteration 23600, lr = 0.0063125, m = 0.9
I0628 19:23:43.932713 32082 solver.cpp:349] Iteration 23700 (58.121 iter/s, 1.72055s/100 iter), loss = 0.00280409
I0628 19:23:43.932749 32082 solver.cpp:371]     Train net output #0: loss = 0.00280409 (* 1 = 0.00280409 loss)
I0628 19:23:43.932752 32082 sgd_solver.cpp:137] Iteration 23700, lr = 0.00629687, m = 0.9
I0628 19:23:45.653357 32082 solver.cpp:349] Iteration 23800 (58.1394 iter/s, 1.72s/100 iter), loss = 0.00110968
I0628 19:23:45.653378 32082 solver.cpp:371]     Train net output #0: loss = 0.00110967 (* 1 = 0.00110967 loss)
I0628 19:23:45.653383 32082 sgd_solver.cpp:137] Iteration 23800, lr = 0.00628125, m = 0.9
I0628 19:23:47.376703 32082 solver.cpp:349] Iteration 23900 (58.0477 iter/s, 1.72272s/100 iter), loss = 0.00161635
I0628 19:23:47.376725 32082 solver.cpp:371]     Train net output #0: loss = 0.00161635 (* 1 = 0.00161635 loss)
I0628 19:23:47.376729 32082 sgd_solver.cpp:137] Iteration 23900, lr = 0.00626562, m = 0.9
I0628 19:23:49.081579 32082 solver.cpp:401] Sparsity after update:
I0628 19:23:49.082677 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:23:49.082686 32082 net.cpp:2170] conv1a_param_0(0.24) 
I0628 19:23:49.082692 32082 net.cpp:2170] conv1b_param_0(0.48) 
I0628 19:23:49.082695 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:23:49.082700 32082 net.cpp:2170] res2a_branch2a_param_0(0.48) 
I0628 19:23:49.082705 32082 net.cpp:2170] res2a_branch2b_param_0(0.48) 
I0628 19:23:49.082708 32082 net.cpp:2170] res3a_branch2a_param_0(0.48) 
I0628 19:23:49.082713 32082 net.cpp:2170] res3a_branch2b_param_0(0.48) 
I0628 19:23:49.082717 32082 net.cpp:2170] res4a_branch2a_param_0(0.48) 
I0628 19:23:49.082721 32082 net.cpp:2170] res4a_branch2b_param_0(0.48) 
I0628 19:23:49.082726 32082 net.cpp:2170] res5a_branch2a_param_0(0.48) 
I0628 19:23:49.082729 32082 net.cpp:2170] res5a_branch2b_param_0(0.48) 
I0628 19:23:49.082733 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.3599e+06) 0.479
I0628 19:23:49.082746 32082 solver.cpp:545] Iteration 24000, Testing net (#0)
I0628 19:23:50.080083 32080 data_reader.cpp:262] Starting prefetch of epoch 24
I0628 19:23:50.101380 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.917
I0628 19:23:50.101393 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:23:50.101398 32082 solver.cpp:630]     Test net output #2: loss = 0.301958 (* 1 = 0.301958 loss)
I0628 19:23:50.101411 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01832s
I0628 19:23:50.121851 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.5
I0628 19:23:50.693084 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:23:50.694553 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:23:50.694911 32082 solver.cpp:349] Iteration 24000 (30.1473 iter/s, 3.31704s/100 iter), loss = 0.00116391
I0628 19:23:50.694928 32082 solver.cpp:371]     Train net output #0: loss = 0.00116391 (* 1 = 0.00116391 loss)
I0628 19:23:50.694934 32082 sgd_solver.cpp:137] Iteration 24000, lr = 0.00625, m = 0.9
I0628 19:23:52.418093 32082 solver.cpp:349] Iteration 24100 (58.053 iter/s, 1.72256s/100 iter), loss = 0.00326255
I0628 19:23:52.418115 32082 solver.cpp:371]     Train net output #0: loss = 0.00326255 (* 1 = 0.00326255 loss)
I0628 19:23:52.418119 32082 sgd_solver.cpp:137] Iteration 24100, lr = 0.00623438, m = 0.9
I0628 19:23:54.136739 32082 solver.cpp:349] Iteration 24200 (58.2063 iter/s, 1.71803s/100 iter), loss = 0.0046329
I0628 19:23:54.136764 32082 solver.cpp:371]     Train net output #0: loss = 0.0046329 (* 1 = 0.0046329 loss)
I0628 19:23:54.136770 32082 sgd_solver.cpp:137] Iteration 24200, lr = 0.00621875, m = 0.9
I0628 19:23:54.394824 32050 data_reader.cpp:262] Starting prefetch of epoch 31
I0628 19:23:55.857755 32082 solver.cpp:349] Iteration 24300 (58.1262 iter/s, 1.72039s/100 iter), loss = 0.00367827
I0628 19:23:55.857775 32082 solver.cpp:371]     Train net output #0: loss = 0.00367826 (* 1 = 0.00367826 loss)
I0628 19:23:55.857779 32082 sgd_solver.cpp:137] Iteration 24300, lr = 0.00620312, m = 0.9
I0628 19:23:57.581557 32082 solver.cpp:349] Iteration 24400 (58.032 iter/s, 1.72319s/100 iter), loss = 0.00171259
I0628 19:23:57.581593 32082 solver.cpp:371]     Train net output #0: loss = 0.00171259 (* 1 = 0.00171259 loss)
I0628 19:23:57.581598 32082 sgd_solver.cpp:137] Iteration 24400, lr = 0.0061875, m = 0.9
I0628 19:23:59.306432 32082 solver.cpp:349] Iteration 24500 (57.9965 iter/s, 1.72424s/100 iter), loss = 0.00254918
I0628 19:23:59.306457 32082 solver.cpp:371]     Train net output #0: loss = 0.00254918 (* 1 = 0.00254918 loss)
I0628 19:23:59.306463 32082 sgd_solver.cpp:137] Iteration 24500, lr = 0.00617187, m = 0.9
I0628 19:24:01.030421 32082 solver.cpp:349] Iteration 24600 (58.0259 iter/s, 1.72337s/100 iter), loss = 0.00365565
I0628 19:24:01.030486 32082 solver.cpp:371]     Train net output #0: loss = 0.00365565 (* 1 = 0.00365565 loss)
I0628 19:24:01.030491 32082 sgd_solver.cpp:137] Iteration 24600, lr = 0.00615625, m = 0.9
I0628 19:24:02.759022 32082 solver.cpp:349] Iteration 24700 (57.8724 iter/s, 1.72794s/100 iter), loss = 0.00223862
I0628 19:24:02.759084 32082 solver.cpp:371]     Train net output #0: loss = 0.00223861 (* 1 = 0.00223861 loss)
I0628 19:24:02.759099 32082 sgd_solver.cpp:137] Iteration 24700, lr = 0.00614062, m = 0.9
I0628 19:24:04.495790 32082 solver.cpp:349] Iteration 24800 (57.6006 iter/s, 1.73609s/100 iter), loss = 0.00418949
I0628 19:24:04.495815 32082 solver.cpp:371]     Train net output #0: loss = 0.00418949 (* 1 = 0.00418949 loss)
I0628 19:24:04.495821 32082 sgd_solver.cpp:137] Iteration 24800, lr = 0.006125, m = 0.9
I0628 19:24:06.214879 32082 solver.cpp:349] Iteration 24900 (58.1912 iter/s, 1.71847s/100 iter), loss = 0.000711182
I0628 19:24:06.214905 32082 solver.cpp:371]     Train net output #0: loss = 0.00071118 (* 1 = 0.00071118 loss)
I0628 19:24:06.214910 32082 sgd_solver.cpp:137] Iteration 24900, lr = 0.00610937, m = 0.9
I0628 19:24:07.866806 32050 data_reader.cpp:262] Starting prefetch of epoch 32
I0628 19:24:07.918223 32082 solver.cpp:401] Sparsity after update:
I0628 19:24:07.919277 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:24:07.919283 32082 net.cpp:2170] conv1a_param_0(0.25) 
I0628 19:24:07.919291 32082 net.cpp:2170] conv1b_param_0(0.5) 
I0628 19:24:07.919293 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:24:07.919296 32082 net.cpp:2170] res2a_branch2a_param_0(0.5) 
I0628 19:24:07.919298 32082 net.cpp:2170] res2a_branch2b_param_0(0.5) 
I0628 19:24:07.919301 32082 net.cpp:2170] res3a_branch2a_param_0(0.5) 
I0628 19:24:07.919303 32082 net.cpp:2170] res3a_branch2b_param_0(0.5) 
I0628 19:24:07.919306 32082 net.cpp:2170] res4a_branch2a_param_0(0.5) 
I0628 19:24:07.919307 32082 net.cpp:2170] res4a_branch2b_param_0(0.5) 
I0628 19:24:07.919309 32082 net.cpp:2170] res5a_branch2a_param_0(0.5) 
I0628 19:24:07.919312 32082 net.cpp:2170] res5a_branch2b_param_0(0.5) 
I0628 19:24:07.919313 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.17678e+06/2.3599e+06) 0.499
I0628 19:24:07.919322 32082 solver.cpp:545] Iteration 25000, Testing net (#0)
I0628 19:24:08.915524 32080 data_reader.cpp:262] Starting prefetch of epoch 25
I0628 19:24:08.941742 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9178
I0628 19:24:08.941771 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:24:08.941779 32082 solver.cpp:630]     Test net output #2: loss = 0.30503 (* 1 = 0.30503 loss)
I0628 19:24:08.941800 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02214s
I0628 19:24:08.959055 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.52
I0628 19:24:09.560417 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:24:09.561887 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:24:09.562248 32082 solver.cpp:349] Iteration 25000 (29.8845 iter/s, 3.34622s/100 iter), loss = 0.00126559
I0628 19:24:09.562268 32082 solver.cpp:371]     Train net output #0: loss = 0.00126558 (* 1 = 0.00126558 loss)
I0628 19:24:09.562273 32082 sgd_solver.cpp:137] Iteration 25000, lr = 0.00609375, m = 0.9
I0628 19:24:11.282735 32082 solver.cpp:349] Iteration 25100 (58.1435 iter/s, 1.71988s/100 iter), loss = 0.00135276
I0628 19:24:11.282760 32082 solver.cpp:371]     Train net output #0: loss = 0.00135276 (* 1 = 0.00135276 loss)
I0628 19:24:11.282766 32082 sgd_solver.cpp:137] Iteration 25100, lr = 0.00607812, m = 0.9
I0628 19:24:13.004425 32082 solver.cpp:349] Iteration 25200 (58.1031 iter/s, 1.72108s/100 iter), loss = 0.00335391
I0628 19:24:13.004446 32082 solver.cpp:371]     Train net output #0: loss = 0.00335391 (* 1 = 0.00335391 loss)
I0628 19:24:13.004449 32082 sgd_solver.cpp:137] Iteration 25200, lr = 0.0060625, m = 0.9
I0628 19:24:14.722285 32082 solver.cpp:349] Iteration 25300 (58.2323 iter/s, 1.71726s/100 iter), loss = 0.00304155
I0628 19:24:14.722327 32082 solver.cpp:371]     Train net output #0: loss = 0.00304155 (* 1 = 0.00304155 loss)
I0628 19:24:14.722333 32082 sgd_solver.cpp:137] Iteration 25300, lr = 0.00604687, m = 0.9
I0628 19:24:16.443972 32082 solver.cpp:349] Iteration 25400 (58.1036 iter/s, 1.72106s/100 iter), loss = 0.00229869
I0628 19:24:16.443995 32082 solver.cpp:371]     Train net output #0: loss = 0.00229869 (* 1 = 0.00229869 loss)
I0628 19:24:16.444000 32082 sgd_solver.cpp:137] Iteration 25400, lr = 0.00603125, m = 0.9
I0628 19:24:18.170461 32082 solver.cpp:349] Iteration 25500 (57.9412 iter/s, 1.72589s/100 iter), loss = 0.00128942
I0628 19:24:18.170487 32082 solver.cpp:371]     Train net output #0: loss = 0.00128942 (* 1 = 0.00128942 loss)
I0628 19:24:18.170493 32082 sgd_solver.cpp:137] Iteration 25500, lr = 0.00601562, m = 0.9
I0628 19:24:19.891433 32082 solver.cpp:349] Iteration 25600 (58.1271 iter/s, 1.72037s/100 iter), loss = 0.00202967
I0628 19:24:19.891460 32082 solver.cpp:371]     Train net output #0: loss = 0.00202967 (* 1 = 0.00202967 loss)
I0628 19:24:19.891466 32082 sgd_solver.cpp:137] Iteration 25600, lr = 0.006, m = 0.9
I0628 19:24:21.609705 32082 solver.cpp:349] Iteration 25700 (58.2185 iter/s, 1.71767s/100 iter), loss = 0.00251289
I0628 19:24:21.609732 32082 solver.cpp:371]     Train net output #0: loss = 0.00251289 (* 1 = 0.00251289 loss)
I0628 19:24:21.609738 32082 sgd_solver.cpp:137] Iteration 25700, lr = 0.00598437, m = 0.9
I0628 19:24:22.956204 32050 data_reader.cpp:262] Starting prefetch of epoch 33
I0628 19:24:23.333916 32082 solver.cpp:349] Iteration 25800 (58.0179 iter/s, 1.72361s/100 iter), loss = 0.00166926
I0628 19:24:23.333942 32082 solver.cpp:371]     Train net output #0: loss = 0.00166926 (* 1 = 0.00166926 loss)
I0628 19:24:23.333948 32082 sgd_solver.cpp:137] Iteration 25800, lr = 0.00596875, m = 0.9
I0628 19:24:25.054853 32082 solver.cpp:349] Iteration 25900 (58.1281 iter/s, 1.72034s/100 iter), loss = 0.00302531
I0628 19:24:25.054873 32082 solver.cpp:371]     Train net output #0: loss = 0.0030253 (* 1 = 0.0030253 loss)
I0628 19:24:25.054877 32082 sgd_solver.cpp:137] Iteration 25900, lr = 0.00595312, m = 0.9
I0628 19:24:26.758683 32082 solver.cpp:401] Sparsity after update:
I0628 19:24:26.759793 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:24:26.759800 32082 net.cpp:2170] conv1a_param_0(0.26) 
I0628 19:24:26.759807 32082 net.cpp:2170] conv1b_param_0(0.52) 
I0628 19:24:26.759809 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:24:26.759812 32082 net.cpp:2170] res2a_branch2a_param_0(0.52) 
I0628 19:24:26.759814 32082 net.cpp:2170] res2a_branch2b_param_0(0.52) 
I0628 19:24:26.759816 32082 net.cpp:2170] res3a_branch2a_param_0(0.52) 
I0628 19:24:26.759819 32082 net.cpp:2170] res3a_branch2b_param_0(0.52) 
I0628 19:24:26.759821 32082 net.cpp:2170] res4a_branch2a_param_0(0.52) 
I0628 19:24:26.759824 32082 net.cpp:2170] res4a_branch2b_param_0(0.52) 
I0628 19:24:26.759827 32082 net.cpp:2170] res5a_branch2a_param_0(0.52) 
I0628 19:24:26.759830 32082 net.cpp:2170] res5a_branch2b_param_0(0.52) 
I0628 19:24:26.759834 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.3599e+06) 0.519
I0628 19:24:26.759850 32082 solver.cpp:545] Iteration 26000, Testing net (#0)
I0628 19:24:27.756052 32080 data_reader.cpp:262] Starting prefetch of epoch 26
I0628 19:24:27.778436 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9172
I0628 19:24:27.778457 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:24:27.778465 32082 solver.cpp:630]     Test net output #2: loss = 0.304759 (* 1 = 0.304759 loss)
I0628 19:24:27.778484 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0183s
I0628 19:24:27.796644 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.54
I0628 19:24:28.409518 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:24:28.410989 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:24:28.411347 32082 solver.cpp:349] Iteration 26000 (29.8029 iter/s, 3.35538s/100 iter), loss = 0.00356309
I0628 19:24:28.411375 32082 solver.cpp:371]     Train net output #0: loss = 0.00356309 (* 1 = 0.00356309 loss)
I0628 19:24:28.411383 32082 sgd_solver.cpp:137] Iteration 26000, lr = 0.0059375, m = 0.9
I0628 19:24:30.132568 32082 solver.cpp:349] Iteration 26100 (58.1186 iter/s, 1.72062s/100 iter), loss = 0.0077015
I0628 19:24:30.132596 32082 solver.cpp:371]     Train net output #0: loss = 0.0077015 (* 1 = 0.0077015 loss)
I0628 19:24:30.132601 32082 sgd_solver.cpp:137] Iteration 26100, lr = 0.00592188, m = 0.9
I0628 19:24:31.860738 32082 solver.cpp:349] Iteration 26200 (57.8848 iter/s, 1.72757s/100 iter), loss = 0.00370453
I0628 19:24:31.860815 32082 solver.cpp:371]     Train net output #0: loss = 0.00370453 (* 1 = 0.00370453 loss)
I0628 19:24:31.860822 32082 sgd_solver.cpp:137] Iteration 26200, lr = 0.00590625, m = 0.9
I0628 19:24:33.582290 32082 solver.cpp:349] Iteration 26300 (58.109 iter/s, 1.7209s/100 iter), loss = 0.00333424
I0628 19:24:33.582316 32082 solver.cpp:371]     Train net output #0: loss = 0.00333424 (* 1 = 0.00333424 loss)
I0628 19:24:33.582322 32082 sgd_solver.cpp:137] Iteration 26300, lr = 0.00589063, m = 0.9
I0628 19:24:35.307708 32082 solver.cpp:349] Iteration 26400 (57.977 iter/s, 1.72482s/100 iter), loss = 0.0023658
I0628 19:24:35.307734 32082 solver.cpp:371]     Train net output #0: loss = 0.0023658 (* 1 = 0.0023658 loss)
I0628 19:24:35.307739 32082 sgd_solver.cpp:137] Iteration 26400, lr = 0.005875, m = 0.9
I0628 19:24:37.027324 32082 solver.cpp:349] Iteration 26500 (58.1725 iter/s, 1.71903s/100 iter), loss = 0.00298276
I0628 19:24:37.027349 32082 solver.cpp:371]     Train net output #0: loss = 0.00298275 (* 1 = 0.00298275 loss)
I0628 19:24:37.027355 32082 sgd_solver.cpp:137] Iteration 26500, lr = 0.00585938, m = 0.9
I0628 19:24:38.041836 32050 data_reader.cpp:262] Starting prefetch of epoch 34
I0628 19:24:38.744817 32082 solver.cpp:349] Iteration 26600 (58.2444 iter/s, 1.7169s/100 iter), loss = 0.00327829
I0628 19:24:38.744843 32082 solver.cpp:371]     Train net output #0: loss = 0.00327829 (* 1 = 0.00327829 loss)
I0628 19:24:38.744849 32082 sgd_solver.cpp:137] Iteration 26600, lr = 0.00584375, m = 0.9
I0628 19:24:40.465169 32082 solver.cpp:349] Iteration 26700 (58.1475 iter/s, 1.71976s/100 iter), loss = 0.00235766
I0628 19:24:40.465194 32082 solver.cpp:371]     Train net output #0: loss = 0.00235765 (* 1 = 0.00235765 loss)
I0628 19:24:40.465200 32082 sgd_solver.cpp:137] Iteration 26700, lr = 0.00582812, m = 0.9
I0628 19:24:42.188380 32082 solver.cpp:349] Iteration 26800 (58.051 iter/s, 1.72262s/100 iter), loss = 0.00331774
I0628 19:24:42.188405 32082 solver.cpp:371]     Train net output #0: loss = 0.00331773 (* 1 = 0.00331773 loss)
I0628 19:24:42.188411 32082 sgd_solver.cpp:137] Iteration 26800, lr = 0.0058125, m = 0.9
I0628 19:24:43.908401 32082 solver.cpp:349] Iteration 26900 (58.1586 iter/s, 1.71944s/100 iter), loss = 0.00156465
I0628 19:24:43.908427 32082 solver.cpp:371]     Train net output #0: loss = 0.00156464 (* 1 = 0.00156464 loss)
I0628 19:24:43.908433 32082 sgd_solver.cpp:137] Iteration 26900, lr = 0.00579687, m = 0.9
I0628 19:24:45.609601 32082 solver.cpp:401] Sparsity after update:
I0628 19:24:45.610652 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:24:45.610659 32082 net.cpp:2170] conv1a_param_0(0.27) 
I0628 19:24:45.610666 32082 net.cpp:2170] conv1b_param_0(0.54) 
I0628 19:24:45.610667 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:24:45.610669 32082 net.cpp:2170] res2a_branch2a_param_0(0.54) 
I0628 19:24:45.610671 32082 net.cpp:2170] res2a_branch2b_param_0(0.54) 
I0628 19:24:45.610676 32082 net.cpp:2170] res3a_branch2a_param_0(0.54) 
I0628 19:24:45.610677 32082 net.cpp:2170] res3a_branch2b_param_0(0.54) 
I0628 19:24:45.610678 32082 net.cpp:2170] res4a_branch2a_param_0(0.54) 
I0628 19:24:45.610680 32082 net.cpp:2170] res4a_branch2b_param_0(0.54) 
I0628 19:24:45.610682 32082 net.cpp:2170] res5a_branch2a_param_0(0.54) 
I0628 19:24:45.610684 32082 net.cpp:2170] res5a_branch2b_param_0(0.54) 
I0628 19:24:45.610687 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.3599e+06) 0.539
I0628 19:24:45.610693 32082 solver.cpp:545] Iteration 27000, Testing net (#0)
I0628 19:24:46.606994 32080 data_reader.cpp:262] Starting prefetch of epoch 27
I0628 19:24:46.629393 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.915
I0628 19:24:46.629406 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:24:46.629411 32082 solver.cpp:630]     Test net output #2: loss = 0.306885 (* 1 = 0.306885 loss)
I0628 19:24:46.629425 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01841s
I0628 19:24:46.646632 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.56
I0628 19:24:47.283013 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:24:47.284489 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:24:47.284848 32082 solver.cpp:349] Iteration 27000 (29.6266 iter/s, 3.37535s/100 iter), loss = 0.00466532
I0628 19:24:47.284865 32082 solver.cpp:371]     Train net output #0: loss = 0.00466531 (* 1 = 0.00466531 loss)
I0628 19:24:47.284871 32082 sgd_solver.cpp:137] Iteration 27000, lr = 0.00578125, m = 0.9
I0628 19:24:49.011363 32082 solver.cpp:349] Iteration 27100 (57.9394 iter/s, 1.72594s/100 iter), loss = 0.0018782
I0628 19:24:49.011389 32082 solver.cpp:371]     Train net output #0: loss = 0.00187819 (* 1 = 0.00187819 loss)
I0628 19:24:49.011395 32082 sgd_solver.cpp:137] Iteration 27100, lr = 0.00576563, m = 0.9
I0628 19:24:50.729307 32082 solver.cpp:349] Iteration 27200 (58.2287 iter/s, 1.71737s/100 iter), loss = 0.00198111
I0628 19:24:50.729329 32082 solver.cpp:371]     Train net output #0: loss = 0.00198111 (* 1 = 0.00198111 loss)
I0628 19:24:50.729336 32082 sgd_solver.cpp:137] Iteration 27200, lr = 0.00575, m = 0.9
I0628 19:24:52.452019 32082 solver.cpp:349] Iteration 27300 (58.0674 iter/s, 1.72214s/100 iter), loss = 0.00366353
I0628 19:24:52.452046 32082 solver.cpp:371]     Train net output #0: loss = 0.00366352 (* 1 = 0.00366352 loss)
I0628 19:24:52.452052 32082 sgd_solver.cpp:137] Iteration 27300, lr = 0.00573438, m = 0.9
I0628 19:24:53.138563 32050 data_reader.cpp:262] Starting prefetch of epoch 35
I0628 19:24:54.171617 32082 solver.cpp:349] Iteration 27400 (58.1728 iter/s, 1.71902s/100 iter), loss = 0.00373996
I0628 19:24:54.171643 32082 solver.cpp:371]     Train net output #0: loss = 0.00373995 (* 1 = 0.00373995 loss)
I0628 19:24:54.171648 32082 sgd_solver.cpp:137] Iteration 27400, lr = 0.00571875, m = 0.9
I0628 19:24:55.890153 32082 solver.cpp:349] Iteration 27500 (58.2085 iter/s, 1.71796s/100 iter), loss = 0.00387776
I0628 19:24:55.890174 32082 solver.cpp:371]     Train net output #0: loss = 0.00387776 (* 1 = 0.00387776 loss)
I0628 19:24:55.890178 32082 sgd_solver.cpp:137] Iteration 27500, lr = 0.00570312, m = 0.9
I0628 19:24:57.609365 32082 solver.cpp:349] Iteration 27600 (58.1853 iter/s, 1.71865s/100 iter), loss = 0.00659175
I0628 19:24:57.609390 32082 solver.cpp:371]     Train net output #0: loss = 0.00659174 (* 1 = 0.00659174 loss)
I0628 19:24:57.609395 32082 sgd_solver.cpp:137] Iteration 27600, lr = 0.0056875, m = 0.9
I0628 19:24:59.330711 32082 solver.cpp:349] Iteration 27700 (58.1134 iter/s, 1.72077s/100 iter), loss = 0.00105971
I0628 19:24:59.330737 32082 solver.cpp:371]     Train net output #0: loss = 0.0010597 (* 1 = 0.0010597 loss)
I0628 19:24:59.330744 32082 sgd_solver.cpp:137] Iteration 27700, lr = 0.00567187, m = 0.9
I0628 19:25:01.051568 32082 solver.cpp:349] Iteration 27800 (58.13 iter/s, 1.72028s/100 iter), loss = 0.00382418
I0628 19:25:01.051589 32082 solver.cpp:371]     Train net output #0: loss = 0.00382417 (* 1 = 0.00382417 loss)
I0628 19:25:01.051594 32082 sgd_solver.cpp:137] Iteration 27800, lr = 0.00565625, m = 0.9
I0628 19:25:02.771811 32082 solver.cpp:349] Iteration 27900 (58.1505 iter/s, 1.71968s/100 iter), loss = 0.00423872
I0628 19:25:02.771880 32082 solver.cpp:371]     Train net output #0: loss = 0.00423872 (* 1 = 0.00423872 loss)
I0628 19:25:02.771888 32082 sgd_solver.cpp:137] Iteration 27900, lr = 0.00564062, m = 0.9
I0628 19:25:04.472621 32082 solver.cpp:401] Sparsity after update:
I0628 19:25:04.473685 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:25:04.473695 32082 net.cpp:2170] conv1a_param_0(0.28) 
I0628 19:25:04.473701 32082 net.cpp:2170] conv1b_param_0(0.56) 
I0628 19:25:04.473703 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:25:04.473706 32082 net.cpp:2170] res2a_branch2a_param_0(0.56) 
I0628 19:25:04.473707 32082 net.cpp:2170] res2a_branch2b_param_0(0.56) 
I0628 19:25:04.473709 32082 net.cpp:2170] res3a_branch2a_param_0(0.56) 
I0628 19:25:04.473711 32082 net.cpp:2170] res3a_branch2b_param_0(0.56) 
I0628 19:25:04.473714 32082 net.cpp:2170] res4a_branch2a_param_0(0.56) 
I0628 19:25:04.473716 32082 net.cpp:2170] res4a_branch2b_param_0(0.56) 
I0628 19:25:04.473718 32082 net.cpp:2170] res5a_branch2a_param_0(0.56) 
I0628 19:25:04.473721 32082 net.cpp:2170] res5a_branch2b_param_0(0.56) 
I0628 19:25:04.473723 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.318e+06/2.3599e+06) 0.558
I0628 19:25:04.473731 32082 solver.cpp:545] Iteration 28000, Testing net (#0)
I0628 19:25:05.471539 32080 data_reader.cpp:262] Starting prefetch of epoch 28
I0628 19:25:05.493854 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.915
I0628 19:25:05.493870 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:25:05.493875 32082 solver.cpp:630]     Test net output #2: loss = 0.308403 (* 1 = 0.308403 loss)
I0628 19:25:05.493890 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01984s
I0628 19:25:05.511186 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.58
I0628 19:25:06.183776 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:25:06.185250 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:25:06.185614 32082 solver.cpp:349] Iteration 28000 (29.3026 iter/s, 3.41267s/100 iter), loss = 0.00758583
I0628 19:25:06.185632 32082 solver.cpp:371]     Train net output #0: loss = 0.00758583 (* 1 = 0.00758583 loss)
I0628 19:25:06.185637 32082 sgd_solver.cpp:137] Iteration 28000, lr = 0.005625, m = 0.9
I0628 19:25:07.913631 32082 solver.cpp:349] Iteration 28100 (57.8886 iter/s, 1.72746s/100 iter), loss = 0.0050168
I0628 19:25:07.913652 32082 solver.cpp:371]     Train net output #0: loss = 0.00501679 (* 1 = 0.00501679 loss)
I0628 19:25:07.913657 32082 sgd_solver.cpp:137] Iteration 28100, lr = 0.00560937, m = 0.9
I0628 19:25:08.274459 32050 data_reader.cpp:262] Starting prefetch of epoch 36
I0628 19:25:09.633776 32082 solver.cpp:349] Iteration 28200 (58.1535 iter/s, 1.71959s/100 iter), loss = 0.00274666
I0628 19:25:09.633802 32082 solver.cpp:371]     Train net output #0: loss = 0.00274665 (* 1 = 0.00274665 loss)
I0628 19:25:09.633808 32082 sgd_solver.cpp:137] Iteration 28200, lr = 0.00559375, m = 0.9
I0628 19:25:11.352881 32082 solver.cpp:349] Iteration 28300 (58.189 iter/s, 1.71854s/100 iter), loss = 0.00279583
I0628 19:25:11.352908 32082 solver.cpp:371]     Train net output #0: loss = 0.00279582 (* 1 = 0.00279582 loss)
I0628 19:25:11.352915 32082 sgd_solver.cpp:137] Iteration 28300, lr = 0.00557812, m = 0.9
I0628 19:25:13.071796 32082 solver.cpp:349] Iteration 28400 (58.1954 iter/s, 1.71835s/100 iter), loss = 0.00220348
I0628 19:25:13.071821 32082 solver.cpp:371]     Train net output #0: loss = 0.00220347 (* 1 = 0.00220347 loss)
I0628 19:25:13.071828 32082 sgd_solver.cpp:137] Iteration 28400, lr = 0.0055625, m = 0.9
I0628 19:25:14.794862 32082 solver.cpp:349] Iteration 28500 (58.0551 iter/s, 1.7225s/100 iter), loss = 0.00616774
I0628 19:25:14.794885 32082 solver.cpp:371]     Train net output #0: loss = 0.00616773 (* 1 = 0.00616773 loss)
I0628 19:25:14.794889 32082 sgd_solver.cpp:137] Iteration 28500, lr = 0.00554687, m = 0.9
I0628 19:25:16.518028 32082 solver.cpp:349] Iteration 28600 (58.0515 iter/s, 1.72261s/100 iter), loss = 0.00408518
I0628 19:25:16.518071 32082 solver.cpp:371]     Train net output #0: loss = 0.00408517 (* 1 = 0.00408517 loss)
I0628 19:25:16.518077 32082 sgd_solver.cpp:137] Iteration 28600, lr = 0.00553125, m = 0.9
I0628 19:25:18.238307 32082 solver.cpp:349] Iteration 28700 (58.1497 iter/s, 1.7197s/100 iter), loss = 0.00249629
I0628 19:25:18.238333 32082 solver.cpp:371]     Train net output #0: loss = 0.00249628 (* 1 = 0.00249628 loss)
I0628 19:25:18.238339 32082 sgd_solver.cpp:137] Iteration 28700, lr = 0.00551562, m = 0.9
I0628 19:25:19.956948 32082 solver.cpp:349] Iteration 28800 (58.2045 iter/s, 1.71808s/100 iter), loss = 0.0042915
I0628 19:25:19.956970 32082 solver.cpp:371]     Train net output #0: loss = 0.00429149 (* 1 = 0.00429149 loss)
I0628 19:25:19.956976 32082 sgd_solver.cpp:137] Iteration 28800, lr = 0.0055, m = 0.9
I0628 19:25:21.679132 32082 solver.cpp:349] Iteration 28900 (58.0846 iter/s, 1.72163s/100 iter), loss = 0.00242194
I0628 19:25:21.679158 32082 solver.cpp:371]     Train net output #0: loss = 0.00242193 (* 1 = 0.00242193 loss)
I0628 19:25:21.679164 32082 sgd_solver.cpp:137] Iteration 28900, lr = 0.00548437, m = 0.9
I0628 19:25:21.731163 32050 data_reader.cpp:262] Starting prefetch of epoch 37
I0628 19:25:23.383343 32082 solver.cpp:401] Sparsity after update:
I0628 19:25:23.384413 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:25:23.384421 32082 net.cpp:2170] conv1a_param_0(0.29) 
I0628 19:25:23.384428 32082 net.cpp:2170] conv1b_param_0(0.58) 
I0628 19:25:23.384430 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:25:23.384433 32082 net.cpp:2170] res2a_branch2a_param_0(0.58) 
I0628 19:25:23.384435 32082 net.cpp:2170] res2a_branch2b_param_0(0.58) 
I0628 19:25:23.384438 32082 net.cpp:2170] res3a_branch2a_param_0(0.58) 
I0628 19:25:23.384439 32082 net.cpp:2170] res3a_branch2b_param_0(0.58) 
I0628 19:25:23.384441 32082 net.cpp:2170] res4a_branch2a_param_0(0.58) 
I0628 19:25:23.384443 32082 net.cpp:2170] res4a_branch2b_param_0(0.58) 
I0628 19:25:23.384445 32082 net.cpp:2170] res5a_branch2a_param_0(0.58) 
I0628 19:25:23.384447 32082 net.cpp:2170] res5a_branch2b_param_0(0.58) 
I0628 19:25:23.384449 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.36507e+06/2.3599e+06) 0.578
I0628 19:25:23.384457 32082 solver.cpp:545] Iteration 29000, Testing net (#0)
I0628 19:25:24.386255 32080 data_reader.cpp:262] Starting prefetch of epoch 29
I0628 19:25:24.406369 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9138
I0628 19:25:24.406383 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:25:24.406386 32082 solver.cpp:630]     Test net output #2: loss = 0.308685 (* 1 = 0.308685 loss)
I0628 19:25:24.406400 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02164s
I0628 19:25:24.423640 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.6
I0628 19:25:25.122592 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:25:25.124065 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:25:25.124423 32082 solver.cpp:349] Iteration 29000 (29.0341 iter/s, 3.44422s/100 iter), loss = 0.00597963
I0628 19:25:25.124440 32082 solver.cpp:371]     Train net output #0: loss = 0.00597962 (* 1 = 0.00597962 loss)
I0628 19:25:25.124445 32082 sgd_solver.cpp:137] Iteration 29000, lr = 0.00546875, m = 0.9
I0628 19:25:26.844480 32082 solver.cpp:349] Iteration 29100 (58.1561 iter/s, 1.71951s/100 iter), loss = 0.00287913
I0628 19:25:26.844506 32082 solver.cpp:371]     Train net output #0: loss = 0.00287913 (* 1 = 0.00287913 loss)
I0628 19:25:26.844512 32082 sgd_solver.cpp:137] Iteration 29100, lr = 0.00545313, m = 0.9
I0628 19:25:28.565696 32082 solver.cpp:349] Iteration 29200 (58.1172 iter/s, 1.72066s/100 iter), loss = 0.00285409
I0628 19:25:28.565723 32082 solver.cpp:371]     Train net output #0: loss = 0.00285408 (* 1 = 0.00285408 loss)
I0628 19:25:28.565728 32082 sgd_solver.cpp:137] Iteration 29200, lr = 0.0054375, m = 0.9
I0628 19:25:30.283984 32082 solver.cpp:349] Iteration 29300 (58.2162 iter/s, 1.71773s/100 iter), loss = 0.00719778
I0628 19:25:30.284024 32082 solver.cpp:371]     Train net output #0: loss = 0.00719777 (* 1 = 0.00719777 loss)
I0628 19:25:30.284031 32082 sgd_solver.cpp:137] Iteration 29300, lr = 0.00542188, m = 0.9
I0628 19:25:32.001339 32082 solver.cpp:349] Iteration 29400 (58.2483 iter/s, 1.71679s/100 iter), loss = 0.00347338
I0628 19:25:32.001365 32082 solver.cpp:371]     Train net output #0: loss = 0.00347338 (* 1 = 0.00347338 loss)
I0628 19:25:32.001371 32082 sgd_solver.cpp:137] Iteration 29400, lr = 0.00540625, m = 0.9
I0628 19:25:33.729456 32082 solver.cpp:349] Iteration 29500 (57.885 iter/s, 1.72756s/100 iter), loss = 0.00272149
I0628 19:25:33.729542 32082 solver.cpp:371]     Train net output #0: loss = 0.00272149 (* 1 = 0.00272149 loss)
I0628 19:25:33.729554 32082 sgd_solver.cpp:137] Iteration 29500, lr = 0.00539062, m = 0.9
I0628 19:25:35.449051 32082 solver.cpp:349] Iteration 29600 (58.1742 iter/s, 1.71898s/100 iter), loss = 0.00742239
I0628 19:25:35.449076 32082 solver.cpp:371]     Train net output #0: loss = 0.00742239 (* 1 = 0.00742239 loss)
I0628 19:25:35.449082 32082 sgd_solver.cpp:137] Iteration 29600, lr = 0.005375, m = 0.9
I0628 19:25:36.896714 32050 data_reader.cpp:262] Starting prefetch of epoch 38
I0628 19:25:37.176542 32082 solver.cpp:349] Iteration 29700 (57.9059 iter/s, 1.72694s/100 iter), loss = 0.00369038
I0628 19:25:37.176570 32082 solver.cpp:371]     Train net output #0: loss = 0.00369037 (* 1 = 0.00369037 loss)
I0628 19:25:37.176578 32082 sgd_solver.cpp:137] Iteration 29700, lr = 0.00535937, m = 0.9
I0628 19:25:38.902904 32082 solver.cpp:349] Iteration 29800 (57.9439 iter/s, 1.72581s/100 iter), loss = 0.00276335
I0628 19:25:38.902930 32082 solver.cpp:371]     Train net output #0: loss = 0.00276334 (* 1 = 0.00276334 loss)
I0628 19:25:38.902935 32082 sgd_solver.cpp:137] Iteration 29800, lr = 0.00534375, m = 0.9
I0628 19:25:40.622119 32082 solver.cpp:349] Iteration 29900 (58.1845 iter/s, 1.71867s/100 iter), loss = 0.00225052
I0628 19:25:40.622143 32082 solver.cpp:371]     Train net output #0: loss = 0.00225051 (* 1 = 0.00225051 loss)
I0628 19:25:40.622146 32082 sgd_solver.cpp:137] Iteration 29900, lr = 0.00532812, m = 0.9
I0628 19:25:42.330240 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0628 19:25:42.338052 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0628 19:25:42.341480 32082 solver.cpp:401] Sparsity after update:
I0628 19:25:42.342700 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:25:42.342707 32082 net.cpp:2170] conv1a_param_0(0.3) 
I0628 19:25:42.342715 32082 net.cpp:2170] conv1b_param_0(0.6) 
I0628 19:25:42.342716 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:25:42.342718 32082 net.cpp:2170] res2a_branch2a_param_0(0.6) 
I0628 19:25:42.342720 32082 net.cpp:2170] res2a_branch2b_param_0(0.6) 
I0628 19:25:42.342722 32082 net.cpp:2170] res3a_branch2a_param_0(0.6) 
I0628 19:25:42.342725 32082 net.cpp:2170] res3a_branch2b_param_0(0.6) 
I0628 19:25:42.342726 32082 net.cpp:2170] res4a_branch2a_param_0(0.6) 
I0628 19:25:42.342728 32082 net.cpp:2170] res4a_branch2b_param_0(0.6) 
I0628 19:25:42.342730 32082 net.cpp:2170] res5a_branch2a_param_0(0.6) 
I0628 19:25:42.342731 32082 net.cpp:2170] res5a_branch2b_param_0(0.6) 
I0628 19:25:42.342733 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.3599e+06) 0.598
I0628 19:25:42.342741 32082 solver.cpp:545] Iteration 30000, Testing net (#0)
I0628 19:25:43.339903 32080 data_reader.cpp:262] Starting prefetch of epoch 30
I0628 19:25:43.359998 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9146
I0628 19:25:43.360010 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:25:43.360015 32082 solver.cpp:630]     Test net output #2: loss = 0.311607 (* 1 = 0.311607 loss)
I0628 19:25:43.360029 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01699s
I0628 19:25:43.377415 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.62
I0628 19:25:44.105124 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:25:44.106595 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:25:44.106950 32082 solver.cpp:349] Iteration 30000 (28.7044 iter/s, 3.48378s/100 iter), loss = 0.00882763
I0628 19:25:44.106968 32082 solver.cpp:371]     Train net output #0: loss = 0.00882763 (* 1 = 0.00882763 loss)
I0628 19:25:44.106973 32082 sgd_solver.cpp:137] Iteration 30000, lr = 0.0053125, m = 0.9
I0628 19:25:45.827836 32082 solver.cpp:349] Iteration 30100 (58.1276 iter/s, 1.72035s/100 iter), loss = 0.0119758
I0628 19:25:45.827878 32082 solver.cpp:371]     Train net output #0: loss = 0.0119758 (* 1 = 0.0119758 loss)
I0628 19:25:45.827885 32082 sgd_solver.cpp:137] Iteration 30100, lr = 0.00529688, m = 0.9
I0628 19:25:47.549576 32082 solver.cpp:349] Iteration 30200 (58.0997 iter/s, 1.72118s/100 iter), loss = 0.00440972
I0628 19:25:47.549598 32082 solver.cpp:371]     Train net output #0: loss = 0.00440972 (* 1 = 0.00440972 loss)
I0628 19:25:47.549602 32082 sgd_solver.cpp:137] Iteration 30200, lr = 0.00528125, m = 0.9
I0628 19:25:49.271919 32082 solver.cpp:349] Iteration 30300 (58.0785 iter/s, 1.72181s/100 iter), loss = 0.00678877
I0628 19:25:49.271946 32082 solver.cpp:371]     Train net output #0: loss = 0.00678877 (* 1 = 0.00678877 loss)
I0628 19:25:49.271952 32082 sgd_solver.cpp:137] Iteration 30300, lr = 0.00526563, m = 0.9
I0628 19:25:50.991188 32082 solver.cpp:349] Iteration 30400 (58.1827 iter/s, 1.71872s/100 iter), loss = 0.00486152
I0628 19:25:50.991214 32082 solver.cpp:371]     Train net output #0: loss = 0.00486151 (* 1 = 0.00486151 loss)
I0628 19:25:50.991219 32082 sgd_solver.cpp:137] Iteration 30400, lr = 0.00525, m = 0.9
I0628 19:25:52.112284 32050 data_reader.cpp:262] Starting prefetch of epoch 39
I0628 19:25:52.713943 32082 solver.cpp:349] Iteration 30500 (58.0648 iter/s, 1.72221s/100 iter), loss = 0.00344304
I0628 19:25:52.713968 32082 solver.cpp:371]     Train net output #0: loss = 0.00344304 (* 1 = 0.00344304 loss)
I0628 19:25:52.713974 32082 sgd_solver.cpp:137] Iteration 30500, lr = 0.00523437, m = 0.9
I0628 19:25:54.435698 32082 solver.cpp:349] Iteration 30600 (58.0984 iter/s, 1.72122s/100 iter), loss = 0.0039297
I0628 19:25:54.435725 32082 solver.cpp:371]     Train net output #0: loss = 0.0039297 (* 1 = 0.0039297 loss)
I0628 19:25:54.435731 32082 sgd_solver.cpp:137] Iteration 30600, lr = 0.00521875, m = 0.9
I0628 19:25:56.160006 32082 solver.cpp:349] Iteration 30700 (58.0125 iter/s, 1.72377s/100 iter), loss = 0.002698
I0628 19:25:56.160032 32082 solver.cpp:371]     Train net output #0: loss = 0.002698 (* 1 = 0.002698 loss)
I0628 19:25:56.160037 32082 sgd_solver.cpp:137] Iteration 30700, lr = 0.00520312, m = 0.9
I0628 19:25:57.883898 32082 solver.cpp:349] Iteration 30800 (58.0264 iter/s, 1.72335s/100 iter), loss = 0.00174224
I0628 19:25:57.883924 32082 solver.cpp:371]     Train net output #0: loss = 0.00174223 (* 1 = 0.00174223 loss)
I0628 19:25:57.883930 32082 sgd_solver.cpp:137] Iteration 30800, lr = 0.0051875, m = 0.9
I0628 19:25:59.608002 32082 solver.cpp:349] Iteration 30900 (58.0192 iter/s, 1.72357s/100 iter), loss = 0.0039794
I0628 19:25:59.608026 32082 solver.cpp:371]     Train net output #0: loss = 0.0039794 (* 1 = 0.0039794 loss)
I0628 19:25:59.608029 32082 sgd_solver.cpp:137] Iteration 30900, lr = 0.00517187, m = 0.9
I0628 19:26:01.311663 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:01.312723 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:01.312731 32082 net.cpp:2170] conv1a_param_0(0.31) 
I0628 19:26:01.312739 32082 net.cpp:2170] conv1b_param_0(0.62) 
I0628 19:26:01.312741 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:01.312744 32082 net.cpp:2170] res2a_branch2a_param_0(0.62) 
I0628 19:26:01.312747 32082 net.cpp:2170] res2a_branch2b_param_0(0.62) 
I0628 19:26:01.312748 32082 net.cpp:2170] res3a_branch2a_param_0(0.62) 
I0628 19:26:01.312750 32082 net.cpp:2170] res3a_branch2b_param_0(0.62) 
I0628 19:26:01.312752 32082 net.cpp:2170] res4a_branch2a_param_0(0.62) 
I0628 19:26:01.312754 32082 net.cpp:2170] res4a_branch2b_param_0(0.62) 
I0628 19:26:01.312757 32082 net.cpp:2170] res5a_branch2a_param_0(0.62) 
I0628 19:26:01.312758 32082 net.cpp:2170] res5a_branch2b_param_0(0.62) 
I0628 19:26:01.312760 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.3599e+06) 0.618
I0628 19:26:01.312768 32082 solver.cpp:545] Iteration 31000, Testing net (#0)
I0628 19:26:02.310564 32080 data_reader.cpp:262] Starting prefetch of epoch 31
I0628 19:26:02.332324 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9126
I0628 19:26:02.332347 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:26:02.332352 32082 solver.cpp:630]     Test net output #2: loss = 0.316316 (* 1 = 0.316316 loss)
I0628 19:26:02.332366 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0193s
I0628 19:26:02.349519 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.64
I0628 19:26:03.105046 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:26:03.106521 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:26:03.106880 32082 solver.cpp:349] Iteration 31000 (28.589 iter/s, 3.49785s/100 iter), loss = 0.00982035
I0628 19:26:03.106899 32082 solver.cpp:371]     Train net output #0: loss = 0.00982035 (* 1 = 0.00982035 loss)
I0628 19:26:03.106904 32082 sgd_solver.cpp:137] Iteration 31000, lr = 0.00515625, m = 0.9
I0628 19:26:04.825475 32082 solver.cpp:349] Iteration 31100 (58.2048 iter/s, 1.71807s/100 iter), loss = 0.002375
I0628 19:26:04.825568 32082 solver.cpp:371]     Train net output #0: loss = 0.002375 (* 1 = 0.002375 loss)
I0628 19:26:04.825577 32082 sgd_solver.cpp:137] Iteration 31100, lr = 0.00514062, m = 0.9
I0628 19:26:06.546826 32082 solver.cpp:349] Iteration 31200 (58.1143 iter/s, 1.72075s/100 iter), loss = 0.00279212
I0628 19:26:06.546849 32082 solver.cpp:371]     Train net output #0: loss = 0.00279212 (* 1 = 0.00279212 loss)
I0628 19:26:06.546852 32082 sgd_solver.cpp:137] Iteration 31200, lr = 0.005125, m = 0.9
I0628 19:26:07.341758 32050 data_reader.cpp:262] Starting prefetch of epoch 40
I0628 19:26:08.269062 32082 solver.cpp:349] Iteration 31300 (58.0818 iter/s, 1.72171s/100 iter), loss = 0.0036589
I0628 19:26:08.269089 32082 solver.cpp:371]     Train net output #0: loss = 0.0036589 (* 1 = 0.0036589 loss)
I0628 19:26:08.269095 32082 sgd_solver.cpp:137] Iteration 31300, lr = 0.00510937, m = 0.9
I0628 19:26:09.990872 32082 solver.cpp:349] Iteration 31400 (58.0963 iter/s, 1.72128s/100 iter), loss = 0.0049309
I0628 19:26:09.990895 32082 solver.cpp:371]     Train net output #0: loss = 0.0049309 (* 1 = 0.0049309 loss)
I0628 19:26:09.990898 32082 sgd_solver.cpp:137] Iteration 31400, lr = 0.00509375, m = 0.9
I0628 19:26:11.709177 32082 solver.cpp:349] Iteration 31500 (58.2146 iter/s, 1.71778s/100 iter), loss = 0.00273789
I0628 19:26:11.709203 32082 solver.cpp:371]     Train net output #0: loss = 0.00273789 (* 1 = 0.00273789 loss)
I0628 19:26:11.709208 32082 sgd_solver.cpp:137] Iteration 31500, lr = 0.00507812, m = 0.9
I0628 19:26:13.426754 32082 solver.cpp:349] Iteration 31600 (58.2394 iter/s, 1.71705s/100 iter), loss = 0.00570961
I0628 19:26:13.426781 32082 solver.cpp:371]     Train net output #0: loss = 0.00570961 (* 1 = 0.00570961 loss)
I0628 19:26:13.426787 32082 sgd_solver.cpp:137] Iteration 31600, lr = 0.0050625, m = 0.9
I0628 19:26:15.153353 32082 solver.cpp:349] Iteration 31700 (57.9351 iter/s, 1.72607s/100 iter), loss = 0.00224975
I0628 19:26:15.153375 32082 solver.cpp:371]     Train net output #0: loss = 0.00224975 (* 1 = 0.00224975 loss)
I0628 19:26:15.153379 32082 sgd_solver.cpp:137] Iteration 31700, lr = 0.00504687, m = 0.9
I0628 19:26:16.873749 32082 solver.cpp:349] Iteration 31800 (58.1437 iter/s, 1.71988s/100 iter), loss = 0.00276529
I0628 19:26:16.873778 32082 solver.cpp:371]     Train net output #0: loss = 0.00276529 (* 1 = 0.00276529 loss)
I0628 19:26:16.873785 32082 sgd_solver.cpp:137] Iteration 31800, lr = 0.00503125, m = 0.9
I0628 19:26:18.595824 32082 solver.cpp:349] Iteration 31900 (58.0873 iter/s, 1.72155s/100 iter), loss = 0.00211174
I0628 19:26:18.595846 32082 solver.cpp:371]     Train net output #0: loss = 0.00211174 (* 1 = 0.00211174 loss)
I0628 19:26:18.595850 32082 sgd_solver.cpp:137] Iteration 31900, lr = 0.00501562, m = 0.9
I0628 19:26:20.299163 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:20.300285 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:20.300293 32082 net.cpp:2170] conv1a_param_0(0.32) 
I0628 19:26:20.300300 32082 net.cpp:2170] conv1b_param_0(0.64) 
I0628 19:26:20.300303 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:20.300307 32082 net.cpp:2170] res2a_branch2a_param_0(0.64) 
I0628 19:26:20.300312 32082 net.cpp:2170] res2a_branch2b_param_0(0.64) 
I0628 19:26:20.300314 32082 net.cpp:2170] res3a_branch2a_param_0(0.64) 
I0628 19:26:20.300318 32082 net.cpp:2170] res3a_branch2b_param_0(0.64) 
I0628 19:26:20.300321 32082 net.cpp:2170] res4a_branch2a_param_0(0.64) 
I0628 19:26:20.300325 32082 net.cpp:2170] res4a_branch2b_param_0(0.64) 
I0628 19:26:20.300329 32082 net.cpp:2170] res5a_branch2a_param_0(0.64) 
I0628 19:26:20.300333 32082 net.cpp:2170] res5a_branch2b_param_0(0.64) 
I0628 19:26:20.300338 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.50628e+06/2.3599e+06) 0.638
I0628 19:26:20.300348 32082 solver.cpp:545] Iteration 32000, Testing net (#0)
I0628 19:26:21.301542 32080 data_reader.cpp:262] Starting prefetch of epoch 32
I0628 19:26:21.321763 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.911
I0628 19:26:21.321774 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:26:21.321789 32082 solver.cpp:630]     Test net output #2: loss = 0.323528 (* 1 = 0.323528 loss)
I0628 19:26:21.321802 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02117s
I0628 19:26:21.338984 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.66
I0628 19:26:22.107825 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:26:22.109290 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:26:22.109648 32082 solver.cpp:349] Iteration 32000 (28.4672 iter/s, 3.51281s/100 iter), loss = 0.00327806
I0628 19:26:22.109665 32082 solver.cpp:371]     Train net output #0: loss = 0.00327806 (* 1 = 0.00327806 loss)
I0628 19:26:22.109670 32082 sgd_solver.cpp:137] Iteration 32000, lr = 0.005, m = 0.9
I0628 19:26:22.592028 32050 data_reader.cpp:262] Starting prefetch of epoch 41
I0628 19:26:23.829520 32082 solver.cpp:349] Iteration 32100 (58.1611 iter/s, 1.71936s/100 iter), loss = 0.0045468
I0628 19:26:23.829542 32082 solver.cpp:371]     Train net output #0: loss = 0.0045468 (* 1 = 0.0045468 loss)
I0628 19:26:23.829550 32082 sgd_solver.cpp:137] Iteration 32100, lr = 0.00498438, m = 0.9
I0628 19:26:25.552453 32082 solver.cpp:349] Iteration 32200 (58.0581 iter/s, 1.72241s/100 iter), loss = 0.0082751
I0628 19:26:25.552479 32082 solver.cpp:371]     Train net output #0: loss = 0.0082751 (* 1 = 0.0082751 loss)
I0628 19:26:25.552485 32082 sgd_solver.cpp:137] Iteration 32200, lr = 0.00496875, m = 0.9
I0628 19:26:27.276376 32082 solver.cpp:349] Iteration 32300 (58.0248 iter/s, 1.7234s/100 iter), loss = 0.0175805
I0628 19:26:27.276402 32082 solver.cpp:371]     Train net output #0: loss = 0.0175805 (* 1 = 0.0175805 loss)
I0628 19:26:27.276408 32082 sgd_solver.cpp:137] Iteration 32300, lr = 0.00495313, m = 0.9
I0628 19:26:28.996325 32082 solver.cpp:349] Iteration 32400 (58.159 iter/s, 1.71943s/100 iter), loss = 0.00649336
I0628 19:26:28.996350 32082 solver.cpp:371]     Train net output #0: loss = 0.00649336 (* 1 = 0.00649336 loss)
I0628 19:26:28.996356 32082 sgd_solver.cpp:137] Iteration 32400, lr = 0.0049375, m = 0.9
I0628 19:26:30.712353 32082 solver.cpp:349] Iteration 32500 (58.2917 iter/s, 1.71551s/100 iter), loss = 0.00696994
I0628 19:26:30.712379 32082 solver.cpp:371]     Train net output #0: loss = 0.00696995 (* 1 = 0.00696995 loss)
I0628 19:26:30.712385 32082 sgd_solver.cpp:137] Iteration 32500, lr = 0.00492187, m = 0.9
I0628 19:26:32.431192 32082 solver.cpp:349] Iteration 32600 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00337892
I0628 19:26:32.431218 32082 solver.cpp:371]     Train net output #0: loss = 0.00337893 (* 1 = 0.00337893 loss)
I0628 19:26:32.431224 32082 sgd_solver.cpp:137] Iteration 32600, lr = 0.00490625, m = 0.9
I0628 19:26:34.148564 32082 solver.cpp:349] Iteration 32700 (58.2461 iter/s, 1.71685s/100 iter), loss = 0.00794096
I0628 19:26:34.148591 32082 solver.cpp:371]     Train net output #0: loss = 0.00794097 (* 1 = 0.00794097 loss)
I0628 19:26:34.148597 32082 sgd_solver.cpp:137] Iteration 32700, lr = 0.00489062, m = 0.9
I0628 19:26:35.864140 32082 solver.cpp:349] Iteration 32800 (58.307 iter/s, 1.71506s/100 iter), loss = 0.00529499
I0628 19:26:35.864225 32082 solver.cpp:371]     Train net output #0: loss = 0.005295 (* 1 = 0.005295 loss)
I0628 19:26:35.864233 32082 sgd_solver.cpp:137] Iteration 32800, lr = 0.004875, m = 0.9
I0628 19:26:36.019273 32050 data_reader.cpp:262] Starting prefetch of epoch 42
I0628 19:26:37.588546 32082 solver.cpp:349] Iteration 32900 (58.0105 iter/s, 1.72383s/100 iter), loss = 0.00389099
I0628 19:26:37.588569 32082 solver.cpp:371]     Train net output #0: loss = 0.00389099 (* 1 = 0.00389099 loss)
I0628 19:26:37.588573 32082 sgd_solver.cpp:137] Iteration 32900, lr = 0.00485937, m = 0.9
I0628 19:26:39.287981 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:39.289034 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:39.289043 32082 net.cpp:2170] conv1a_param_0(0.33) 
I0628 19:26:39.289052 32082 net.cpp:2170] conv1b_param_0(0.66) 
I0628 19:26:39.289057 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:39.289060 32082 net.cpp:2170] res2a_branch2a_param_0(0.66) 
I0628 19:26:39.289064 32082 net.cpp:2170] res2a_branch2b_param_0(0.647) 
I0628 19:26:39.289068 32082 net.cpp:2170] res3a_branch2a_param_0(0.66) 
I0628 19:26:39.289072 32082 net.cpp:2170] res3a_branch2b_param_0(0.66) 
I0628 19:26:39.289077 32082 net.cpp:2170] res4a_branch2a_param_0(0.66) 
I0628 19:26:39.289080 32082 net.cpp:2170] res4a_branch2b_param_0(0.66) 
I0628 19:26:39.289083 32082 net.cpp:2170] res5a_branch2a_param_0(0.66) 
I0628 19:26:39.289088 32082 net.cpp:2170] res5a_branch2b_param_0(0.66) 
I0628 19:26:39.289091 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.55324e+06/2.3599e+06) 0.658
I0628 19:26:39.289103 32082 solver.cpp:545] Iteration 33000, Testing net (#0)
I0628 19:26:40.284121 32080 data_reader.cpp:262] Starting prefetch of epoch 33
I0628 19:26:40.307348 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.908
I0628 19:26:40.307364 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:26:40.307371 32082 solver.cpp:630]     Test net output #2: loss = 0.329499 (* 1 = 0.329499 loss)
I0628 19:26:40.307389 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.018s
I0628 19:26:40.324646 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.68
I0628 19:26:41.133483 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:26:41.134955 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:26:41.135313 32082 solver.cpp:349] Iteration 33000 (28.2027 iter/s, 3.54576s/100 iter), loss = 0.00446094
I0628 19:26:41.135332 32082 solver.cpp:371]     Train net output #0: loss = 0.00446095 (* 1 = 0.00446095 loss)
I0628 19:26:41.135340 32082 sgd_solver.cpp:137] Iteration 33000, lr = 0.00484375, m = 0.9
I0628 19:26:42.854140 32082 solver.cpp:349] Iteration 33100 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00299629
I0628 19:26:42.854162 32082 solver.cpp:371]     Train net output #0: loss = 0.0029963 (* 1 = 0.0029963 loss)
I0628 19:26:42.854166 32082 sgd_solver.cpp:137] Iteration 33100, lr = 0.00482813, m = 0.9
I0628 19:26:44.571943 32082 solver.cpp:349] Iteration 33200 (58.231 iter/s, 1.7173s/100 iter), loss = 0.00456729
I0628 19:26:44.571990 32082 solver.cpp:371]     Train net output #0: loss = 0.0045673 (* 1 = 0.0045673 loss)
I0628 19:26:44.571997 32082 sgd_solver.cpp:137] Iteration 33200, lr = 0.0048125, m = 0.9
I0628 19:26:46.293244 32082 solver.cpp:349] Iteration 33300 (58.1137 iter/s, 1.72076s/100 iter), loss = 0.00215493
I0628 19:26:46.293269 32082 solver.cpp:371]     Train net output #0: loss = 0.00215493 (* 1 = 0.00215493 loss)
I0628 19:26:46.293275 32082 sgd_solver.cpp:137] Iteration 33300, lr = 0.00479688, m = 0.9
I0628 19:26:48.011871 32082 solver.cpp:349] Iteration 33400 (58.2032 iter/s, 1.71812s/100 iter), loss = 0.00868004
I0628 19:26:48.011893 32082 solver.cpp:371]     Train net output #0: loss = 0.00868005 (* 1 = 0.00868005 loss)
I0628 19:26:48.011898 32082 sgd_solver.cpp:137] Iteration 33400, lr = 0.00478125, m = 0.9
I0628 19:26:49.732007 32082 solver.cpp:349] Iteration 33500 (58.1519 iter/s, 1.71963s/100 iter), loss = 0.0053904
I0628 19:26:49.732044 32082 solver.cpp:371]     Train net output #0: loss = 0.00539041 (* 1 = 0.00539041 loss)
I0628 19:26:49.732050 32082 sgd_solver.cpp:137] Iteration 33500, lr = 0.00476563, m = 0.9
I0628 19:26:51.276684 32050 data_reader.cpp:262] Starting prefetch of epoch 43
I0628 19:26:51.448038 32082 solver.cpp:349] Iteration 33600 (58.2916 iter/s, 1.71551s/100 iter), loss = 0.00551467
I0628 19:26:51.448062 32082 solver.cpp:371]     Train net output #0: loss = 0.00551468 (* 1 = 0.00551468 loss)
I0628 19:26:51.448066 32082 sgd_solver.cpp:137] Iteration 33600, lr = 0.00475, m = 0.9
I0628 19:26:53.169694 32082 solver.cpp:349] Iteration 33700 (58.1006 iter/s, 1.72115s/100 iter), loss = 0.0162314
I0628 19:26:53.169719 32082 solver.cpp:371]     Train net output #0: loss = 0.0162314 (* 1 = 0.0162314 loss)
I0628 19:26:53.169725 32082 sgd_solver.cpp:137] Iteration 33700, lr = 0.00473437, m = 0.9
I0628 19:26:54.887886 32082 solver.cpp:349] Iteration 33800 (58.2179 iter/s, 1.71768s/100 iter), loss = 0.00340678
I0628 19:26:54.887912 32082 solver.cpp:371]     Train net output #0: loss = 0.00340679 (* 1 = 0.00340679 loss)
I0628 19:26:54.887917 32082 sgd_solver.cpp:137] Iteration 33800, lr = 0.00471875, m = 0.9
I0628 19:26:56.610631 32082 solver.cpp:349] Iteration 33900 (58.064 iter/s, 1.72224s/100 iter), loss = 0.00347558
I0628 19:26:56.610652 32082 solver.cpp:371]     Train net output #0: loss = 0.00347559 (* 1 = 0.00347559 loss)
I0628 19:26:56.610657 32082 sgd_solver.cpp:137] Iteration 33900, lr = 0.00470312, m = 0.9
I0628 19:26:58.313221 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:58.314347 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:58.314354 32082 net.cpp:2170] conv1a_param_0(0.34) 
I0628 19:26:58.314362 32082 net.cpp:2170] conv1b_param_0(0.68) 
I0628 19:26:58.314363 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:58.314366 32082 net.cpp:2170] res2a_branch2a_param_0(0.68) 
I0628 19:26:58.314368 32082 net.cpp:2170] res2a_branch2b_param_0(0.652) 
I0628 19:26:58.314371 32082 net.cpp:2170] res3a_branch2a_param_0(0.68) 
I0628 19:26:58.314373 32082 net.cpp:2170] res3a_branch2b_param_0(0.68) 
I0628 19:26:58.314374 32082 net.cpp:2170] res4a_branch2a_param_0(0.68) 
I0628 19:26:58.314378 32082 net.cpp:2170] res4a_branch2b_param_0(0.68) 
I0628 19:26:58.314379 32082 net.cpp:2170] res5a_branch2a_param_0(0.68) 
I0628 19:26:58.314381 32082 net.cpp:2170] res5a_branch2b_param_0(0.68) 
I0628 19:26:58.314383 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.60018e+06/2.3599e+06) 0.678
I0628 19:26:58.314390 32082 solver.cpp:545] Iteration 34000, Testing net (#0)
I0628 19:26:59.313334 32080 data_reader.cpp:262] Starting prefetch of epoch 34
I0628 19:26:59.333452 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9088
I0628 19:26:59.333464 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:26:59.333469 32082 solver.cpp:630]     Test net output #2: loss = 0.329585 (* 1 = 0.329585 loss)
I0628 19:26:59.333482 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01882s
I0628 19:26:59.350714 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.7
I0628 19:27:00.167042 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:00.168521 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:00.168893 32082 solver.cpp:349] Iteration 34000 (28.1114 iter/s, 3.55728s/100 iter), loss = 0.00725057
I0628 19:27:00.168911 32082 solver.cpp:371]     Train net output #0: loss = 0.00725057 (* 1 = 0.00725057 loss)
I0628 19:27:00.168917 32082 sgd_solver.cpp:137] Iteration 34000, lr = 0.0046875, m = 0.9
I0628 19:27:01.888778 32082 solver.cpp:349] Iteration 34100 (58.1602 iter/s, 1.71939s/100 iter), loss = 0.0120626
I0628 19:27:01.888801 32082 solver.cpp:371]     Train net output #0: loss = 0.0120626 (* 1 = 0.0120626 loss)
I0628 19:27:01.888808 32082 sgd_solver.cpp:137] Iteration 34100, lr = 0.00467187, m = 0.9
I0628 19:27:03.607036 32082 solver.cpp:349] Iteration 34200 (58.2154 iter/s, 1.71776s/100 iter), loss = 0.00361765
I0628 19:27:03.607075 32082 solver.cpp:371]     Train net output #0: loss = 0.00361765 (* 1 = 0.00361765 loss)
I0628 19:27:03.607080 32082 sgd_solver.cpp:137] Iteration 34200, lr = 0.00465625, m = 0.9
I0628 19:27:05.329390 32082 solver.cpp:349] Iteration 34300 (58.0775 iter/s, 1.72184s/100 iter), loss = 0.00612782
I0628 19:27:05.329416 32082 solver.cpp:371]     Train net output #0: loss = 0.00612782 (* 1 = 0.00612782 loss)
I0628 19:27:05.329421 32082 sgd_solver.cpp:137] Iteration 34300, lr = 0.00464062, m = 0.9
I0628 19:27:06.550937 32050 data_reader.cpp:262] Starting prefetch of epoch 44
I0628 19:27:07.051458 32082 solver.cpp:349] Iteration 34400 (58.0866 iter/s, 1.72157s/100 iter), loss = 0.00793967
I0628 19:27:07.051481 32082 solver.cpp:371]     Train net output #0: loss = 0.00793967 (* 1 = 0.00793967 loss)
I0628 19:27:07.051487 32082 sgd_solver.cpp:137] Iteration 34400, lr = 0.004625, m = 0.9
I0628 19:27:08.781559 32082 solver.cpp:349] Iteration 34500 (57.8169 iter/s, 1.7296s/100 iter), loss = 0.004718
I0628 19:27:08.781584 32082 solver.cpp:371]     Train net output #0: loss = 0.004718 (* 1 = 0.004718 loss)
I0628 19:27:08.781589 32082 sgd_solver.cpp:137] Iteration 34500, lr = 0.00460937, m = 0.9
I0628 19:27:10.501622 32082 solver.cpp:349] Iteration 34600 (58.1543 iter/s, 1.71956s/100 iter), loss = 0.0100171
I0628 19:27:10.501648 32082 solver.cpp:371]     Train net output #0: loss = 0.0100171 (* 1 = 0.0100171 loss)
I0628 19:27:10.501654 32082 sgd_solver.cpp:137] Iteration 34600, lr = 0.00459375, m = 0.9
I0628 19:27:12.221623 32082 solver.cpp:349] Iteration 34700 (58.1565 iter/s, 1.7195s/100 iter), loss = 0.0059734
I0628 19:27:12.221650 32082 solver.cpp:371]     Train net output #0: loss = 0.0059734 (* 1 = 0.0059734 loss)
I0628 19:27:12.221657 32082 sgd_solver.cpp:137] Iteration 34700, lr = 0.00457812, m = 0.9
I0628 19:27:13.945973 32082 solver.cpp:349] Iteration 34800 (58.0099 iter/s, 1.72385s/100 iter), loss = 0.00990715
I0628 19:27:13.946028 32082 solver.cpp:371]     Train net output #0: loss = 0.00990715 (* 1 = 0.00990715 loss)
I0628 19:27:13.946045 32082 sgd_solver.cpp:137] Iteration 34800, lr = 0.0045625, m = 0.9
I0628 19:27:15.666646 32082 solver.cpp:349] Iteration 34900 (58.1353 iter/s, 1.72013s/100 iter), loss = 0.0101621
I0628 19:27:15.666672 32082 solver.cpp:371]     Train net output #0: loss = 0.0101621 (* 1 = 0.0101621 loss)
I0628 19:27:15.666678 32082 sgd_solver.cpp:137] Iteration 34900, lr = 0.00454687, m = 0.9
I0628 19:27:17.368623 32082 solver.cpp:401] Sparsity after update:
I0628 19:27:17.369701 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:27:17.369709 32082 net.cpp:2170] conv1a_param_0(0.35) 
I0628 19:27:17.369715 32082 net.cpp:2170] conv1b_param_0(0.7) 
I0628 19:27:17.369717 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:27:17.369720 32082 net.cpp:2170] res2a_branch2a_param_0(0.7) 
I0628 19:27:17.369725 32082 net.cpp:2170] res2a_branch2b_param_0(0.654) 
I0628 19:27:17.369727 32082 net.cpp:2170] res3a_branch2a_param_0(0.7) 
I0628 19:27:17.369729 32082 net.cpp:2170] res3a_branch2b_param_0(0.686) 
I0628 19:27:17.369732 32082 net.cpp:2170] res4a_branch2a_param_0(0.7) 
I0628 19:27:17.369735 32082 net.cpp:2170] res4a_branch2b_param_0(0.7) 
I0628 19:27:17.369737 32082 net.cpp:2170] res5a_branch2a_param_0(0.7) 
I0628 19:27:17.369740 32082 net.cpp:2170] res5a_branch2b_param_0(0.7) 
I0628 19:27:17.369741 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.64658e+06/2.3599e+06) 0.698
I0628 19:27:17.369748 32082 solver.cpp:545] Iteration 35000, Testing net (#0)
I0628 19:27:18.367383 32080 data_reader.cpp:262] Starting prefetch of epoch 35
I0628 19:27:18.389145 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9094
I0628 19:27:18.389159 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:27:18.389163 32082 solver.cpp:630]     Test net output #2: loss = 0.326457 (* 1 = 0.326457 loss)
I0628 19:27:18.389179 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01916s
I0628 19:27:18.406579 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.72
I0628 19:27:19.252629 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:19.254091 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:19.254451 32082 solver.cpp:349] Iteration 35000 (27.8799 iter/s, 3.58682s/100 iter), loss = 0.00427119
I0628 19:27:19.254468 32082 solver.cpp:371]     Train net output #0: loss = 0.00427119 (* 1 = 0.00427119 loss)
I0628 19:27:19.254477 32082 sgd_solver.cpp:137] Iteration 35000, lr = 0.00453125, m = 0.9
I0628 19:27:20.975774 32082 solver.cpp:349] Iteration 35100 (58.1114 iter/s, 1.72083s/100 iter), loss = 0.0176839
I0628 19:27:20.975808 32082 solver.cpp:371]     Train net output #0: loss = 0.0176839 (* 1 = 0.0176839 loss)
I0628 19:27:20.975812 32082 sgd_solver.cpp:137] Iteration 35100, lr = 0.00451563, m = 0.9
I0628 19:27:21.887156 32050 data_reader.cpp:262] Starting prefetch of epoch 45
I0628 19:27:22.697123 32082 solver.cpp:349] Iteration 35200 (58.1109 iter/s, 1.72085s/100 iter), loss = 0.00321784
I0628 19:27:22.697149 32082 solver.cpp:371]     Train net output #0: loss = 0.00321784 (* 1 = 0.00321784 loss)
I0628 19:27:22.697154 32082 sgd_solver.cpp:137] Iteration 35200, lr = 0.0045, m = 0.9
I0628 19:27:24.416890 32082 solver.cpp:349] Iteration 35300 (58.1642 iter/s, 1.71927s/100 iter), loss = 0.00864619
I0628 19:27:24.416916 32082 solver.cpp:371]     Train net output #0: loss = 0.00864619 (* 1 = 0.00864619 loss)
I0628 19:27:24.416923 32082 sgd_solver.cpp:137] Iteration 35300, lr = 0.00448438, m = 0.9
I0628 19:27:26.137315 32082 solver.cpp:349] Iteration 35400 (58.1419 iter/s, 1.71993s/100 iter), loss = 0.00812749
I0628 19:27:26.137338 32082 solver.cpp:371]     Train net output #0: loss = 0.00812748 (* 1 = 0.00812748 loss)
I0628 19:27:26.137344 32082 sgd_solver.cpp:137] Iteration 35400, lr = 0.00446875, m = 0.9
I0628 19:27:27.864836 32082 solver.cpp:349] Iteration 35500 (57.9029 iter/s, 1.72703s/100 iter), loss = 0.00765098
I0628 19:27:27.864863 32082 solver.cpp:371]     Train net output #0: loss = 0.00765098 (* 1 = 0.00765098 loss)
I0628 19:27:27.864871 32082 sgd_solver.cpp:137] Iteration 35500, lr = 0.00445312, m = 0.9
I0628 19:27:29.586872 32082 solver.cpp:349] Iteration 35600 (58.0876 iter/s, 1.72154s/100 iter), loss = 0.00669238
I0628 19:27:29.586899 32082 solver.cpp:371]     Train net output #0: loss = 0.00669238 (* 1 = 0.00669238 loss)
I0628 19:27:29.586905 32082 sgd_solver.cpp:137] Iteration 35600, lr = 0.0044375, m = 0.9
I0628 19:27:31.305361 32082 solver.cpp:349] Iteration 35700 (58.2073 iter/s, 1.718s/100 iter), loss = 0.00395535
I0628 19:27:31.305388 32082 solver.cpp:371]     Train net output #0: loss = 0.00395534 (* 1 = 0.00395534 loss)
I0628 19:27:31.305394 32082 sgd_solver.cpp:137] Iteration 35700, lr = 0.00442187, m = 0.9
I0628 19:27:33.022294 32082 solver.cpp:349] Iteration 35800 (58.2601 iter/s, 1.71644s/100 iter), loss = 0.00449342
I0628 19:27:33.022316 32082 solver.cpp:371]     Train net output #0: loss = 0.00449342 (* 1 = 0.00449342 loss)
I0628 19:27:33.022322 32082 sgd_solver.cpp:137] Iteration 35800, lr = 0.00440625, m = 0.9
I0628 19:27:34.742115 32082 solver.cpp:349] Iteration 35900 (58.1621 iter/s, 1.71933s/100 iter), loss = 0.00591834
I0628 19:27:34.742141 32082 solver.cpp:371]     Train net output #0: loss = 0.00591834 (* 1 = 0.00591834 loss)
I0628 19:27:34.742146 32082 sgd_solver.cpp:137] Iteration 35900, lr = 0.00439062, m = 0.9
I0628 19:27:35.327350 32050 data_reader.cpp:262] Starting prefetch of epoch 46
I0628 19:27:36.443222 32082 solver.cpp:401] Sparsity after update:
I0628 19:27:36.444237 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:27:36.444244 32082 net.cpp:2170] conv1a_param_0(0.36) 
I0628 19:27:36.444252 32082 net.cpp:2170] conv1b_param_0(0.72) 
I0628 19:27:36.444253 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:27:36.444257 32082 net.cpp:2170] res2a_branch2a_param_0(0.72) 
I0628 19:27:36.444258 32082 net.cpp:2170] res2a_branch2b_param_0(0.655) 
I0628 19:27:36.444262 32082 net.cpp:2170] res3a_branch2a_param_0(0.72) 
I0628 19:27:36.444263 32082 net.cpp:2170] res3a_branch2b_param_0(0.689) 
I0628 19:27:36.444265 32082 net.cpp:2170] res4a_branch2a_param_0(0.72) 
I0628 19:27:36.444267 32082 net.cpp:2170] res4a_branch2b_param_0(0.72) 
I0628 19:27:36.444269 32082 net.cpp:2170] res5a_branch2a_param_0(0.72) 
I0628 19:27:36.444272 32082 net.cpp:2170] res5a_branch2b_param_0(0.72) 
I0628 19:27:36.444274 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.69284e+06/2.3599e+06) 0.717
I0628 19:27:36.444281 32082 solver.cpp:545] Iteration 36000, Testing net (#0)
I0628 19:27:37.438995 32080 data_reader.cpp:262] Starting prefetch of epoch 36
I0628 19:27:37.462795 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9078
I0628 19:27:37.462806 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:27:37.462811 32082 solver.cpp:630]     Test net output #2: loss = 0.335488 (* 1 = 0.335488 loss)
I0628 19:27:37.462823 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01828s
I0628 19:27:37.480068 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.74
I0628 19:27:38.353996 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:38.355464 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:38.355820 32082 solver.cpp:349] Iteration 36000 (27.6799 iter/s, 3.61273s/100 iter), loss = 0.0107648
I0628 19:27:38.355839 32082 solver.cpp:371]     Train net output #0: loss = 0.0107648 (* 1 = 0.0107648 loss)
I0628 19:27:38.355844 32082 sgd_solver.cpp:137] Iteration 36000, lr = 0.004375, m = 0.9
I0628 19:27:40.082172 32082 solver.cpp:349] Iteration 36100 (57.9417 iter/s, 1.72587s/100 iter), loss = 0.0116852
I0628 19:27:40.082193 32082 solver.cpp:371]     Train net output #0: loss = 0.0116852 (* 1 = 0.0116852 loss)
I0628 19:27:40.082197 32082 sgd_solver.cpp:137] Iteration 36100, lr = 0.00435938, m = 0.9
I0628 19:27:41.798470 32082 solver.cpp:349] Iteration 36200 (58.2813 iter/s, 1.71582s/100 iter), loss = 0.00544455
I0628 19:27:41.798496 32082 solver.cpp:371]     Train net output #0: loss = 0.00544455 (* 1 = 0.00544455 loss)
I0628 19:27:41.798501 32082 sgd_solver.cpp:137] Iteration 36200, lr = 0.00434375, m = 0.9
I0628 19:27:43.518506 32082 solver.cpp:349] Iteration 36300 (58.1548 iter/s, 1.71955s/100 iter), loss = 0.00510266
I0628 19:27:43.518527 32082 solver.cpp:371]     Train net output #0: loss = 0.00510266 (* 1 = 0.00510266 loss)
I0628 19:27:43.518532 32082 sgd_solver.cpp:137] Iteration 36300, lr = 0.00432813, m = 0.9
I0628 19:27:45.239980 32082 solver.cpp:349] Iteration 36400 (58.106 iter/s, 1.72099s/100 iter), loss = 0.00581676
I0628 19:27:45.240006 32082 solver.cpp:371]     Train net output #0: loss = 0.00581676 (* 1 = 0.00581676 loss)
I0628 19:27:45.240012 32082 sgd_solver.cpp:137] Iteration 36400, lr = 0.0043125, m = 0.9
I0628 19:27:46.961422 32082 solver.cpp:349] Iteration 36500 (58.1073 iter/s, 1.72095s/100 iter), loss = 0.00692566
I0628 19:27:46.961448 32082 solver.cpp:371]     Train net output #0: loss = 0.00692566 (* 1 = 0.00692566 loss)
I0628 19:27:46.961454 32082 sgd_solver.cpp:137] Iteration 36500, lr = 0.00429688, m = 0.9
I0628 19:27:48.683187 32082 solver.cpp:349] Iteration 36600 (58.0964 iter/s, 1.72128s/100 iter), loss = 0.00646818
I0628 19:27:48.683212 32082 solver.cpp:371]     Train net output #0: loss = 0.00646819 (* 1 = 0.00646819 loss)
I0628 19:27:48.683218 32082 sgd_solver.cpp:137] Iteration 36600, lr = 0.00428125, m = 0.9
I0628 19:27:50.400107 32082 solver.cpp:349] Iteration 36700 (58.2603 iter/s, 1.71644s/100 iter), loss = 0.00795696
I0628 19:27:50.400135 32082 solver.cpp:371]     Train net output #0: loss = 0.00795696 (* 1 = 0.00795696 loss)
I0628 19:27:50.400141 32082 sgd_solver.cpp:137] Iteration 36700, lr = 0.00426562, m = 0.9
I0628 19:27:50.658504 32050 data_reader.cpp:262] Starting prefetch of epoch 47
I0628 19:27:52.128213 32082 solver.cpp:349] Iteration 36800 (57.8832 iter/s, 1.72762s/100 iter), loss = 0.00791638
I0628 19:27:52.128240 32082 solver.cpp:371]     Train net output #0: loss = 0.00791638 (* 1 = 0.00791638 loss)
I0628 19:27:52.128247 32082 sgd_solver.cpp:137] Iteration 36800, lr = 0.00425, m = 0.9
I0628 19:27:53.850055 32082 solver.cpp:349] Iteration 36900 (58.0938 iter/s, 1.72136s/100 iter), loss = 0.00439089
I0628 19:27:53.850082 32082 solver.cpp:371]     Train net output #0: loss = 0.00439089 (* 1 = 0.00439089 loss)
I0628 19:27:53.850088 32082 sgd_solver.cpp:137] Iteration 36900, lr = 0.00423437, m = 0.9
I0628 19:27:55.551638 32082 solver.cpp:401] Sparsity after update:
I0628 19:27:55.552758 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:27:55.552767 32082 net.cpp:2170] conv1a_param_0(0.37) 
I0628 19:27:55.552788 32082 net.cpp:2170] conv1b_param_0(0.74) 
I0628 19:27:55.552791 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:27:55.552794 32082 net.cpp:2170] res2a_branch2a_param_0(0.74) 
I0628 19:27:55.552799 32082 net.cpp:2170] res2a_branch2b_param_0(0.659) 
I0628 19:27:55.552803 32082 net.cpp:2170] res3a_branch2a_param_0(0.727) 
I0628 19:27:55.552809 32082 net.cpp:2170] res3a_branch2b_param_0(0.692) 
I0628 19:27:55.552812 32082 net.cpp:2170] res4a_branch2a_param_0(0.74) 
I0628 19:27:55.552815 32082 net.cpp:2170] res4a_branch2b_param_0(0.74) 
I0628 19:27:55.552820 32082 net.cpp:2170] res5a_branch2a_param_0(0.74) 
I0628 19:27:55.552824 32082 net.cpp:2170] res5a_branch2b_param_0(0.74) 
I0628 19:27:55.552826 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.73816e+06/2.3599e+06) 0.737
I0628 19:27:55.552840 32082 solver.cpp:545] Iteration 37000, Testing net (#0)
I0628 19:27:56.547806 32080 data_reader.cpp:262] Starting prefetch of epoch 37
I0628 19:27:56.571777 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9058
I0628 19:27:56.571790 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:27:56.571795 32082 solver.cpp:630]     Test net output #2: loss = 0.337024 (* 1 = 0.337024 loss)
I0628 19:27:56.571808 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01871s
I0628 19:27:56.589052 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.76
I0628 19:27:57.483656 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:57.485136 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:57.485492 32082 solver.cpp:349] Iteration 37000 (27.5143 iter/s, 3.63447s/100 iter), loss = 0.0100698
I0628 19:27:57.485512 32082 solver.cpp:371]     Train net output #0: loss = 0.0100698 (* 1 = 0.0100698 loss)
I0628 19:27:57.485518 32082 sgd_solver.cpp:137] Iteration 37000, lr = 0.00421875, m = 0.9
I0628 19:27:59.204883 32082 solver.cpp:349] Iteration 37100 (58.1763 iter/s, 1.71891s/100 iter), loss = 0.0133468
I0628 19:27:59.204902 32082 solver.cpp:371]     Train net output #0: loss = 0.0133468 (* 1 = 0.0133468 loss)
I0628 19:27:59.204906 32082 sgd_solver.cpp:137] Iteration 37100, lr = 0.00420313, m = 0.9
I0628 19:28:00.925164 32082 solver.cpp:349] Iteration 37200 (58.1459 iter/s, 1.71981s/100 iter), loss = 0.0173099
I0628 19:28:00.925186 32082 solver.cpp:371]     Train net output #0: loss = 0.0173099 (* 1 = 0.0173099 loss)
I0628 19:28:00.925190 32082 sgd_solver.cpp:137] Iteration 37200, lr = 0.0041875, m = 0.9
I0628 19:28:02.646306 32082 solver.cpp:349] Iteration 37300 (58.117 iter/s, 1.72067s/100 iter), loss = 0.0309862
I0628 19:28:02.646327 32082 solver.cpp:371]     Train net output #0: loss = 0.0309862 (* 1 = 0.0309862 loss)
I0628 19:28:02.646330 32082 sgd_solver.cpp:137] Iteration 37300, lr = 0.00417187, m = 0.9
I0628 19:28:04.362717 32082 solver.cpp:349] Iteration 37400 (58.277 iter/s, 1.71594s/100 iter), loss = 0.00203535
I0628 19:28:04.362742 32082 solver.cpp:371]     Train net output #0: loss = 0.00203535 (* 1 = 0.00203535 loss)
I0628 19:28:04.362747 32082 sgd_solver.cpp:137] Iteration 37400, lr = 0.00415625, m = 0.9
I0628 19:28:06.012223 32050 data_reader.cpp:262] Starting prefetch of epoch 48
I0628 19:28:06.080940 32082 solver.cpp:349] Iteration 37500 (58.2158 iter/s, 1.71775s/100 iter), loss = 0.00507345
I0628 19:28:06.080967 32082 solver.cpp:371]     Train net output #0: loss = 0.00507345 (* 1 = 0.00507345 loss)
I0628 19:28:06.080973 32082 sgd_solver.cpp:137] Iteration 37500, lr = 0.00414062, m = 0.9
I0628 19:28:07.800045 32082 solver.cpp:349] Iteration 37600 (58.1861 iter/s, 1.71862s/100 iter), loss = 0.0239773
I0628 19:28:07.800114 32082 solver.cpp:371]     Train net output #0: loss = 0.0239773 (* 1 = 0.0239773 loss)
I0628 19:28:07.800122 32082 sgd_solver.cpp:137] Iteration 37600, lr = 0.004125, m = 0.9
I0628 19:28:09.521589 32082 solver.cpp:349] Iteration 37700 (58.1051 iter/s, 1.72102s/100 iter), loss = 0.0157856
I0628 19:28:09.521613 32082 solver.cpp:371]     Train net output #0: loss = 0.0157856 (* 1 = 0.0157856 loss)
I0628 19:28:09.521620 32082 sgd_solver.cpp:137] Iteration 37700, lr = 0.00410937, m = 0.9
I0628 19:28:11.242199 32082 solver.cpp:349] Iteration 37800 (58.135 iter/s, 1.72013s/100 iter), loss = 0.0107801
I0628 19:28:11.242221 32082 solver.cpp:371]     Train net output #0: loss = 0.0107801 (* 1 = 0.0107801 loss)
I0628 19:28:11.242224 32082 sgd_solver.cpp:137] Iteration 37800, lr = 0.00409375, m = 0.9
I0628 19:28:12.963493 32082 solver.cpp:349] Iteration 37900 (58.1117 iter/s, 1.72082s/100 iter), loss = 0.0176397
I0628 19:28:12.963520 32082 solver.cpp:371]     Train net output #0: loss = 0.0176397 (* 1 = 0.0176397 loss)
I0628 19:28:12.963524 32082 sgd_solver.cpp:137] Iteration 37900, lr = 0.00407812, m = 0.9
I0628 19:28:14.663517 32082 solver.cpp:401] Sparsity after update:
I0628 19:28:14.664553 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:28:14.664561 32082 net.cpp:2170] conv1a_param_0(0.38) 
I0628 19:28:14.664567 32082 net.cpp:2170] conv1b_param_0(0.76) 
I0628 19:28:14.664571 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:28:14.664573 32082 net.cpp:2170] res2a_branch2a_param_0(0.76) 
I0628 19:28:14.664575 32082 net.cpp:2170] res2a_branch2b_param_0(0.66) 
I0628 19:28:14.664578 32082 net.cpp:2170] res3a_branch2a_param_0(0.732) 
I0628 19:28:14.664580 32082 net.cpp:2170] res3a_branch2b_param_0(0.696) 
I0628 19:28:14.664582 32082 net.cpp:2170] res4a_branch2a_param_0(0.76) 
I0628 19:28:14.664584 32082 net.cpp:2170] res4a_branch2b_param_0(0.76) 
I0628 19:28:14.664587 32082 net.cpp:2170] res5a_branch2a_param_0(0.76) 
I0628 19:28:14.664589 32082 net.cpp:2170] res5a_branch2b_param_0(0.76) 
I0628 19:28:14.664592 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.78336e+06/2.3599e+06) 0.756
I0628 19:28:14.664598 32082 solver.cpp:545] Iteration 38000, Testing net (#0)
I0628 19:28:15.659682 32080 data_reader.cpp:262] Starting prefetch of epoch 38
I0628 19:28:15.685371 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9088
I0628 19:28:15.685384 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:28:15.685389 32082 solver.cpp:630]     Test net output #2: loss = 0.325561 (* 1 = 0.325561 loss)
I0628 19:28:15.685403 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02054s
I0628 19:28:15.702728 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.78
I0628 19:28:16.614776 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:28:16.616246 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:28:16.616603 32082 solver.cpp:349] Iteration 38000 (27.3811 iter/s, 3.65215s/100 iter), loss = 0.0085587
I0628 19:28:16.616622 32082 solver.cpp:371]     Train net output #0: loss = 0.00855871 (* 1 = 0.00855871 loss)
I0628 19:28:16.616627 32082 sgd_solver.cpp:137] Iteration 38000, lr = 0.0040625, m = 0.9
I0628 19:28:18.337291 32082 solver.cpp:349] Iteration 38100 (58.132 iter/s, 1.72022s/100 iter), loss = 0.00451715
I0628 19:28:18.337316 32082 solver.cpp:371]     Train net output #0: loss = 0.00451716 (* 1 = 0.00451716 loss)
I0628 19:28:18.337321 32082 sgd_solver.cpp:137] Iteration 38100, lr = 0.00404688, m = 0.9
I0628 19:28:20.055465 32082 solver.cpp:349] Iteration 38200 (58.2173 iter/s, 1.7177s/100 iter), loss = 0.0135893
I0628 19:28:20.055490 32082 solver.cpp:371]     Train net output #0: loss = 0.0135893 (* 1 = 0.0135893 loss)
I0628 19:28:20.055496 32082 sgd_solver.cpp:137] Iteration 38200, lr = 0.00403125, m = 0.9
I0628 19:28:21.395459 32050 data_reader.cpp:262] Starting prefetch of epoch 49
I0628 19:28:21.774197 32082 solver.cpp:349] Iteration 38300 (58.1984 iter/s, 1.71826s/100 iter), loss = 0.00881385
I0628 19:28:21.774233 32082 solver.cpp:371]     Train net output #0: loss = 0.00881385 (* 1 = 0.00881385 loss)
I0628 19:28:21.774237 32082 sgd_solver.cpp:137] Iteration 38300, lr = 0.00401562, m = 0.9
I0628 19:28:23.493762 32082 solver.cpp:349] Iteration 38400 (58.1706 iter/s, 1.71908s/100 iter), loss = 0.00543489
I0628 19:28:23.493784 32082 solver.cpp:371]     Train net output #0: loss = 0.00543489 (* 1 = 0.00543489 loss)
I0628 19:28:23.493790 32082 sgd_solver.cpp:137] Iteration 38400, lr = 0.004, m = 0.9
I0628 19:28:25.218673 32082 solver.cpp:349] Iteration 38500 (57.9898 iter/s, 1.72444s/100 iter), loss = 0.010471
I0628 19:28:25.218694 32082 solver.cpp:371]     Train net output #0: loss = 0.010471 (* 1 = 0.010471 loss)
I0628 19:28:25.218698 32082 sgd_solver.cpp:137] Iteration 38500, lr = 0.00398437, m = 0.9
I0628 19:28:26.937132 32082 solver.cpp:349] Iteration 38600 (58.2074 iter/s, 1.718s/100 iter), loss = 0.0326741
I0628 19:28:26.937153 32082 solver.cpp:371]     Train net output #0: loss = 0.0326741 (* 1 = 0.0326741 loss)
I0628 19:28:26.937156 32082 sgd_solver.cpp:137] Iteration 38600, lr = 0.00396875, m = 0.9
I0628 19:28:28.656319 32082 solver.cpp:349] Iteration 38700 (58.1826 iter/s, 1.71873s/100 iter), loss = 0.0100293
I0628 19:28:28.656342 32082 solver.cpp:371]     Train net output #0: loss = 0.0100293 (* 1 = 0.0100293 loss)
I0628 19:28:28.656347 32082 sgd_solver.cpp:137] Iteration 38700, lr = 0.00395312, m = 0.9
I0628 19:28:30.374647 32082 solver.cpp:349] Iteration 38800 (58.2118 iter/s, 1.71786s/100 iter), loss = 0.0128048
I0628 19:28:30.374670 32082 solver.cpp:371]     Train net output #0: loss = 0.0128048 (* 1 = 0.0128048 loss)
I0628 19:28:30.374675 32082 sgd_solver.cpp:137] Iteration 38800, lr = 0.0039375, m = 0.9
I0628 19:28:32.093144 32082 solver.cpp:349] Iteration 38900 (58.2061 iter/s, 1.71803s/100 iter), loss = 0.0151223
I0628 19:28:32.093164 32082 solver.cpp:371]     Train net output #0: loss = 0.0151223 (* 1 = 0.0151223 loss)
I0628 19:28:32.093168 32082 sgd_solver.cpp:137] Iteration 38900, lr = 0.00392187, m = 0.9
I0628 19:28:33.793215 32082 solver.cpp:401] Sparsity after update:
I0628 19:28:33.794261 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:28:33.794270 32082 net.cpp:2170] conv1a_param_0(0.39) 
I0628 19:28:33.794276 32082 net.cpp:2170] conv1b_param_0(0.78) 
I0628 19:28:33.794278 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:28:33.794281 32082 net.cpp:2170] res2a_branch2a_param_0(0.78) 
I0628 19:28:33.794282 32082 net.cpp:2170] res2a_branch2b_param_0(0.661) 
I0628 19:28:33.794286 32082 net.cpp:2170] res3a_branch2a_param_0(0.736) 
I0628 19:28:33.794291 32082 net.cpp:2170] res3a_branch2b_param_0(0.697) 
I0628 19:28:33.794296 32082 net.cpp:2170] res4a_branch2a_param_0(0.779) 
I0628 19:28:33.794297 32082 net.cpp:2170] res4a_branch2b_param_0(0.78) 
I0628 19:28:33.794301 32082 net.cpp:2170] res5a_branch2a_param_0(0.78) 
I0628 19:28:33.794304 32082 net.cpp:2170] res5a_branch2b_param_0(0.78) 
I0628 19:28:33.794308 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.82806e+06/2.3599e+06) 0.775
I0628 19:28:33.794318 32082 solver.cpp:545] Iteration 39000, Testing net (#0)
I0628 19:28:34.790611 32080 data_reader.cpp:262] Starting prefetch of epoch 39
I0628 19:28:34.812432 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.904
I0628 19:28:34.812443 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9936
I0628 19:28:34.812448 32082 solver.cpp:630]     Test net output #2: loss = 0.339945 (* 1 = 0.339945 loss)
I0628 19:28:34.812463 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01789s
I0628 19:28:34.829643 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.8
I0628 19:28:35.772253 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:28:35.773727 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:28:35.774091 32082 solver.cpp:349] Iteration 39000 (27.1739 iter/s, 3.68s/100 iter), loss = 0.0125196
I0628 19:28:35.774108 32082 solver.cpp:371]     Train net output #0: loss = 0.0125196 (* 1 = 0.0125196 loss)
I0628 19:28:35.774123 32082 sgd_solver.cpp:137] Iteration 39000, lr = 0.00390625, m = 0.9
I0628 19:28:36.803174 32050 data_reader.cpp:262] Starting prefetch of epoch 50
I0628 19:28:37.507954 32082 solver.cpp:349] Iteration 39100 (57.6904 iter/s, 1.73339s/100 iter), loss = 0.0163694
I0628 19:28:37.507998 32082 solver.cpp:371]     Train net output #0: loss = 0.0163694 (* 1 = 0.0163694 loss)
I0628 19:28:37.508005 32082 sgd_solver.cpp:137] Iteration 39100, lr = 0.00389063, m = 0.9
I0628 19:28:39.225128 32082 solver.cpp:349] Iteration 39200 (58.2519 iter/s, 1.71668s/100 iter), loss = 0.0129611
I0628 19:28:39.225208 32082 solver.cpp:371]     Train net output #0: loss = 0.0129611 (* 1 = 0.0129611 loss)
I0628 19:28:39.225214 32082 sgd_solver.cpp:137] Iteration 39200, lr = 0.003875, m = 0.9
I0628 19:28:40.942929 32082 solver.cpp:349] Iteration 39300 (58.2317 iter/s, 1.71728s/100 iter), loss = 0.0142701
I0628 19:28:40.942950 32082 solver.cpp:371]     Train net output #0: loss = 0.0142701 (* 1 = 0.0142701 loss)
I0628 19:28:40.942955 32082 sgd_solver.cpp:137] Iteration 39300, lr = 0.00385938, m = 0.9
I0628 19:28:42.660531 32082 solver.cpp:349] Iteration 39400 (58.2363 iter/s, 1.71714s/100 iter), loss = 0.0127994
I0628 19:28:42.660557 32082 solver.cpp:371]     Train net output #0: loss = 0.0127995 (* 1 = 0.0127995 loss)
I0628 19:28:42.660563 32082 sgd_solver.cpp:137] Iteration 39400, lr = 0.00384375, m = 0.9
I0628 19:28:44.380776 32082 solver.cpp:349] Iteration 39500 (58.147 iter/s, 1.71978s/100 iter), loss = 0.035984
I0628 19:28:44.380798 32082 solver.cpp:371]     Train net output #0: loss = 0.035984 (* 1 = 0.035984 loss)
I0628 19:28:44.380802 32082 sgd_solver.cpp:137] Iteration 39500, lr = 0.00382812, m = 0.9
I0628 19:28:46.101076 32082 solver.cpp:349] Iteration 39600 (58.1449 iter/s, 1.71984s/100 iter), loss = 0.00729266
I0628 19:28:46.101099 32082 solver.cpp:371]     Train net output #0: loss = 0.00729268 (* 1 = 0.00729268 loss)
I0628 19:28:46.101101 32082 sgd_solver.cpp:137] Iteration 39600, lr = 0.0038125, m = 0.9
I0628 19:28:47.819993 32082 solver.cpp:349] Iteration 39700 (58.1917 iter/s, 1.71846s/100 iter), loss = 0.00837137
I0628 19:28:47.820014 32082 solver.cpp:371]     Train net output #0: loss = 0.00837139 (* 1 = 0.00837139 loss)
I0628 19:28:47.820019 32082 sgd_solver.cpp:137] Iteration 39700, lr = 0.00379687, m = 0.9
I0628 19:28:49.542105 32082 solver.cpp:349] Iteration 39800 (58.0834 iter/s, 1.72166s/100 iter), loss = 0.0125428
I0628 19:28:49.542132 32082 solver.cpp:371]     Train net output #0: loss = 0.0125428 (* 1 = 0.0125428 loss)
I0628 19:28:49.542138 32082 sgd_solver.cpp:137] Iteration 39800, lr = 0.00378125, m = 0.9
I0628 19:28:50.230829 32050 data_reader.cpp:262] Starting prefetch of epoch 51
I0628 19:28:51.262450 32082 solver.cpp:349] Iteration 39900 (58.1203 iter/s, 1.72057s/100 iter), loss = 0.00869081
I0628 19:28:51.262472 32082 solver.cpp:371]     Train net output #0: loss = 0.00869083 (* 1 = 0.00869083 loss)
I0628 19:28:51.262476 32082 sgd_solver.cpp:137] Iteration 39900, lr = 0.00376562, m = 0.9
I0628 19:28:52.964654 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0628 19:28:52.972439 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0628 19:28:52.975863 32082 solver.cpp:401] Sparsity after update:
I0628 19:28:52.976987 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:28:52.976994 32082 net.cpp:2170] conv1a_param_0(0.4) 
I0628 19:28:52.977001 32082 net.cpp:2170] conv1b_param_0(0.8) 
I0628 19:28:52.977004 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:28:52.977005 32082 net.cpp:2170] res2a_branch2a_param_0(0.8) 
I0628 19:28:52.977007 32082 net.cpp:2170] res2a_branch2b_param_0(0.663) 
I0628 19:28:52.977010 32082 net.cpp:2170] res3a_branch2a_param_0(0.74) 
I0628 19:28:52.977011 32082 net.cpp:2170] res3a_branch2b_param_0(0.699) 
I0628 19:28:52.977013 32082 net.cpp:2170] res4a_branch2a_param_0(0.785) 
I0628 19:28:52.977015 32082 net.cpp:2170] res4a_branch2b_param_0(0.799) 
I0628 19:28:52.977016 32082 net.cpp:2170] res5a_branch2a_param_0(0.8) 
I0628 19:28:52.977018 32082 net.cpp:2170] res5a_branch2b_param_0(0.8) 
I0628 19:28:52.977020 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.86882e+06/2.3599e+06) 0.792
I0628 19:28:52.977027 32082 solver.cpp:545] Iteration 40000, Testing net (#0)
I0628 19:28:53.971825 32080 data_reader.cpp:262] Starting prefetch of epoch 40
I0628 19:28:53.991928 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.906
I0628 19:28:53.991951 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9942
I0628 19:28:53.991957 32082 solver.cpp:630]     Test net output #2: loss = 0.331833 (* 1 = 0.331833 loss)
I0628 19:28:53.991971 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01523s
I0628 19:28:54.009136 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.82
I0628 19:28:54.977824 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:28:54.979295 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:28:54.979653 32082 solver.cpp:349] Iteration 40000 (26.8946 iter/s, 3.71822s/100 iter), loss = 0.017854
I0628 19:28:54.979671 32082 solver.cpp:371]     Train net output #0: loss = 0.017854 (* 1 = 0.017854 loss)
I0628 19:28:54.979676 32082 sgd_solver.cpp:137] Iteration 40000, lr = 0.00375, m = 0.9
I0628 19:28:56.696931 32082 solver.cpp:349] Iteration 40100 (58.2167 iter/s, 1.71772s/100 iter), loss = 0.0257802
I0628 19:28:56.696959 32082 solver.cpp:371]     Train net output #0: loss = 0.0257802 (* 1 = 0.0257802 loss)
I0628 19:28:56.696965 32082 sgd_solver.cpp:137] Iteration 40100, lr = 0.00373438, m = 0.9
I0628 19:28:58.414034 32082 solver.cpp:349] Iteration 40200 (58.2232 iter/s, 1.71753s/100 iter), loss = 0.00336482
I0628 19:28:58.414057 32082 solver.cpp:371]     Train net output #0: loss = 0.00336484 (* 1 = 0.00336484 loss)
I0628 19:28:58.414060 32082 sgd_solver.cpp:137] Iteration 40200, lr = 0.00371875, m = 0.9
I0628 19:29:00.132112 32082 solver.cpp:349] Iteration 40300 (58.1901 iter/s, 1.71851s/100 iter), loss = 0.0131555
I0628 19:29:00.132136 32082 solver.cpp:371]     Train net output #0: loss = 0.0131555 (* 1 = 0.0131555 loss)
I0628 19:29:00.132143 32082 sgd_solver.cpp:137] Iteration 40300, lr = 0.00370313, m = 0.9
I0628 19:29:01.848062 32082 solver.cpp:349] Iteration 40400 (58.2626 iter/s, 1.71637s/100 iter), loss = 0.0187084
I0628 19:29:01.848088 32082 solver.cpp:371]     Train net output #0: loss = 0.0187084 (* 1 = 0.0187084 loss)
I0628 19:29:01.848094 32082 sgd_solver.cpp:137] Iteration 40400, lr = 0.0036875, m = 0.9
I0628 19:29:03.567414 32082 solver.cpp:349] Iteration 40500 (58.1475 iter/s, 1.71976s/100 iter), loss = 0.0372087
I0628 19:29:03.567441 32082 solver.cpp:371]     Train net output #0: loss = 0.0372087 (* 1 = 0.0372087 loss)
I0628 19:29:03.567447 32082 sgd_solver.cpp:137] Iteration 40500, lr = 0.00367187, m = 0.9
I0628 19:29:05.287065 32082 solver.cpp:349] Iteration 40600 (58.1376 iter/s, 1.72006s/100 iter), loss = 0.014595
I0628 19:29:05.287088 32082 solver.cpp:371]     Train net output #0: loss = 0.014595 (* 1 = 0.014595 loss)
I0628 19:29:05.287091 32082 sgd_solver.cpp:137] Iteration 40600, lr = 0.00365625, m = 0.9
I0628 19:29:05.648756 32050 data_reader.cpp:262] Starting prefetch of epoch 52
I0628 19:29:07.005853 32082 solver.cpp:349] Iteration 40700 (58.1666 iter/s, 1.7192s/100 iter), loss = 0.0100605
I0628 19:29:07.005879 32082 solver.cpp:371]     Train net output #0: loss = 0.0100606 (* 1 = 0.0100606 loss)
I0628 19:29:07.005885 32082 sgd_solver.cpp:137] Iteration 40700, lr = 0.00364062, m = 0.9
I0628 19:29:08.723845 32082 solver.cpp:349] Iteration 40800 (58.194 iter/s, 1.71839s/100 iter), loss = 0.0127572
I0628 19:29:08.723871 32082 solver.cpp:371]     Train net output #0: loss = 0.0127572 (* 1 = 0.0127572 loss)
I0628 19:29:08.723877 32082 sgd_solver.cpp:137] Iteration 40800, lr = 0.003625, m = 0.9
I0628 19:29:10.442031 32082 solver.cpp:349] Iteration 40900 (58.1876 iter/s, 1.71858s/100 iter), loss = 0.0090329
I0628 19:29:10.442098 32082 solver.cpp:371]     Train net output #0: loss = 0.00903292 (* 1 = 0.00903292 loss)
I0628 19:29:10.442106 32082 sgd_solver.cpp:137] Iteration 40900, lr = 0.00360937, m = 0.9
I0628 19:29:12.145032 32082 solver.cpp:401] Sparsity after update:
I0628 19:29:12.146100 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:29:12.146107 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:29:12.146113 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:29:12.146116 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:29:12.146118 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:29:12.146121 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:29:12.146122 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:29:12.146124 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:29:12.146127 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:29:12.146129 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:29:12.146132 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:29:12.146134 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:29:12.146136 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:29:12.146144 32082 solver.cpp:545] Iteration 41000, Testing net (#0)
I0628 19:29:13.143688 32080 data_reader.cpp:262] Starting prefetch of epoch 41
I0628 19:29:13.165563 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9062
I0628 19:29:13.165575 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:29:13.165580 32082 solver.cpp:630]     Test net output #2: loss = 0.321509 (* 1 = 0.321509 loss)
I0628 19:29:13.165594 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0197s
I0628 19:29:13.182957 32082 solver.cpp:349] Iteration 41000 (36.476 iter/s, 2.74153s/100 iter), loss = 0.00918568
I0628 19:29:13.182981 32082 solver.cpp:371]     Train net output #0: loss = 0.0091857 (* 1 = 0.0091857 loss)
I0628 19:29:13.182984 32082 sgd_solver.cpp:137] Iteration 41000, lr = 0.00359375, m = 0.9
I0628 19:29:14.907570 32082 solver.cpp:349] Iteration 41100 (57.9709 iter/s, 1.725s/100 iter), loss = 0.0451678
I0628 19:29:14.907598 32082 solver.cpp:371]     Train net output #0: loss = 0.0451678 (* 1 = 0.0451678 loss)
I0628 19:29:14.907604 32082 sgd_solver.cpp:137] Iteration 41100, lr = 0.00357813, m = 0.9
I0628 19:29:16.630242 32082 solver.cpp:349] Iteration 41200 (58.0367 iter/s, 1.72305s/100 iter), loss = 0.00724304
I0628 19:29:16.630270 32082 solver.cpp:371]     Train net output #0: loss = 0.00724307 (* 1 = 0.00724307 loss)
I0628 19:29:16.630275 32082 sgd_solver.cpp:137] Iteration 41200, lr = 0.0035625, m = 0.9
I0628 19:29:18.349261 32082 solver.cpp:349] Iteration 41300 (58.1603 iter/s, 1.71939s/100 iter), loss = 0.00720539
I0628 19:29:18.349285 32082 solver.cpp:371]     Train net output #0: loss = 0.00720543 (* 1 = 0.00720543 loss)
I0628 19:29:18.349292 32082 sgd_solver.cpp:137] Iteration 41300, lr = 0.00354687, m = 0.9
I0628 19:29:20.064653 32082 solver.cpp:349] Iteration 41400 (58.2832 iter/s, 1.71576s/100 iter), loss = 0.00917817
I0628 19:29:20.064680 32082 solver.cpp:371]     Train net output #0: loss = 0.00917821 (* 1 = 0.00917821 loss)
I0628 19:29:20.064687 32082 sgd_solver.cpp:137] Iteration 41400, lr = 0.00353125, m = 0.9
I0628 19:29:20.119894 32050 data_reader.cpp:262] Starting prefetch of epoch 53
I0628 19:29:21.791862 32082 solver.cpp:349] Iteration 41500 (57.8847 iter/s, 1.72757s/100 iter), loss = 0.0126066
I0628 19:29:21.791887 32082 solver.cpp:371]     Train net output #0: loss = 0.0126067 (* 1 = 0.0126067 loss)
I0628 19:29:21.791893 32082 sgd_solver.cpp:137] Iteration 41500, lr = 0.00351562, m = 0.9
I0628 19:29:23.512588 32082 solver.cpp:349] Iteration 41600 (58.1029 iter/s, 1.72109s/100 iter), loss = 0.0109971
I0628 19:29:23.512609 32082 solver.cpp:371]     Train net output #0: loss = 0.0109971 (* 1 = 0.0109971 loss)
I0628 19:29:23.512614 32082 sgd_solver.cpp:137] Iteration 41600, lr = 0.0035, m = 0.9
I0628 19:29:25.231779 32082 solver.cpp:349] Iteration 41700 (58.1547 iter/s, 1.71955s/100 iter), loss = 0.00551805
I0628 19:29:25.231843 32082 solver.cpp:371]     Train net output #0: loss = 0.00551808 (* 1 = 0.00551808 loss)
I0628 19:29:25.231850 32082 sgd_solver.cpp:137] Iteration 41700, lr = 0.00348437, m = 0.9
I0628 19:29:26.949162 32082 solver.cpp:349] Iteration 41800 (58.2177 iter/s, 1.71769s/100 iter), loss = 0.0135826
I0628 19:29:26.949188 32082 solver.cpp:371]     Train net output #0: loss = 0.0135826 (* 1 = 0.0135826 loss)
I0628 19:29:26.949194 32082 sgd_solver.cpp:137] Iteration 41800, lr = 0.00346875, m = 0.9
I0628 19:29:28.668037 32082 solver.cpp:349] Iteration 41900 (58.166 iter/s, 1.71922s/100 iter), loss = 0.0109861
I0628 19:29:28.668063 32082 solver.cpp:371]     Train net output #0: loss = 0.0109862 (* 1 = 0.0109862 loss)
I0628 19:29:28.668069 32082 sgd_solver.cpp:137] Iteration 41900, lr = 0.00345312, m = 0.9
I0628 19:29:30.373868 32082 solver.cpp:401] Sparsity after update:
I0628 19:29:30.374994 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:29:30.375001 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:29:30.375008 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:29:30.375010 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:29:30.375012 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:29:30.375015 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:29:30.375017 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:29:30.375020 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:29:30.375022 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:29:30.375025 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:29:30.375025 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:29:30.375027 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:29:30.375030 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:29:30.375037 32082 solver.cpp:545] Iteration 42000, Testing net (#0)
I0628 19:29:31.374879 32080 data_reader.cpp:262] Starting prefetch of epoch 42
I0628 19:29:31.395395 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9072
I0628 19:29:31.395407 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:29:31.395412 32082 solver.cpp:630]     Test net output #2: loss = 0.326982 (* 1 = 0.326982 loss)
I0628 19:29:31.395426 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02061s
I0628 19:29:31.413235 32082 solver.cpp:349] Iteration 42000 (36.4197 iter/s, 2.74577s/100 iter), loss = 0.0130987
I0628 19:29:31.413260 32082 solver.cpp:371]     Train net output #0: loss = 0.0130987 (* 1 = 0.0130987 loss)
I0628 19:29:31.413266 32082 sgd_solver.cpp:137] Iteration 42000, lr = 0.0034375, m = 0.9
I0628 19:29:33.133997 32082 solver.cpp:349] Iteration 42100 (58.1025 iter/s, 1.7211s/100 iter), loss = 0.0179411
I0628 19:29:33.134016 32082 solver.cpp:371]     Train net output #0: loss = 0.0179411 (* 1 = 0.0179411 loss)
I0628 19:29:33.134021 32082 sgd_solver.cpp:137] Iteration 42100, lr = 0.00342188, m = 0.9
I0628 19:29:34.579499 32050 data_reader.cpp:262] Starting prefetch of epoch 54
I0628 19:29:34.854001 32082 solver.cpp:349] Iteration 42200 (58.128 iter/s, 1.72034s/100 iter), loss = 0.00732109
I0628 19:29:34.854029 32082 solver.cpp:371]     Train net output #0: loss = 0.00732114 (* 1 = 0.00732114 loss)
I0628 19:29:34.854035 32082 sgd_solver.cpp:137] Iteration 42200, lr = 0.00340625, m = 0.9
I0628 19:29:36.575470 32082 solver.cpp:349] Iteration 42300 (58.0791 iter/s, 1.72179s/100 iter), loss = 0.0098538
I0628 19:29:36.575497 32082 solver.cpp:371]     Train net output #0: loss = 0.00985384 (* 1 = 0.00985384 loss)
I0628 19:29:36.575503 32082 sgd_solver.cpp:137] Iteration 42300, lr = 0.00339063, m = 0.9
I0628 19:29:38.294020 32082 solver.cpp:349] Iteration 42400 (58.1779 iter/s, 1.71887s/100 iter), loss = 0.00521113
I0628 19:29:38.294044 32082 solver.cpp:371]     Train net output #0: loss = 0.00521117 (* 1 = 0.00521117 loss)
I0628 19:29:38.294049 32082 sgd_solver.cpp:137] Iteration 42400, lr = 0.003375, m = 0.9
I0628 19:29:40.016149 32082 solver.cpp:349] Iteration 42500 (58.0569 iter/s, 1.72245s/100 iter), loss = 0.00789747
I0628 19:29:40.016187 32082 solver.cpp:371]     Train net output #0: loss = 0.00789751 (* 1 = 0.00789751 loss)
I0628 19:29:40.016191 32082 sgd_solver.cpp:137] Iteration 42500, lr = 0.00335937, m = 0.9
I0628 19:29:41.735357 32082 solver.cpp:349] Iteration 42600 (58.1561 iter/s, 1.71951s/100 iter), loss = 0.0287423
I0628 19:29:41.735414 32082 solver.cpp:371]     Train net output #0: loss = 0.0287423 (* 1 = 0.0287423 loss)
I0628 19:29:41.735419 32082 sgd_solver.cpp:137] Iteration 42600, lr = 0.00334375, m = 0.9
I0628 19:29:43.453660 32082 solver.cpp:349] Iteration 42700 (58.1877 iter/s, 1.71858s/100 iter), loss = 0.0108914
I0628 19:29:43.453683 32082 solver.cpp:371]     Train net output #0: loss = 0.0108915 (* 1 = 0.0108915 loss)
I0628 19:29:43.453689 32082 sgd_solver.cpp:137] Iteration 42700, lr = 0.00332812, m = 0.9
I0628 19:29:45.174269 32082 solver.cpp:349] Iteration 42800 (58.1086 iter/s, 1.72092s/100 iter), loss = 0.0085382
I0628 19:29:45.174290 32082 solver.cpp:371]     Train net output #0: loss = 0.00853825 (* 1 = 0.00853825 loss)
I0628 19:29:45.174294 32082 sgd_solver.cpp:137] Iteration 42800, lr = 0.0033125, m = 0.9
I0628 19:29:46.891454 32082 solver.cpp:349] Iteration 42900 (58.2245 iter/s, 1.71749s/100 iter), loss = 0.0180744
I0628 19:29:46.891479 32082 solver.cpp:371]     Train net output #0: loss = 0.0180744 (* 1 = 0.0180744 loss)
I0628 19:29:46.891482 32082 sgd_solver.cpp:137] Iteration 42900, lr = 0.00329687, m = 0.9
I0628 19:29:48.010766 32050 data_reader.cpp:262] Starting prefetch of epoch 55
I0628 19:29:48.594993 32082 solver.cpp:401] Sparsity after update:
I0628 19:29:48.596055 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:29:48.596061 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:29:48.596068 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:29:48.596072 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:29:48.596076 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:29:48.596081 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:29:48.596084 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:29:48.596088 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:29:48.596091 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:29:48.596096 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:29:48.596101 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:29:48.596104 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:29:48.596107 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:29:48.596117 32082 solver.cpp:545] Iteration 43000, Testing net (#0)
I0628 19:29:49.594431 32080 data_reader.cpp:262] Starting prefetch of epoch 43
I0628 19:29:49.614636 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9072
I0628 19:29:49.614650 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:29:49.614655 32082 solver.cpp:630]     Test net output #2: loss = 0.32724 (* 1 = 0.32724 loss)
I0628 19:29:49.614668 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01875s
I0628 19:29:49.635653 32082 solver.cpp:349] Iteration 43000 (36.4339 iter/s, 2.7447s/100 iter), loss = 0.01558
I0628 19:29:49.635679 32082 solver.cpp:371]     Train net output #0: loss = 0.01558 (* 1 = 0.01558 loss)
I0628 19:29:49.635685 32082 sgd_solver.cpp:137] Iteration 43000, lr = 0.00328125, m = 0.9
I0628 19:29:51.355626 32082 solver.cpp:349] Iteration 43100 (58.1308 iter/s, 1.72026s/100 iter), loss = 0.00520991
I0628 19:29:51.355654 32082 solver.cpp:371]     Train net output #0: loss = 0.00520995 (* 1 = 0.00520995 loss)
I0628 19:29:51.355659 32082 sgd_solver.cpp:137] Iteration 43100, lr = 0.00326563, m = 0.9
I0628 19:29:53.077832 32082 solver.cpp:349] Iteration 43200 (58.0556 iter/s, 1.72249s/100 iter), loss = 0.0088419
I0628 19:29:53.077858 32082 solver.cpp:371]     Train net output #0: loss = 0.00884195 (* 1 = 0.00884195 loss)
I0628 19:29:53.077865 32082 sgd_solver.cpp:137] Iteration 43200, lr = 0.00325, m = 0.9
I0628 19:29:54.799535 32082 solver.cpp:349] Iteration 43300 (58.0727 iter/s, 1.72198s/100 iter), loss = 0.00762213
I0628 19:29:54.799561 32082 solver.cpp:371]     Train net output #0: loss = 0.00762218 (* 1 = 0.00762218 loss)
I0628 19:29:54.799567 32082 sgd_solver.cpp:137] Iteration 43300, lr = 0.00323438, m = 0.9
I0628 19:29:56.516005 32082 solver.cpp:349] Iteration 43400 (58.2499 iter/s, 1.71674s/100 iter), loss = 0.00712828
I0628 19:29:56.516047 32082 solver.cpp:371]     Train net output #0: loss = 0.00712833 (* 1 = 0.00712833 loss)
I0628 19:29:56.516052 32082 sgd_solver.cpp:137] Iteration 43400, lr = 0.00321875, m = 0.9
I0628 19:29:58.234392 32082 solver.cpp:349] Iteration 43500 (58.1856 iter/s, 1.71864s/100 iter), loss = 0.010086
I0628 19:29:58.234416 32082 solver.cpp:371]     Train net output #0: loss = 0.0100861 (* 1 = 0.0100861 loss)
I0628 19:29:58.234421 32082 sgd_solver.cpp:137] Iteration 43500, lr = 0.00320312, m = 0.9
I0628 19:29:59.951349 32082 solver.cpp:349] Iteration 43600 (58.2335 iter/s, 1.71722s/100 iter), loss = 0.00716173
I0628 19:29:59.951375 32082 solver.cpp:371]     Train net output #0: loss = 0.00716178 (* 1 = 0.00716178 loss)
I0628 19:29:59.951381 32082 sgd_solver.cpp:137] Iteration 43600, lr = 0.0031875, m = 0.9
I0628 19:30:01.672484 32082 solver.cpp:349] Iteration 43700 (58.0923 iter/s, 1.7214s/100 iter), loss = 0.00528269
I0628 19:30:01.672511 32082 solver.cpp:371]     Train net output #0: loss = 0.00528274 (* 1 = 0.00528274 loss)
I0628 19:30:01.672518 32082 sgd_solver.cpp:137] Iteration 43700, lr = 0.00317187, m = 0.9
I0628 19:30:02.462254 32050 data_reader.cpp:262] Starting prefetch of epoch 56
I0628 19:30:03.391921 32082 solver.cpp:349] Iteration 43800 (58.1499 iter/s, 1.71969s/100 iter), loss = 0.0104525
I0628 19:30:03.391947 32082 solver.cpp:371]     Train net output #0: loss = 0.0104526 (* 1 = 0.0104526 loss)
I0628 19:30:03.391952 32082 sgd_solver.cpp:137] Iteration 43800, lr = 0.00315625, m = 0.9
I0628 19:30:05.112218 32082 solver.cpp:349] Iteration 43900 (58.1209 iter/s, 1.72055s/100 iter), loss = 0.005248
I0628 19:30:05.112243 32082 solver.cpp:371]     Train net output #0: loss = 0.00524806 (* 1 = 0.00524806 loss)
I0628 19:30:05.112249 32082 sgd_solver.cpp:137] Iteration 43900, lr = 0.00314062, m = 0.9
I0628 19:30:06.816552 32082 solver.cpp:401] Sparsity after update:
I0628 19:30:06.817664 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:30:06.817672 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:30:06.817677 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:30:06.817679 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:30:06.817682 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:30:06.817683 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:30:06.817685 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:30:06.817689 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:30:06.817692 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:30:06.817692 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:30:06.817694 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:30:06.817697 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:30:06.817698 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:30:06.817705 32082 solver.cpp:545] Iteration 44000, Testing net (#0)
I0628 19:30:07.816700 32080 data_reader.cpp:262] Starting prefetch of epoch 44
I0628 19:30:07.841500 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9078
I0628 19:30:07.841513 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:30:07.841518 32082 solver.cpp:630]     Test net output #2: loss = 0.327823 (* 1 = 0.327823 loss)
I0628 19:30:07.841531 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.024s
I0628 19:30:07.858922 32082 solver.cpp:349] Iteration 44000 (36.4016 iter/s, 2.74713s/100 iter), loss = 0.0145247
I0628 19:30:07.858945 32082 solver.cpp:371]     Train net output #0: loss = 0.0145247 (* 1 = 0.0145247 loss)
I0628 19:30:07.858948 32082 sgd_solver.cpp:137] Iteration 44000, lr = 0.003125, m = 0.9
I0628 19:30:09.577044 32082 solver.cpp:349] Iteration 44100 (58.1945 iter/s, 1.71837s/100 iter), loss = 0.013836
I0628 19:30:09.577066 32082 solver.cpp:371]     Train net output #0: loss = 0.013836 (* 1 = 0.013836 loss)
I0628 19:30:09.577070 32082 sgd_solver.cpp:137] Iteration 44100, lr = 0.00310938, m = 0.9
I0628 19:30:11.296895 32082 solver.cpp:349] Iteration 44200 (58.1363 iter/s, 1.7201s/100 iter), loss = 0.00296077
I0628 19:30:11.296936 32082 solver.cpp:371]     Train net output #0: loss = 0.00296084 (* 1 = 0.00296084 loss)
I0628 19:30:11.296943 32082 sgd_solver.cpp:137] Iteration 44200, lr = 0.00309375, m = 0.9
I0628 19:30:13.015236 32082 solver.cpp:349] Iteration 44300 (58.1882 iter/s, 1.71856s/100 iter), loss = 0.00231409
I0628 19:30:13.015303 32082 solver.cpp:371]     Train net output #0: loss = 0.00231415 (* 1 = 0.00231415 loss)
I0628 19:30:13.015308 32082 sgd_solver.cpp:137] Iteration 44300, lr = 0.00307812, m = 0.9
I0628 19:30:14.743685 32082 solver.cpp:349] Iteration 44400 (57.8488 iter/s, 1.72864s/100 iter), loss = 0.00645771
I0628 19:30:14.743706 32082 solver.cpp:371]     Train net output #0: loss = 0.00645777 (* 1 = 0.00645777 loss)
I0628 19:30:14.743710 32082 sgd_solver.cpp:137] Iteration 44400, lr = 0.0030625, m = 0.9
I0628 19:30:16.466238 32082 solver.cpp:349] Iteration 44500 (58.0454 iter/s, 1.72279s/100 iter), loss = 0.00843079
I0628 19:30:16.466260 32082 solver.cpp:371]     Train net output #0: loss = 0.00843085 (* 1 = 0.00843085 loss)
I0628 19:30:16.466266 32082 sgd_solver.cpp:137] Iteration 44500, lr = 0.00304687, m = 0.9
I0628 19:30:16.947729 32050 data_reader.cpp:262] Starting prefetch of epoch 57
I0628 19:30:18.186101 32082 solver.cpp:349] Iteration 44600 (58.1364 iter/s, 1.72009s/100 iter), loss = 0.00357081
I0628 19:30:18.186121 32082 solver.cpp:371]     Train net output #0: loss = 0.00357087 (* 1 = 0.00357087 loss)
I0628 19:30:18.186125 32082 sgd_solver.cpp:137] Iteration 44600, lr = 0.00303125, m = 0.9
I0628 19:30:19.904723 32082 solver.cpp:349] Iteration 44700 (58.1783 iter/s, 1.71885s/100 iter), loss = 0.00318138
I0628 19:30:19.904749 32082 solver.cpp:371]     Train net output #0: loss = 0.00318144 (* 1 = 0.00318144 loss)
I0628 19:30:19.904754 32082 sgd_solver.cpp:137] Iteration 44700, lr = 0.00301562, m = 0.9
I0628 19:30:21.623790 32082 solver.cpp:349] Iteration 44800 (58.1637 iter/s, 1.71929s/100 iter), loss = 0.0193758
I0628 19:30:21.623811 32082 solver.cpp:371]     Train net output #0: loss = 0.0193759 (* 1 = 0.0193759 loss)
I0628 19:30:21.623814 32082 sgd_solver.cpp:137] Iteration 44800, lr = 0.003, m = 0.9
I0628 19:30:23.345237 32082 solver.cpp:349] Iteration 44900 (58.0831 iter/s, 1.72167s/100 iter), loss = 0.00643501
I0628 19:30:23.345259 32082 solver.cpp:371]     Train net output #0: loss = 0.00643507 (* 1 = 0.00643507 loss)
I0628 19:30:23.345263 32082 sgd_solver.cpp:137] Iteration 44900, lr = 0.00298437, m = 0.9
I0628 19:30:25.044358 32082 solver.cpp:401] Sparsity after update:
I0628 19:30:25.045480 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:30:25.045486 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:30:25.045492 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:30:25.045495 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:30:25.045497 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:30:25.045498 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:30:25.045500 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:30:25.045502 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:30:25.045506 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:30:25.045507 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:30:25.045511 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:30:25.045512 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:30:25.045514 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:30:25.045521 32082 solver.cpp:545] Iteration 45000, Testing net (#0)
I0628 19:30:26.044845 32080 data_reader.cpp:262] Starting prefetch of epoch 45
I0628 19:30:26.065284 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.909
I0628 19:30:26.065300 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:30:26.065305 32082 solver.cpp:630]     Test net output #2: loss = 0.326119 (* 1 = 0.326119 loss)
I0628 19:30:26.065321 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01995s
I0628 19:30:26.082873 32082 solver.cpp:349] Iteration 45000 (36.5229 iter/s, 2.73801s/100 iter), loss = 0.00913076
I0628 19:30:26.082896 32082 solver.cpp:371]     Train net output #0: loss = 0.00913083 (* 1 = 0.00913083 loss)
I0628 19:30:26.082902 32082 sgd_solver.cpp:137] Iteration 45000, lr = 0.00296875, m = 0.9
I0628 19:30:27.801398 32082 solver.cpp:349] Iteration 45100 (58.1824 iter/s, 1.71873s/100 iter), loss = 0.0139378
I0628 19:30:27.801434 32082 solver.cpp:371]     Train net output #0: loss = 0.0139378 (* 1 = 0.0139378 loss)
I0628 19:30:27.801439 32082 sgd_solver.cpp:137] Iteration 45100, lr = 0.00295313, m = 0.9
I0628 19:30:29.526639 32082 solver.cpp:349] Iteration 45200 (57.9564 iter/s, 1.72544s/100 iter), loss = 0.0170134
I0628 19:30:29.526661 32082 solver.cpp:371]     Train net output #0: loss = 0.0170135 (* 1 = 0.0170135 loss)
I0628 19:30:29.526665 32082 sgd_solver.cpp:137] Iteration 45200, lr = 0.0029375, m = 0.9
I0628 19:30:31.245883 32082 solver.cpp:349] Iteration 45300 (58.1582 iter/s, 1.71945s/100 iter), loss = 0.00509872
I0628 19:30:31.245905 32082 solver.cpp:371]     Train net output #0: loss = 0.00509879 (* 1 = 0.00509879 loss)
I0628 19:30:31.245909 32082 sgd_solver.cpp:137] Iteration 45300, lr = 0.00292188, m = 0.9
I0628 19:30:31.400969 32050 data_reader.cpp:262] Starting prefetch of epoch 58
I0628 19:30:32.968406 32082 solver.cpp:349] Iteration 45400 (58.0476 iter/s, 1.72273s/100 iter), loss = 0.00380326
I0628 19:30:32.968430 32082 solver.cpp:371]     Train net output #0: loss = 0.00380332 (* 1 = 0.00380332 loss)
I0628 19:30:32.968435 32082 sgd_solver.cpp:137] Iteration 45400, lr = 0.00290625, m = 0.9
I0628 19:30:34.688326 32082 solver.cpp:349] Iteration 45500 (58.1357 iter/s, 1.72011s/100 iter), loss = 0.00527788
I0628 19:30:34.688352 32082 solver.cpp:371]     Train net output #0: loss = 0.00527794 (* 1 = 0.00527794 loss)
I0628 19:30:34.688359 32082 sgd_solver.cpp:137] Iteration 45500, lr = 0.00289063, m = 0.9
I0628 19:30:36.412231 32082 solver.cpp:349] Iteration 45600 (58.0015 iter/s, 1.72409s/100 iter), loss = 0.00687773
I0628 19:30:36.412253 32082 solver.cpp:371]     Train net output #0: loss = 0.00687779 (* 1 = 0.00687779 loss)
I0628 19:30:36.412258 32082 sgd_solver.cpp:137] Iteration 45600, lr = 0.002875, m = 0.9
I0628 19:30:38.131016 32082 solver.cpp:349] Iteration 45700 (58.1742 iter/s, 1.71898s/100 iter), loss = 0.0133891
I0628 19:30:38.131043 32082 solver.cpp:371]     Train net output #0: loss = 0.0133892 (* 1 = 0.0133892 loss)
I0628 19:30:38.131048 32082 sgd_solver.cpp:137] Iteration 45700, lr = 0.00285937, m = 0.9
I0628 19:30:39.849946 32082 solver.cpp:349] Iteration 45800 (58.1697 iter/s, 1.71911s/100 iter), loss = 0.00459244
I0628 19:30:39.849972 32082 solver.cpp:371]     Train net output #0: loss = 0.0045925 (* 1 = 0.0045925 loss)
I0628 19:30:39.849978 32082 sgd_solver.cpp:137] Iteration 45800, lr = 0.00284375, m = 0.9
I0628 19:30:41.568409 32082 solver.cpp:349] Iteration 45900 (58.1856 iter/s, 1.71864s/100 iter), loss = 0.00529667
I0628 19:30:41.568436 32082 solver.cpp:371]     Train net output #0: loss = 0.00529673 (* 1 = 0.00529673 loss)
I0628 19:30:41.568442 32082 sgd_solver.cpp:137] Iteration 45900, lr = 0.00282812, m = 0.9
I0628 19:30:43.268610 32082 solver.cpp:401] Sparsity after update:
I0628 19:30:43.269696 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:30:43.269704 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:30:43.269713 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:30:43.269717 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:30:43.269721 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:30:43.269726 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:30:43.269728 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:30:43.269733 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:30:43.269737 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:30:43.269740 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:30:43.269744 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:30:43.269748 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:30:43.269752 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:30:43.269762 32082 solver.cpp:545] Iteration 46000, Testing net (#0)
I0628 19:30:44.271152 32080 data_reader.cpp:262] Starting prefetch of epoch 46
I0628 19:30:44.291303 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9092
I0628 19:30:44.291316 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:30:44.291322 32082 solver.cpp:630]     Test net output #2: loss = 0.32822 (* 1 = 0.32822 loss)
I0628 19:30:44.291338 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0217s
I0628 19:30:44.309070 32082 solver.cpp:349] Iteration 46000 (36.4835 iter/s, 2.74096s/100 iter), loss = 0.00551848
I0628 19:30:44.309098 32082 solver.cpp:371]     Train net output #0: loss = 0.00551854 (* 1 = 0.00551854 loss)
I0628 19:30:44.309103 32082 sgd_solver.cpp:137] Iteration 46000, lr = 0.0028125, m = 0.9
I0628 19:30:45.858000 32050 data_reader.cpp:262] Starting prefetch of epoch 59
I0628 19:30:46.029335 32082 solver.cpp:349] Iteration 46100 (58.1249 iter/s, 1.72043s/100 iter), loss = 0.00680064
I0628 19:30:46.029353 32082 solver.cpp:371]     Train net output #0: loss = 0.0068007 (* 1 = 0.0068007 loss)
I0628 19:30:46.029357 32082 sgd_solver.cpp:137] Iteration 46100, lr = 0.00279688, m = 0.9
I0628 19:30:47.747095 32082 solver.cpp:349] Iteration 46200 (58.2094 iter/s, 1.71793s/100 iter), loss = 0.0103332
I0628 19:30:47.747122 32082 solver.cpp:371]     Train net output #0: loss = 0.0103333 (* 1 = 0.0103333 loss)
I0628 19:30:47.747128 32082 sgd_solver.cpp:137] Iteration 46200, lr = 0.00278125, m = 0.9
I0628 19:30:49.467548 32082 solver.cpp:349] Iteration 46300 (58.1188 iter/s, 1.72061s/100 iter), loss = 0.00277061
I0628 19:30:49.467574 32082 solver.cpp:371]     Train net output #0: loss = 0.00277067 (* 1 = 0.00277067 loss)
I0628 19:30:49.467581 32082 sgd_solver.cpp:137] Iteration 46300, lr = 0.00276563, m = 0.9
I0628 19:30:51.189524 32082 solver.cpp:349] Iteration 46400 (58.0675 iter/s, 1.72213s/100 iter), loss = 0.0107746
I0628 19:30:51.189550 32082 solver.cpp:371]     Train net output #0: loss = 0.0107747 (* 1 = 0.0107747 loss)
I0628 19:30:51.189558 32082 sgd_solver.cpp:137] Iteration 46400, lr = 0.00275, m = 0.9
I0628 19:30:52.907680 32082 solver.cpp:349] Iteration 46500 (58.1968 iter/s, 1.71831s/100 iter), loss = 0.00777216
I0628 19:30:52.907702 32082 solver.cpp:371]     Train net output #0: loss = 0.00777223 (* 1 = 0.00777223 loss)
I0628 19:30:52.907706 32082 sgd_solver.cpp:137] Iteration 46500, lr = 0.00273437, m = 0.9
I0628 19:30:54.626277 32082 solver.cpp:349] Iteration 46600 (58.1816 iter/s, 1.71876s/100 iter), loss = 0.00644095
I0628 19:30:54.626302 32082 solver.cpp:371]     Train net output #0: loss = 0.00644101 (* 1 = 0.00644101 loss)
I0628 19:30:54.626308 32082 sgd_solver.cpp:137] Iteration 46600, lr = 0.00271875, m = 0.9
I0628 19:30:56.346366 32082 solver.cpp:349] Iteration 46700 (58.1315 iter/s, 1.72024s/100 iter), loss = 0.00683091
I0628 19:30:56.346387 32082 solver.cpp:371]     Train net output #0: loss = 0.00683097 (* 1 = 0.00683097 loss)
I0628 19:30:56.346391 32082 sgd_solver.cpp:137] Iteration 46700, lr = 0.00270312, m = 0.9
I0628 19:30:58.065832 32082 solver.cpp:349] Iteration 46800 (58.1525 iter/s, 1.71962s/100 iter), loss = 0.0041775
I0628 19:30:58.065872 32082 solver.cpp:371]     Train net output #0: loss = 0.00417756 (* 1 = 0.00417756 loss)
I0628 19:30:58.065878 32082 sgd_solver.cpp:137] Iteration 46800, lr = 0.0026875, m = 0.9
I0628 19:30:59.284381 32050 data_reader.cpp:262] Starting prefetch of epoch 60
I0628 19:30:59.781443 32082 solver.cpp:349] Iteration 46900 (58.2839 iter/s, 1.71574s/100 iter), loss = 0.0119092
I0628 19:30:59.781467 32082 solver.cpp:371]     Train net output #0: loss = 0.0119093 (* 1 = 0.0119093 loss)
I0628 19:30:59.781471 32082 sgd_solver.cpp:137] Iteration 46900, lr = 0.00267187, m = 0.9
I0628 19:31:01.484293 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:01.485360 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:01.485368 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:01.485375 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:01.485378 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:01.485379 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:01.485381 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:01.485384 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:01.485386 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:01.485388 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:01.485391 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:01.485394 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:01.485395 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:01.485397 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:01.485404 32082 solver.cpp:545] Iteration 47000, Testing net (#0)
I0628 19:31:02.481218 32080 data_reader.cpp:262] Starting prefetch of epoch 47
I0628 19:31:02.503768 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9082
I0628 19:31:02.503782 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:31:02.503787 32082 solver.cpp:630]     Test net output #2: loss = 0.33079 (* 1 = 0.33079 loss)
I0628 19:31:02.503801 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0185s
I0628 19:31:02.521303 32082 solver.cpp:349] Iteration 47000 (36.4948 iter/s, 2.74011s/100 iter), loss = 0.00554575
I0628 19:31:02.521327 32082 solver.cpp:371]     Train net output #0: loss = 0.0055458 (* 1 = 0.0055458 loss)
I0628 19:31:02.521330 32082 sgd_solver.cpp:137] Iteration 47000, lr = 0.00265625, m = 0.9
I0628 19:31:04.243083 32082 solver.cpp:349] Iteration 47100 (58.0747 iter/s, 1.72192s/100 iter), loss = 0.00557048
I0628 19:31:04.243103 32082 solver.cpp:371]     Train net output #0: loss = 0.00557053 (* 1 = 0.00557053 loss)
I0628 19:31:04.243108 32082 sgd_solver.cpp:137] Iteration 47100, lr = 0.00264063, m = 0.9
I0628 19:31:05.964433 32082 solver.cpp:349] Iteration 47200 (58.0892 iter/s, 1.72149s/100 iter), loss = 0.00727172
I0628 19:31:05.964458 32082 solver.cpp:371]     Train net output #0: loss = 0.00727178 (* 1 = 0.00727178 loss)
I0628 19:31:05.964463 32082 sgd_solver.cpp:137] Iteration 47200, lr = 0.002625, m = 0.9
I0628 19:31:07.690390 32082 solver.cpp:349] Iteration 47300 (57.9345 iter/s, 1.72609s/100 iter), loss = 0.00347837
I0628 19:31:07.690413 32082 solver.cpp:371]     Train net output #0: loss = 0.00347843 (* 1 = 0.00347843 loss)
I0628 19:31:07.690418 32082 sgd_solver.cpp:137] Iteration 47300, lr = 0.00260938, m = 0.9
I0628 19:31:09.412642 32082 solver.cpp:349] Iteration 47400 (58.0591 iter/s, 1.72238s/100 iter), loss = 0.00957891
I0628 19:31:09.412669 32082 solver.cpp:371]     Train net output #0: loss = 0.00957897 (* 1 = 0.00957897 loss)
I0628 19:31:09.412675 32082 sgd_solver.cpp:137] Iteration 47400, lr = 0.00259375, m = 0.9
I0628 19:31:11.134905 32082 solver.cpp:349] Iteration 47500 (58.0591 iter/s, 1.72238s/100 iter), loss = 0.00404905
I0628 19:31:11.134932 32082 solver.cpp:371]     Train net output #0: loss = 0.0040491 (* 1 = 0.0040491 loss)
I0628 19:31:11.134938 32082 sgd_solver.cpp:137] Iteration 47500, lr = 0.00257812, m = 0.9
I0628 19:31:12.858428 32082 solver.cpp:349] Iteration 47600 (58.0201 iter/s, 1.72354s/100 iter), loss = 0.00737942
I0628 19:31:12.858453 32082 solver.cpp:371]     Train net output #0: loss = 0.00737948 (* 1 = 0.00737948 loss)
I0628 19:31:12.858459 32082 sgd_solver.cpp:137] Iteration 47600, lr = 0.0025625, m = 0.9
I0628 19:31:13.770386 32050 data_reader.cpp:262] Starting prefetch of epoch 61
I0628 19:31:14.576947 32082 solver.cpp:349] Iteration 47700 (58.1857 iter/s, 1.71863s/100 iter), loss = 0.00309651
I0628 19:31:14.576972 32082 solver.cpp:371]     Train net output #0: loss = 0.00309657 (* 1 = 0.00309657 loss)
I0628 19:31:14.576977 32082 sgd_solver.cpp:137] Iteration 47700, lr = 0.00254687, m = 0.9
I0628 19:31:16.295563 32082 solver.cpp:349] Iteration 47800 (58.1825 iter/s, 1.71873s/100 iter), loss = 0.00687449
I0628 19:31:16.295588 32082 solver.cpp:371]     Train net output #0: loss = 0.00687455 (* 1 = 0.00687455 loss)
I0628 19:31:16.295593 32082 sgd_solver.cpp:137] Iteration 47800, lr = 0.00253125, m = 0.9
I0628 19:31:18.012522 32082 solver.cpp:349] Iteration 47900 (58.2387 iter/s, 1.71707s/100 iter), loss = 0.0064046
I0628 19:31:18.012544 32082 solver.cpp:371]     Train net output #0: loss = 0.00640465 (* 1 = 0.00640465 loss)
I0628 19:31:18.012548 32082 sgd_solver.cpp:137] Iteration 47900, lr = 0.00251562, m = 0.9
I0628 19:31:19.724723 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:19.725774 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:19.725781 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:19.725788 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:19.725790 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:19.725793 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:19.725795 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:19.725797 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:19.725800 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:19.725802 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:19.725805 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:19.725806 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:19.725808 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:19.725811 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:19.725818 32082 solver.cpp:545] Iteration 48000, Testing net (#0)
I0628 19:31:20.720028 32080 data_reader.cpp:262] Starting prefetch of epoch 48
I0628 19:31:20.746903 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.909
I0628 19:31:20.746927 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9944
I0628 19:31:20.746932 32082 solver.cpp:630]     Test net output #2: loss = 0.330493 (* 1 = 0.330493 loss)
I0628 19:31:20.746948 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02121s
I0628 19:31:20.764407 32082 solver.cpp:349] Iteration 48000 (36.336 iter/s, 2.75209s/100 iter), loss = 0.0110237
I0628 19:31:20.764425 32082 solver.cpp:371]     Train net output #0: loss = 0.0110238 (* 1 = 0.0110238 loss)
I0628 19:31:20.764430 32082 sgd_solver.cpp:137] Iteration 48000, lr = 0.0025, m = 0.9
I0628 19:31:22.486985 32082 solver.cpp:349] Iteration 48100 (58.0488 iter/s, 1.72269s/100 iter), loss = 0.012843
I0628 19:31:22.487010 32082 solver.cpp:371]     Train net output #0: loss = 0.012843 (* 1 = 0.012843 loss)
I0628 19:31:22.487016 32082 sgd_solver.cpp:137] Iteration 48100, lr = 0.00248438, m = 0.9
I0628 19:31:24.203696 32082 solver.cpp:349] Iteration 48200 (58.2477 iter/s, 1.71681s/100 iter), loss = 0.0037552
I0628 19:31:24.203719 32082 solver.cpp:371]     Train net output #0: loss = 0.00375525 (* 1 = 0.00375525 loss)
I0628 19:31:24.203725 32082 sgd_solver.cpp:137] Iteration 48200, lr = 0.00246875, m = 0.9
I0628 19:31:25.921236 32082 solver.cpp:349] Iteration 48300 (58.2196 iter/s, 1.71764s/100 iter), loss = 0.00390803
I0628 19:31:25.921262 32082 solver.cpp:371]     Train net output #0: loss = 0.00390808 (* 1 = 0.00390808 loss)
I0628 19:31:25.921267 32082 sgd_solver.cpp:137] Iteration 48300, lr = 0.00245313, m = 0.9
I0628 19:31:27.646749 32082 solver.cpp:349] Iteration 48400 (57.9506 iter/s, 1.72561s/100 iter), loss = 0.00473092
I0628 19:31:27.646775 32082 solver.cpp:371]     Train net output #0: loss = 0.00473096 (* 1 = 0.00473096 loss)
I0628 19:31:27.646781 32082 sgd_solver.cpp:137] Iteration 48400, lr = 0.0024375, m = 0.9
I0628 19:31:28.232884 32050 data_reader.cpp:262] Starting prefetch of epoch 62
I0628 19:31:29.365824 32082 solver.cpp:349] Iteration 48500 (58.1678 iter/s, 1.71916s/100 iter), loss = 0.00767589
I0628 19:31:29.365849 32082 solver.cpp:371]     Train net output #0: loss = 0.00767594 (* 1 = 0.00767594 loss)
I0628 19:31:29.365854 32082 sgd_solver.cpp:137] Iteration 48500, lr = 0.00242188, m = 0.9
I0628 19:31:31.085402 32082 solver.cpp:349] Iteration 48600 (58.1508 iter/s, 1.71967s/100 iter), loss = 0.0031787
I0628 19:31:31.085427 32082 solver.cpp:371]     Train net output #0: loss = 0.00317875 (* 1 = 0.00317875 loss)
I0628 19:31:31.085433 32082 sgd_solver.cpp:137] Iteration 48600, lr = 0.00240625, m = 0.9
I0628 19:31:32.806442 32082 solver.cpp:349] Iteration 48700 (58.1015 iter/s, 1.72113s/100 iter), loss = 0.00741158
I0628 19:31:32.806469 32082 solver.cpp:371]     Train net output #0: loss = 0.00741163 (* 1 = 0.00741163 loss)
I0628 19:31:32.806475 32082 sgd_solver.cpp:137] Iteration 48700, lr = 0.00239062, m = 0.9
I0628 19:31:34.526782 32082 solver.cpp:349] Iteration 48800 (58.1253 iter/s, 1.72042s/100 iter), loss = 0.00253744
I0628 19:31:34.526803 32082 solver.cpp:371]     Train net output #0: loss = 0.00253749 (* 1 = 0.00253749 loss)
I0628 19:31:34.526808 32082 sgd_solver.cpp:137] Iteration 48800, lr = 0.002375, m = 0.9
I0628 19:31:36.245029 32082 solver.cpp:349] Iteration 48900 (58.1959 iter/s, 1.71833s/100 iter), loss = 0.0036672
I0628 19:31:36.245052 32082 solver.cpp:371]     Train net output #0: loss = 0.00366725 (* 1 = 0.00366725 loss)
I0628 19:31:36.245056 32082 sgd_solver.cpp:137] Iteration 48900, lr = 0.00235937, m = 0.9
I0628 19:31:37.945616 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:37.946679 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:37.946687 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:37.946696 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:37.946701 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:37.946704 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:37.946708 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:37.946712 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:37.946717 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:37.946720 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:37.946724 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:37.946727 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:37.946732 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:37.946735 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:37.946745 32082 solver.cpp:545] Iteration 49000, Testing net (#0)
I0628 19:31:38.944047 32080 data_reader.cpp:262] Starting prefetch of epoch 49
I0628 19:31:38.965816 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9084
I0628 19:31:38.965827 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:31:38.965832 32082 solver.cpp:630]     Test net output #2: loss = 0.329053 (* 1 = 0.329053 loss)
I0628 19:31:38.965847 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01917s
I0628 19:31:38.983292 32082 solver.cpp:349] Iteration 49000 (36.5174 iter/s, 2.73842s/100 iter), loss = 0.00571343
I0628 19:31:38.983319 32082 solver.cpp:371]     Train net output #0: loss = 0.00571348 (* 1 = 0.00571348 loss)
I0628 19:31:38.983325 32082 sgd_solver.cpp:137] Iteration 49000, lr = 0.00234375, m = 0.9
I0628 19:31:40.704135 32082 solver.cpp:349] Iteration 49100 (58.1087 iter/s, 1.72091s/100 iter), loss = 0.00505917
I0628 19:31:40.704161 32082 solver.cpp:371]     Train net output #0: loss = 0.00505921 (* 1 = 0.00505921 loss)
I0628 19:31:40.704167 32082 sgd_solver.cpp:137] Iteration 49100, lr = 0.00232813, m = 0.9
I0628 19:31:42.421161 32082 solver.cpp:349] Iteration 49200 (58.2379 iter/s, 1.71709s/100 iter), loss = 0.00428611
I0628 19:31:42.421186 32082 solver.cpp:371]     Train net output #0: loss = 0.00428616 (* 1 = 0.00428616 loss)
I0628 19:31:42.421192 32082 sgd_solver.cpp:137] Iteration 49200, lr = 0.0023125, m = 0.9
I0628 19:31:42.679154 32050 data_reader.cpp:262] Starting prefetch of epoch 63
I0628 19:31:44.139909 32082 solver.cpp:349] Iteration 49300 (58.1803 iter/s, 1.71879s/100 iter), loss = 0.00565032
I0628 19:31:44.139995 32082 solver.cpp:371]     Train net output #0: loss = 0.00565036 (* 1 = 0.00565036 loss)
I0628 19:31:44.140002 32082 sgd_solver.cpp:137] Iteration 49300, lr = 0.00229687, m = 0.9
I0628 19:31:45.858863 32082 solver.cpp:349] Iteration 49400 (58.1749 iter/s, 1.71896s/100 iter), loss = 0.00440927
I0628 19:31:45.858891 32082 solver.cpp:371]     Train net output #0: loss = 0.00440932 (* 1 = 0.00440932 loss)
I0628 19:31:45.858896 32082 sgd_solver.cpp:137] Iteration 49400, lr = 0.00228125, m = 0.9
I0628 19:31:47.576354 32082 solver.cpp:349] Iteration 49500 (58.2224 iter/s, 1.71755s/100 iter), loss = 0.0094743
I0628 19:31:47.576380 32082 solver.cpp:371]     Train net output #0: loss = 0.00947434 (* 1 = 0.00947434 loss)
I0628 19:31:47.576385 32082 sgd_solver.cpp:137] Iteration 49500, lr = 0.00226562, m = 0.9
I0628 19:31:49.298944 32082 solver.cpp:349] Iteration 49600 (58.0501 iter/s, 1.72265s/100 iter), loss = 0.0118733
I0628 19:31:49.298966 32082 solver.cpp:371]     Train net output #0: loss = 0.0118734 (* 1 = 0.0118734 loss)
I0628 19:31:49.298970 32082 sgd_solver.cpp:137] Iteration 49600, lr = 0.00225, m = 0.9
I0628 19:31:51.018218 32082 solver.cpp:349] Iteration 49700 (58.162 iter/s, 1.71934s/100 iter), loss = 0.0119982
I0628 19:31:51.018244 32082 solver.cpp:371]     Train net output #0: loss = 0.0119982 (* 1 = 0.0119982 loss)
I0628 19:31:51.018249 32082 sgd_solver.cpp:137] Iteration 49700, lr = 0.00223437, m = 0.9
I0628 19:31:52.740928 32082 solver.cpp:349] Iteration 49800 (58.0462 iter/s, 1.72276s/100 iter), loss = 0.00718952
I0628 19:31:52.740952 32082 solver.cpp:371]     Train net output #0: loss = 0.00718956 (* 1 = 0.00718956 loss)
I0628 19:31:52.740957 32082 sgd_solver.cpp:137] Iteration 49800, lr = 0.00221875, m = 0.9
I0628 19:31:54.463337 32082 solver.cpp:349] Iteration 49900 (58.0563 iter/s, 1.72247s/100 iter), loss = 0.00158542
I0628 19:31:54.463362 32082 solver.cpp:371]     Train net output #0: loss = 0.00158547 (* 1 = 0.00158547 loss)
I0628 19:31:54.463368 32082 sgd_solver.cpp:137] Iteration 49900, lr = 0.00220312, m = 0.9
I0628 19:31:56.112381 32050 data_reader.cpp:262] Starting prefetch of epoch 64
I0628 19:31:56.163964 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0628 19:31:56.172559 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0628 19:31:56.176095 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:56.177222 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:56.177229 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:56.177235 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:56.177237 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:56.177239 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:56.177242 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:56.177243 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:56.177245 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:56.177248 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:56.177249 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:56.177250 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:56.177253 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:56.177254 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:56.177263 32082 solver.cpp:545] Iteration 50000, Testing net (#0)
I0628 19:31:57.175150 32080 data_reader.cpp:262] Starting prefetch of epoch 50
I0628 19:31:57.195279 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9078
I0628 19:31:57.195291 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9944
I0628 19:31:57.195297 32082 solver.cpp:630]     Test net output #2: loss = 0.337476 (* 1 = 0.337476 loss)
I0628 19:31:57.195310 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0181s
I0628 19:31:57.212707 32082 solver.cpp:349] Iteration 50000 (36.3706 iter/s, 2.74947s/100 iter), loss = 0.00362595
I0628 19:31:57.212733 32082 solver.cpp:371]     Train net output #0: loss = 0.003626 (* 1 = 0.003626 loss)
I0628 19:31:57.212739 32082 sgd_solver.cpp:137] Iteration 50000, lr = 0.0021875, m = 0.9
I0628 19:31:58.930752 32082 solver.cpp:349] Iteration 50100 (58.2042 iter/s, 1.71809s/100 iter), loss = 0.0023176
I0628 19:31:58.930778 32082 solver.cpp:371]     Train net output #0: loss = 0.00231765 (* 1 = 0.00231765 loss)
I0628 19:31:58.930783 32082 sgd_solver.cpp:137] Iteration 50100, lr = 0.00217188, m = 0.9
I0628 19:32:00.649725 32082 solver.cpp:349] Iteration 50200 (58.1729 iter/s, 1.71901s/100 iter), loss = 0.0106033
I0628 19:32:00.649751 32082 solver.cpp:371]     Train net output #0: loss = 0.0106033 (* 1 = 0.0106033 loss)
I0628 19:32:00.649756 32082 sgd_solver.cpp:137] Iteration 50200, lr = 0.00215625, m = 0.9
I0628 19:32:02.368999 32082 solver.cpp:349] Iteration 50300 (58.1628 iter/s, 1.71931s/100 iter), loss = 0.0102234
I0628 19:32:02.369019 32082 solver.cpp:371]     Train net output #0: loss = 0.0102235 (* 1 = 0.0102235 loss)
I0628 19:32:02.369022 32082 sgd_solver.cpp:137] Iteration 50300, lr = 0.00214063, m = 0.9
I0628 19:32:04.086170 32082 solver.cpp:349] Iteration 50400 (58.2337 iter/s, 1.71722s/100 iter), loss = 0.00746123
I0628 19:32:04.086195 32082 solver.cpp:371]     Train net output #0: loss = 0.00746127 (* 1 = 0.00746127 loss)
I0628 19:32:04.086200 32082 sgd_solver.cpp:137] Iteration 50400, lr = 0.002125, m = 0.9
I0628 19:32:05.804507 32082 solver.cpp:349] Iteration 50500 (58.1946 iter/s, 1.71837s/100 iter), loss = 0.00221503
I0628 19:32:05.804530 32082 solver.cpp:371]     Train net output #0: loss = 0.00221508 (* 1 = 0.00221508 loss)
I0628 19:32:05.804535 32082 sgd_solver.cpp:137] Iteration 50500, lr = 0.00210937, m = 0.9
I0628 19:32:07.524612 32082 solver.cpp:349] Iteration 50600 (58.1348 iter/s, 1.72014s/100 iter), loss = 0.00134335
I0628 19:32:07.524636 32082 solver.cpp:371]     Train net output #0: loss = 0.00134339 (* 1 = 0.00134339 loss)
I0628 19:32:07.524639 32082 sgd_solver.cpp:137] Iteration 50600, lr = 0.00209375, m = 0.9
I0628 19:32:09.249861 32082 solver.cpp:349] Iteration 50700 (57.9616 iter/s, 1.72528s/100 iter), loss = 0.00947948
I0628 19:32:09.249917 32082 solver.cpp:371]     Train net output #0: loss = 0.00947953 (* 1 = 0.00947953 loss)
I0628 19:32:09.249935 32082 sgd_solver.cpp:137] Iteration 50700, lr = 0.00207812, m = 0.9
I0628 19:32:10.591418 32050 data_reader.cpp:262] Starting prefetch of epoch 65
I0628 19:32:10.968454 32082 solver.cpp:349] Iteration 50800 (58.188 iter/s, 1.71857s/100 iter), loss = 0.00238978
I0628 19:32:10.968482 32082 solver.cpp:371]     Train net output #0: loss = 0.00238982 (* 1 = 0.00238982 loss)
I0628 19:32:10.968487 32082 sgd_solver.cpp:137] Iteration 50800, lr = 0.0020625, m = 0.9
I0628 19:32:12.685891 32082 solver.cpp:349] Iteration 50900 (58.2255 iter/s, 1.71746s/100 iter), loss = 0.00464554
I0628 19:32:12.685919 32082 solver.cpp:371]     Train net output #0: loss = 0.00464558 (* 1 = 0.00464558 loss)
I0628 19:32:12.685925 32082 sgd_solver.cpp:137] Iteration 50900, lr = 0.00204687, m = 0.9
I0628 19:32:14.384325 32082 solver.cpp:401] Sparsity after update:
I0628 19:32:14.385426 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:32:14.385433 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:32:14.385439 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:32:14.385442 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:32:14.385443 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:32:14.385445 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:32:14.385447 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:32:14.385449 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:32:14.385452 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:32:14.385453 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:32:14.385455 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:32:14.385457 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:32:14.385458 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:32:14.385466 32082 solver.cpp:545] Iteration 51000, Testing net (#0)
I0628 19:32:15.381454 32080 data_reader.cpp:262] Starting prefetch of epoch 51
I0628 19:32:15.404444 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.91
I0628 19:32:15.404458 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:32:15.404461 32082 solver.cpp:630]     Test net output #2: loss = 0.330124 (* 1 = 0.330124 loss)
I0628 19:32:15.404475 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01904s
I0628 19:32:15.421845 32082 solver.cpp:349] Iteration 51000 (36.5495 iter/s, 2.73601s/100 iter), loss = 0.00721269
I0628 19:32:15.421871 32082 solver.cpp:371]     Train net output #0: loss = 0.00721274 (* 1 = 0.00721274 loss)
I0628 19:32:15.421876 32082 sgd_solver.cpp:137] Iteration 51000, lr = 0.00203125, m = 0.9
I0628 19:32:17.137773 32082 solver.cpp:349] Iteration 51100 (58.2768 iter/s, 1.71595s/100 iter), loss = 0.0102852
I0628 19:32:17.137796 32082 solver.cpp:371]     Train net output #0: loss = 0.0102853 (* 1 = 0.0102853 loss)
I0628 19:32:17.137801 32082 sgd_solver.cpp:137] Iteration 51100, lr = 0.00201563, m = 0.9
I0628 19:32:18.862146 32082 solver.cpp:349] Iteration 51200 (57.9914 iter/s, 1.72439s/100 iter), loss = 0.00549418
I0628 19:32:18.862174 32082 solver.cpp:371]     Train net output #0: loss = 0.00549423 (* 1 = 0.00549423 loss)
I0628 19:32:18.862180 32082 sgd_solver.cpp:137] Iteration 51200, lr = 0.002, m = 0.9
I0628 19:32:20.580976 32082 solver.cpp:349] Iteration 51300 (58.1787 iter/s, 1.71884s/100 iter), loss = 0.0055683
I0628 19:32:20.580998 32082 solver.cpp:371]     Train net output #0: loss = 0.00556834 (* 1 = 0.00556834 loss)
I0628 19:32:20.581002 32082 sgd_solver.cpp:137] Iteration 51300, lr = 0.00198438, m = 0.9
I0628 19:32:22.303722 32082 solver.cpp:349] Iteration 51400 (58.0462 iter/s, 1.72277s/100 iter), loss = 0.00289521
I0628 19:32:22.303743 32082 solver.cpp:371]     Train net output #0: loss = 0.00289525 (* 1 = 0.00289525 loss)
I0628 19:32:22.303747 32082 sgd_solver.cpp:137] Iteration 51400, lr = 0.00196875, m = 0.9
I0628 19:32:24.022572 32082 solver.cpp:349] Iteration 51500 (58.1779 iter/s, 1.71887s/100 iter), loss = 0.0050806
I0628 19:32:24.022625 32082 solver.cpp:371]     Train net output #0: loss = 0.00508065 (* 1 = 0.00508065 loss)
I0628 19:32:24.022639 32082 sgd_solver.cpp:137] Iteration 51500, lr = 0.00195312, m = 0.9
I0628 19:32:25.036737 32050 data_reader.cpp:262] Starting prefetch of epoch 66
I0628 19:32:25.744645 32082 solver.cpp:349] Iteration 51600 (58.0706 iter/s, 1.72204s/100 iter), loss = 0.00779561
I0628 19:32:25.744670 32082 solver.cpp:371]     Train net output #0: loss = 0.00779566 (* 1 = 0.00779566 loss)
I0628 19:32:25.744675 32082 sgd_solver.cpp:137] Iteration 51600, lr = 0.0019375, m = 0.9
I0628 19:32:27.464205 32082 solver.cpp:349] Iteration 51700 (58.1542 iter/s, 1.71957s/100 iter), loss = 0.00430424
I0628 19:32:27.464231 32082 solver.cpp:371]     Train net output #0: loss = 0.00430428 (* 1 = 0.00430428 loss)
I0628 19:32:27.464236 32082 sgd_solver.cpp:137] Iteration 51700, lr = 0.00192187, m = 0.9
I0628 19:32:29.181900 32082 solver.cpp:349] Iteration 51800 (58.2175 iter/s, 1.7177s/100 iter), loss = 0.00764516
I0628 19:32:29.181943 32082 solver.cpp:371]     Train net output #0: loss = 0.0076452 (* 1 = 0.0076452 loss)
I0628 19:32:29.181949 32082 sgd_solver.cpp:137] Iteration 51800, lr = 0.00190625, m = 0.9
I0628 19:32:30.900447 32082 solver.cpp:349] Iteration 51900 (58.1893 iter/s, 1.71853s/100 iter), loss = 0.00733223
I0628 19:32:30.900475 32082 solver.cpp:371]     Train net output #0: loss = 0.00733227 (* 1 = 0.00733227 loss)
I0628 19:32:30.900481 32082 sgd_solver.cpp:137] Iteration 51900, lr = 0.00189062, m = 0.9
I0628 19:32:32.601670 32082 solver.cpp:401] Sparsity after update:
I0628 19:32:32.602761 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:32:32.602768 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:32:32.602777 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:32:32.602782 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:32:32.602785 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:32:32.602790 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:32:32.602793 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:32:32.602797 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:32:32.602800 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:32:32.602804 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:32:32.602808 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:32:32.602811 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:32:32.602815 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:32:32.602828 32082 solver.cpp:545] Iteration 52000, Testing net (#0)
I0628 19:32:33.600270 32080 data_reader.cpp:262] Starting prefetch of epoch 52
I0628 19:32:33.620359 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.91
I0628 19:32:33.620371 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:32:33.620380 32082 solver.cpp:630]     Test net output #2: loss = 0.331645 (* 1 = 0.331645 loss)
I0628 19:32:33.620398 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01787s
I0628 19:32:33.637749 32082 solver.cpp:349] Iteration 52000 (36.5258 iter/s, 2.73779s/100 iter), loss = 0.00607719
I0628 19:32:33.637775 32082 solver.cpp:371]     Train net output #0: loss = 0.00607723 (* 1 = 0.00607723 loss)
I0628 19:32:33.637781 32082 sgd_solver.cpp:137] Iteration 52000, lr = 0.001875, m = 0.9
I0628 19:32:35.355564 32082 solver.cpp:349] Iteration 52100 (58.1978 iter/s, 1.71828s/100 iter), loss = 0.00339066
I0628 19:32:35.355587 32082 solver.cpp:371]     Train net output #0: loss = 0.00339071 (* 1 = 0.00339071 loss)
I0628 19:32:35.355593 32082 sgd_solver.cpp:137] Iteration 52100, lr = 0.00185938, m = 0.9
I0628 19:32:37.072065 32082 solver.cpp:349] Iteration 52200 (58.2424 iter/s, 1.71696s/100 iter), loss = 0.00347627
I0628 19:32:37.072089 32082 solver.cpp:371]     Train net output #0: loss = 0.00347632 (* 1 = 0.00347632 loss)
I0628 19:32:37.072095 32082 sgd_solver.cpp:137] Iteration 52200, lr = 0.00184375, m = 0.9
I0628 19:32:38.788353 32082 solver.cpp:349] Iteration 52300 (58.2498 iter/s, 1.71674s/100 iter), loss = 0.00600969
I0628 19:32:38.788378 32082 solver.cpp:371]     Train net output #0: loss = 0.00600974 (* 1 = 0.00600974 loss)
I0628 19:32:38.788384 32082 sgd_solver.cpp:137] Iteration 52300, lr = 0.00182813, m = 0.9
I0628 19:32:39.477370 32050 data_reader.cpp:262] Starting prefetch of epoch 67
I0628 19:32:40.509044 32082 solver.cpp:349] Iteration 52400 (58.1009 iter/s, 1.72114s/100 iter), loss = 0.00371
I0628 19:32:40.509071 32082 solver.cpp:371]     Train net output #0: loss = 0.00371004 (* 1 = 0.00371004 loss)
I0628 19:32:40.509076 32082 sgd_solver.cpp:137] Iteration 52400, lr = 0.0018125, m = 0.9
I0628 19:32:42.230979 32082 solver.cpp:349] Iteration 52500 (58.0592 iter/s, 1.72238s/100 iter), loss = 0.010918
I0628 19:32:42.231003 32082 solver.cpp:371]     Train net output #0: loss = 0.010918 (* 1 = 0.010918 loss)
I0628 19:32:42.231006 32082 sgd_solver.cpp:137] Iteration 52500, lr = 0.00179687, m = 0.9
I0628 19:32:43.954047 32082 solver.cpp:349] Iteration 52600 (58.0214 iter/s, 1.7235s/100 iter), loss = 0.0080129
I0628 19:32:43.954072 32082 solver.cpp:371]     Train net output #0: loss = 0.00801295 (* 1 = 0.00801295 loss)
I0628 19:32:43.954079 32082 sgd_solver.cpp:137] Iteration 52600, lr = 0.00178125, m = 0.9
I0628 19:32:45.674789 32082 solver.cpp:349] Iteration 52700 (58.0997 iter/s, 1.72118s/100 iter), loss = 0.001916
I0628 19:32:45.674852 32082 solver.cpp:371]     Train net output #0: loss = 0.00191604 (* 1 = 0.00191604 loss)
I0628 19:32:45.674857 32082 sgd_solver.cpp:137] Iteration 52700, lr = 0.00176562, m = 0.9
I0628 19:32:47.397868 32082 solver.cpp:349] Iteration 52800 (58.0223 iter/s, 1.72347s/100 iter), loss = 0.00435365
I0628 19:32:47.397893 32082 solver.cpp:371]     Train net output #0: loss = 0.00435369 (* 1 = 0.00435369 loss)
I0628 19:32:47.397898 32082 sgd_solver.cpp:137] Iteration 52800, lr = 0.00175, m = 0.9
I0628 19:32:49.116621 32082 solver.cpp:349] Iteration 52900 (58.1672 iter/s, 1.71918s/100 iter), loss = 0.00373482
I0628 19:32:49.116647 32082 solver.cpp:371]     Train net output #0: loss = 0.00373487 (* 1 = 0.00373487 loss)
I0628 19:32:49.116652 32082 sgd_solver.cpp:137] Iteration 52900, lr = 0.00173437, m = 0.9
I0628 19:32:50.816567 32082 solver.cpp:401] Sparsity after update:
I0628 19:32:50.817733 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:32:50.817740 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:32:50.817745 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:32:50.817747 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:32:50.817749 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:32:50.817751 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:32:50.817754 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:32:50.817756 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:32:50.817759 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:32:50.817760 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:32:50.817762 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:32:50.817764 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:32:50.817766 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:32:50.817775 32082 solver.cpp:545] Iteration 53000, Testing net (#0)
I0628 19:32:51.813743 32080 data_reader.cpp:262] Starting prefetch of epoch 53
I0628 19:32:51.837363 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9106
I0628 19:32:51.837376 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:32:51.837381 32082 solver.cpp:630]     Test net output #2: loss = 0.331414 (* 1 = 0.331414 loss)
I0628 19:32:51.837396 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01989s
I0628 19:32:51.855115 32082 solver.cpp:349] Iteration 53000 (36.5071 iter/s, 2.73919s/100 iter), loss = 0.00817138
I0628 19:32:51.855140 32082 solver.cpp:371]     Train net output #0: loss = 0.00817143 (* 1 = 0.00817143 loss)
I0628 19:32:51.855146 32082 sgd_solver.cpp:137] Iteration 53000, lr = 0.00171875, m = 0.9
I0628 19:32:53.575655 32082 solver.cpp:349] Iteration 53100 (58.1072 iter/s, 1.72096s/100 iter), loss = 0.00802852
I0628 19:32:53.575677 32082 solver.cpp:371]     Train net output #0: loss = 0.00802857 (* 1 = 0.00802857 loss)
I0628 19:32:53.575681 32082 sgd_solver.cpp:137] Iteration 53100, lr = 0.00170313, m = 0.9
I0628 19:32:53.936532 32050 data_reader.cpp:262] Starting prefetch of epoch 68
I0628 19:32:55.293876 32082 solver.cpp:349] Iteration 53200 (58.1857 iter/s, 1.71864s/100 iter), loss = 0.00424734
I0628 19:32:55.293901 32082 solver.cpp:371]     Train net output #0: loss = 0.00424739 (* 1 = 0.00424739 loss)
I0628 19:32:55.293907 32082 sgd_solver.cpp:137] Iteration 53200, lr = 0.0016875, m = 0.9
I0628 19:32:57.018399 32082 solver.cpp:349] Iteration 53300 (57.9734 iter/s, 1.72493s/100 iter), loss = 0.00652922
I0628 19:32:57.018420 32082 solver.cpp:371]     Train net output #0: loss = 0.00652927 (* 1 = 0.00652927 loss)
I0628 19:32:57.018424 32082 sgd_solver.cpp:137] Iteration 53300, lr = 0.00167188, m = 0.9
I0628 19:32:58.737228 32082 solver.cpp:349] Iteration 53400 (58.1653 iter/s, 1.71924s/100 iter), loss = 0.00406245
I0628 19:32:58.737254 32082 solver.cpp:371]     Train net output #0: loss = 0.00406249 (* 1 = 0.00406249 loss)
I0628 19:32:58.737260 32082 sgd_solver.cpp:137] Iteration 53400, lr = 0.00165625, m = 0.9
I0628 19:33:00.453879 32082 solver.cpp:349] Iteration 53500 (58.2396 iter/s, 1.71704s/100 iter), loss = 0.00591737
I0628 19:33:00.453920 32082 solver.cpp:371]     Train net output #0: loss = 0.00591742 (* 1 = 0.00591742 loss)
I0628 19:33:00.453927 32082 sgd_solver.cpp:137] Iteration 53500, lr = 0.00164062, m = 0.9
I0628 19:33:02.174006 32082 solver.cpp:349] Iteration 53600 (58.1226 iter/s, 1.7205s/100 iter), loss = 0.00766969
I0628 19:33:02.174032 32082 solver.cpp:371]     Train net output #0: loss = 0.00766973 (* 1 = 0.00766973 loss)
I0628 19:33:02.174038 32082 sgd_solver.cpp:137] Iteration 53600, lr = 0.001625, m = 0.9
I0628 19:33:03.890817 32082 solver.cpp:349] Iteration 53700 (58.2345 iter/s, 1.7172s/100 iter), loss = 0.00600879
I0628 19:33:03.890843 32082 solver.cpp:371]     Train net output #0: loss = 0.00600883 (* 1 = 0.00600883 loss)
I0628 19:33:03.890848 32082 sgd_solver.cpp:137] Iteration 53700, lr = 0.00160937, m = 0.9
I0628 19:33:05.610847 32082 solver.cpp:349] Iteration 53800 (58.1256 iter/s, 1.72041s/100 iter), loss = 0.00573171
I0628 19:33:05.610872 32082 solver.cpp:371]     Train net output #0: loss = 0.00573176 (* 1 = 0.00573176 loss)
I0628 19:33:05.610877 32082 sgd_solver.cpp:137] Iteration 53800, lr = 0.00159375, m = 0.9
I0628 19:33:07.328786 32082 solver.cpp:349] Iteration 53900 (58.1965 iter/s, 1.71832s/100 iter), loss = 0.00416038
I0628 19:33:07.328809 32082 solver.cpp:371]     Train net output #0: loss = 0.00416043 (* 1 = 0.00416043 loss)
I0628 19:33:07.328814 32082 sgd_solver.cpp:137] Iteration 53900, lr = 0.00157812, m = 0.9
I0628 19:33:07.380633 32050 data_reader.cpp:262] Starting prefetch of epoch 69
I0628 19:33:09.030648 32082 solver.cpp:401] Sparsity after update:
I0628 19:33:09.031774 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:33:09.031781 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:33:09.031790 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:33:09.031791 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:33:09.031793 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:33:09.031796 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:33:09.031798 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:33:09.031800 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:33:09.031802 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:33:09.031805 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:33:09.031807 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:33:09.031810 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:33:09.031811 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:33:09.031818 32082 solver.cpp:545] Iteration 54000, Testing net (#0)
I0628 19:33:10.032057 32080 data_reader.cpp:262] Starting prefetch of epoch 54
I0628 19:33:10.052670 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9084
I0628 19:33:10.052682 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:33:10.052687 32082 solver.cpp:630]     Test net output #2: loss = 0.331596 (* 1 = 0.331596 loss)
I0628 19:33:10.052700 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02112s
I0628 19:33:10.070014 32082 solver.cpp:349] Iteration 54000 (36.4717 iter/s, 2.74185s/100 iter), loss = 0.00854644
I0628 19:33:10.070040 32082 solver.cpp:371]     Train net output #0: loss = 0.00854648 (* 1 = 0.00854648 loss)
I0628 19:33:10.070046 32082 sgd_solver.cpp:137] Iteration 54000, lr = 0.0015625, m = 0.9
I0628 19:33:11.791560 32082 solver.cpp:349] Iteration 54100 (58.075 iter/s, 1.72191s/100 iter), loss = 0.00860527
I0628 19:33:11.791585 32082 solver.cpp:371]     Train net output #0: loss = 0.00860532 (* 1 = 0.00860532 loss)
I0628 19:33:11.791591 32082 sgd_solver.cpp:137] Iteration 54100, lr = 0.00154688, m = 0.9
I0628 19:33:13.508347 32082 solver.cpp:349] Iteration 54200 (58.2361 iter/s, 1.71715s/100 iter), loss = 0.0046466
I0628 19:33:13.508368 32082 solver.cpp:371]     Train net output #0: loss = 0.00464664 (* 1 = 0.00464664 loss)
I0628 19:33:13.508373 32082 sgd_solver.cpp:137] Iteration 54200, lr = 0.00153125, m = 0.9
I0628 19:33:15.224827 32082 solver.cpp:349] Iteration 54300 (58.2472 iter/s, 1.71682s/100 iter), loss = 0.00634444
I0628 19:33:15.224848 32082 solver.cpp:371]     Train net output #0: loss = 0.00634448 (* 1 = 0.00634448 loss)
I0628 19:33:15.224854 32082 sgd_solver.cpp:137] Iteration 54300, lr = 0.00151563, m = 0.9
I0628 19:33:16.949244 32082 solver.cpp:349] Iteration 54400 (57.9786 iter/s, 1.72478s/100 iter), loss = 0.00482436
I0628 19:33:16.949334 32082 solver.cpp:371]     Train net output #0: loss = 0.0048244 (* 1 = 0.0048244 loss)
I0628 19:33:16.949340 32082 sgd_solver.cpp:137] Iteration 54400, lr = 0.0015, m = 0.9
I0628 19:33:18.667095 32082 solver.cpp:349] Iteration 54500 (58.2027 iter/s, 1.71813s/100 iter), loss = 0.00403198
I0628 19:33:18.667122 32082 solver.cpp:371]     Train net output #0: loss = 0.00403203 (* 1 = 0.00403203 loss)
I0628 19:33:18.667129 32082 sgd_solver.cpp:137] Iteration 54500, lr = 0.00148437, m = 0.9
I0628 19:33:20.386566 32082 solver.cpp:349] Iteration 54600 (58.1459 iter/s, 1.71981s/100 iter), loss = 0.00438608
I0628 19:33:20.386590 32082 solver.cpp:371]     Train net output #0: loss = 0.00438613 (* 1 = 0.00438613 loss)
I0628 19:33:20.386595 32082 sgd_solver.cpp:137] Iteration 54600, lr = 0.00146875, m = 0.9
I0628 19:33:21.831696 32050 data_reader.cpp:262] Starting prefetch of epoch 70
I0628 19:33:22.106022 32082 solver.cpp:349] Iteration 54700 (58.1464 iter/s, 1.7198s/100 iter), loss = 0.00291828
I0628 19:33:22.106048 32082 solver.cpp:371]     Train net output #0: loss = 0.00291832 (* 1 = 0.00291832 loss)
I0628 19:33:22.106055 32082 sgd_solver.cpp:137] Iteration 54700, lr = 0.00145312, m = 0.9
I0628 19:33:23.828219 32082 solver.cpp:349] Iteration 54800 (58.054 iter/s, 1.72253s/100 iter), loss = 0.0102088
I0628 19:33:23.828243 32082 solver.cpp:371]     Train net output #0: loss = 0.0102089 (* 1 = 0.0102089 loss)
I0628 19:33:23.828246 32082 sgd_solver.cpp:137] Iteration 54800, lr = 0.0014375, m = 0.9
I0628 19:33:25.544909 32082 solver.cpp:349] Iteration 54900 (58.2402 iter/s, 1.71703s/100 iter), loss = 0.00365792
I0628 19:33:25.544935 32082 solver.cpp:371]     Train net output #0: loss = 0.00365796 (* 1 = 0.00365796 loss)
I0628 19:33:25.544941 32082 sgd_solver.cpp:137] Iteration 54900, lr = 0.00142187, m = 0.9
I0628 19:33:27.246131 32082 solver.cpp:401] Sparsity after update:
I0628 19:33:27.247216 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:33:27.247223 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:33:27.247229 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:33:27.247231 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:33:27.247234 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:33:27.247236 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:33:27.247238 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:33:27.247241 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:33:27.247242 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:33:27.247244 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:33:27.247246 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:33:27.247248 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:33:27.247251 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:33:27.247256 32082 solver.cpp:545] Iteration 55000, Testing net (#0)
I0628 19:33:28.242290 32080 data_reader.cpp:262] Starting prefetch of epoch 55
I0628 19:33:28.266381 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.908
I0628 19:33:28.266394 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:33:28.266398 32082 solver.cpp:630]     Test net output #2: loss = 0.333397 (* 1 = 0.333397 loss)
I0628 19:33:28.266413 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01937s
I0628 19:33:28.283821 32082 solver.cpp:349] Iteration 55000 (36.5035 iter/s, 2.73946s/100 iter), loss = 0.00335355
I0628 19:33:28.283844 32082 solver.cpp:371]     Train net output #0: loss = 0.0033536 (* 1 = 0.0033536 loss)
I0628 19:33:28.283850 32082 sgd_solver.cpp:137] Iteration 55000, lr = 0.00140625, m = 0.9
I0628 19:33:30.004431 32082 solver.cpp:349] Iteration 55100 (58.1079 iter/s, 1.72094s/100 iter), loss = 0.00423073
I0628 19:33:30.004453 32082 solver.cpp:371]     Train net output #0: loss = 0.00423077 (* 1 = 0.00423077 loss)
I0628 19:33:30.004457 32082 sgd_solver.cpp:137] Iteration 55100, lr = 0.00139063, m = 0.9
I0628 19:33:31.726378 32082 solver.cpp:349] Iteration 55200 (58.0629 iter/s, 1.72227s/100 iter), loss = 0.0089226
I0628 19:33:31.726420 32082 solver.cpp:371]     Train net output #0: loss = 0.00892264 (* 1 = 0.00892264 loss)
I0628 19:33:31.726426 32082 sgd_solver.cpp:137] Iteration 55200, lr = 0.001375, m = 0.9
I0628 19:33:33.446234 32082 solver.cpp:349] Iteration 55300 (58.1345 iter/s, 1.72015s/100 iter), loss = 0.00865376
I0628 19:33:33.446260 32082 solver.cpp:371]     Train net output #0: loss = 0.0086538 (* 1 = 0.0086538 loss)
I0628 19:33:33.446267 32082 sgd_solver.cpp:137] Iteration 55300, lr = 0.00135938, m = 0.9
I0628 19:33:35.160712 32082 solver.cpp:349] Iteration 55400 (58.3165 iter/s, 1.71478s/100 iter), loss = 0.0122648
I0628 19:33:35.160737 32082 solver.cpp:371]     Train net output #0: loss = 0.0122648 (* 1 = 0.0122648 loss)
I0628 19:33:35.160742 32082 sgd_solver.cpp:137] Iteration 55400, lr = 0.00134375, m = 0.9
I0628 19:33:36.277487 32050 data_reader.cpp:262] Starting prefetch of epoch 71
I0628 19:33:36.878252 32082 solver.cpp:349] Iteration 55500 (58.2125 iter/s, 1.71784s/100 iter), loss = 0.00849211
I0628 19:33:36.878276 32082 solver.cpp:371]     Train net output #0: loss = 0.00849216 (* 1 = 0.00849216 loss)
I0628 19:33:36.878281 32082 sgd_solver.cpp:137] Iteration 55500, lr = 0.00132813, m = 0.9
I0628 19:33:38.597296 32082 solver.cpp:349] Iteration 55600 (58.1616 iter/s, 1.71935s/100 iter), loss = 0.00394615
I0628 19:33:38.597321 32082 solver.cpp:371]     Train net output #0: loss = 0.0039462 (* 1 = 0.0039462 loss)
I0628 19:33:38.597326 32082 sgd_solver.cpp:137] Iteration 55600, lr = 0.0013125, m = 0.9
I0628 19:33:40.315601 32082 solver.cpp:349] Iteration 55700 (58.1861 iter/s, 1.71862s/100 iter), loss = 0.00622475
I0628 19:33:40.315625 32082 solver.cpp:371]     Train net output #0: loss = 0.00622479 (* 1 = 0.00622479 loss)
I0628 19:33:40.315630 32082 sgd_solver.cpp:137] Iteration 55700, lr = 0.00129687, m = 0.9
I0628 19:33:42.046097 32082 solver.cpp:349] Iteration 55800 (57.7762 iter/s, 1.73082s/100 iter), loss = 0.00557415
I0628 19:33:42.046120 32082 solver.cpp:371]     Train net output #0: loss = 0.0055742 (* 1 = 0.0055742 loss)
I0628 19:33:42.046128 32082 sgd_solver.cpp:137] Iteration 55800, lr = 0.00128125, m = 0.9
I0628 19:33:43.763483 32082 solver.cpp:349] Iteration 55900 (58.2175 iter/s, 1.7177s/100 iter), loss = 0.00502246
I0628 19:33:43.763509 32082 solver.cpp:371]     Train net output #0: loss = 0.00502251 (* 1 = 0.00502251 loss)
I0628 19:33:43.763515 32082 sgd_solver.cpp:137] Iteration 55900, lr = 0.00126562, m = 0.9
I0628 19:33:45.467501 32082 solver.cpp:401] Sparsity after update:
I0628 19:33:45.468544 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:33:45.468551 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:33:45.468559 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:33:45.468564 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:33:45.468566 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:33:45.468569 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:33:45.468574 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:33:45.468577 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:33:45.468581 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:33:45.468585 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:33:45.468588 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:33:45.468592 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:33:45.468597 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:33:45.468607 32082 solver.cpp:545] Iteration 56000, Testing net (#0)
I0628 19:33:46.463043 32080 data_reader.cpp:262] Starting prefetch of epoch 56
I0628 19:33:46.489207 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9108
I0628 19:33:46.489223 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:33:46.489230 32082 solver.cpp:630]     Test net output #2: loss = 0.331827 (* 1 = 0.331827 loss)
I0628 19:33:46.489248 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02084s
I0628 19:33:46.506772 32082 solver.cpp:349] Iteration 56000 (36.4457 iter/s, 2.74381s/100 iter), loss = 0.00610609
I0628 19:33:46.506791 32082 solver.cpp:371]     Train net output #0: loss = 0.00610613 (* 1 = 0.00610613 loss)
I0628 19:33:46.506796 32082 sgd_solver.cpp:137] Iteration 56000, lr = 0.00125, m = 0.9
I0628 19:33:48.225307 32082 solver.cpp:349] Iteration 56100 (58.1786 iter/s, 1.71884s/100 iter), loss = 0.0059128
I0628 19:33:48.225378 32082 solver.cpp:371]     Train net output #0: loss = 0.00591285 (* 1 = 0.00591285 loss)
I0628 19:33:48.225391 32082 sgd_solver.cpp:137] Iteration 56100, lr = 0.00123438, m = 0.9
I0628 19:33:49.941817 32082 solver.cpp:349] Iteration 56200 (58.2497 iter/s, 1.71675s/100 iter), loss = 0.00310108
I0628 19:33:49.941843 32082 solver.cpp:371]     Train net output #0: loss = 0.00310112 (* 1 = 0.00310112 loss)
I0628 19:33:49.941849 32082 sgd_solver.cpp:137] Iteration 56200, lr = 0.00121875, m = 0.9
I0628 19:33:50.733929 32050 data_reader.cpp:262] Starting prefetch of epoch 72
I0628 19:33:51.661631 32082 solver.cpp:349] Iteration 56300 (58.1359 iter/s, 1.72011s/100 iter), loss = 0.00477077
I0628 19:33:51.661658 32082 solver.cpp:371]     Train net output #0: loss = 0.00477082 (* 1 = 0.00477082 loss)
I0628 19:33:51.661664 32082 sgd_solver.cpp:137] Iteration 56300, lr = 0.00120313, m = 0.9
I0628 19:33:53.388430 32082 solver.cpp:349] Iteration 56400 (57.9009 iter/s, 1.72709s/100 iter), loss = 0.00270256
I0628 19:33:53.388451 32082 solver.cpp:371]     Train net output #0: loss = 0.00270261 (* 1 = 0.00270261 loss)
I0628 19:33:53.388458 32082 sgd_solver.cpp:137] Iteration 56400, lr = 0.0011875, m = 0.9
I0628 19:33:55.108036 32082 solver.cpp:349] Iteration 56500 (58.143 iter/s, 1.7199s/100 iter), loss = 0.00875451
I0628 19:33:55.108062 32082 solver.cpp:371]     Train net output #0: loss = 0.00875456 (* 1 = 0.00875456 loss)
I0628 19:33:55.108067 32082 sgd_solver.cpp:137] Iteration 56500, lr = 0.00117187, m = 0.9
I0628 19:33:56.824056 32082 solver.cpp:349] Iteration 56600 (58.2648 iter/s, 1.7163s/100 iter), loss = 0.00605158
I0628 19:33:56.824082 32082 solver.cpp:371]     Train net output #0: loss = 0.00605163 (* 1 = 0.00605163 loss)
I0628 19:33:56.824087 32082 sgd_solver.cpp:137] Iteration 56600, lr = 0.00115625, m = 0.9
I0628 19:33:58.541224 32082 solver.cpp:349] Iteration 56700 (58.2261 iter/s, 1.71744s/100 iter), loss = 0.00180055
I0628 19:33:58.541249 32082 solver.cpp:371]     Train net output #0: loss = 0.0018006 (* 1 = 0.0018006 loss)
I0628 19:33:58.541255 32082 sgd_solver.cpp:137] Iteration 56700, lr = 0.00114062, m = 0.9
I0628 19:34:00.263839 32082 solver.cpp:349] Iteration 56800 (58.042 iter/s, 1.72289s/100 iter), loss = 0.00327673
I0628 19:34:00.263861 32082 solver.cpp:371]     Train net output #0: loss = 0.00327678 (* 1 = 0.00327678 loss)
I0628 19:34:00.263865 32082 sgd_solver.cpp:137] Iteration 56800, lr = 0.001125, m = 0.9
I0628 19:34:01.981704 32082 solver.cpp:349] Iteration 56900 (58.2023 iter/s, 1.71814s/100 iter), loss = 0.0072868
I0628 19:34:01.981725 32082 solver.cpp:371]     Train net output #0: loss = 0.00728685 (* 1 = 0.00728685 loss)
I0628 19:34:01.981729 32082 sgd_solver.cpp:137] Iteration 56900, lr = 0.00110937, m = 0.9
I0628 19:34:03.681535 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:03.682577 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:03.682585 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:03.682595 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:03.682600 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:03.682603 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:03.682607 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:03.682611 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:03.682615 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:03.682618 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:03.682622 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:03.682626 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:03.682631 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:03.682633 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:03.682644 32082 solver.cpp:545] Iteration 57000, Testing net (#0)
I0628 19:34:04.676272 32080 data_reader.cpp:262] Starting prefetch of epoch 57
I0628 19:34:04.703379 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.909
I0628 19:34:04.703397 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:34:04.703418 32082 solver.cpp:630]     Test net output #2: loss = 0.332614 (* 1 = 0.332614 loss)
I0628 19:34:04.703436 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02097s
I0628 19:34:04.721128 32082 solver.cpp:349] Iteration 57000 (36.4979 iter/s, 2.73988s/100 iter), loss = 0.00524302
I0628 19:34:04.721148 32082 solver.cpp:371]     Train net output #0: loss = 0.00524307 (* 1 = 0.00524307 loss)
I0628 19:34:04.721153 32082 sgd_solver.cpp:137] Iteration 57000, lr = 0.00109375, m = 0.9
I0628 19:34:05.202630 32050 data_reader.cpp:262] Starting prefetch of epoch 73
I0628 19:34:06.440837 32082 solver.cpp:349] Iteration 57100 (58.1403 iter/s, 1.71998s/100 iter), loss = 0.00144296
I0628 19:34:06.440860 32082 solver.cpp:371]     Train net output #0: loss = 0.001443 (* 1 = 0.001443 loss)
I0628 19:34:06.440865 32082 sgd_solver.cpp:137] Iteration 57100, lr = 0.00107813, m = 0.9
I0628 19:34:08.161502 32082 solver.cpp:349] Iteration 57200 (58.1082 iter/s, 1.72093s/100 iter), loss = 0.00379741
I0628 19:34:08.161523 32082 solver.cpp:371]     Train net output #0: loss = 0.00379746 (* 1 = 0.00379746 loss)
I0628 19:34:08.161527 32082 sgd_solver.cpp:137] Iteration 57200, lr = 0.0010625, m = 0.9
I0628 19:34:09.879668 32082 solver.cpp:349] Iteration 57300 (58.1927 iter/s, 1.71843s/100 iter), loss = 0.0110087
I0628 19:34:09.879695 32082 solver.cpp:371]     Train net output #0: loss = 0.0110087 (* 1 = 0.0110087 loss)
I0628 19:34:09.879701 32082 sgd_solver.cpp:137] Iteration 57300, lr = 0.00104688, m = 0.9
I0628 19:34:11.597204 32082 solver.cpp:349] Iteration 57400 (58.2145 iter/s, 1.71779s/100 iter), loss = 0.00756564
I0628 19:34:11.597229 32082 solver.cpp:371]     Train net output #0: loss = 0.00756569 (* 1 = 0.00756569 loss)
I0628 19:34:11.597235 32082 sgd_solver.cpp:137] Iteration 57400, lr = 0.00103125, m = 0.9
I0628 19:34:13.313872 32082 solver.cpp:349] Iteration 57500 (58.244 iter/s, 1.71692s/100 iter), loss = 0.0108518
I0628 19:34:13.313899 32082 solver.cpp:371]     Train net output #0: loss = 0.0108518 (* 1 = 0.0108518 loss)
I0628 19:34:13.313905 32082 sgd_solver.cpp:137] Iteration 57500, lr = 0.00101562, m = 0.9
I0628 19:34:15.032825 32082 solver.cpp:349] Iteration 57600 (58.1667 iter/s, 1.7192s/100 iter), loss = 0.00660956
I0628 19:34:15.032845 32082 solver.cpp:371]     Train net output #0: loss = 0.0066096 (* 1 = 0.0066096 loss)
I0628 19:34:15.032850 32082 sgd_solver.cpp:137] Iteration 57600, lr = 0.001, m = 0.9
I0628 19:34:16.750895 32082 solver.cpp:349] Iteration 57700 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00581368
I0628 19:34:16.750917 32082 solver.cpp:371]     Train net output #0: loss = 0.00581373 (* 1 = 0.00581373 loss)
I0628 19:34:16.750922 32082 sgd_solver.cpp:137] Iteration 57700, lr = 0.000984375, m = 0.9
I0628 19:34:18.468333 32082 solver.cpp:349] Iteration 57800 (58.2181 iter/s, 1.71768s/100 iter), loss = 0.00566295
I0628 19:34:18.468406 32082 solver.cpp:371]     Train net output #0: loss = 0.00566299 (* 1 = 0.00566299 loss)
I0628 19:34:18.468413 32082 sgd_solver.cpp:137] Iteration 57800, lr = 0.00096875, m = 0.9
I0628 19:34:18.623225 32050 data_reader.cpp:262] Starting prefetch of epoch 74
I0628 19:34:20.184119 32082 solver.cpp:349] Iteration 57900 (58.2762 iter/s, 1.71597s/100 iter), loss = 0.003471
I0628 19:34:20.184146 32082 solver.cpp:371]     Train net output #0: loss = 0.00347105 (* 1 = 0.00347105 loss)
I0628 19:34:20.184152 32082 sgd_solver.cpp:137] Iteration 57900, lr = 0.000953125, m = 0.9
I0628 19:34:21.885618 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:21.886695 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:21.886703 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:21.886710 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:21.886713 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:21.886715 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:21.886718 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:21.886720 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:21.886723 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:21.886724 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:21.886726 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:21.886729 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:21.886730 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:21.886732 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:21.886739 32082 solver.cpp:545] Iteration 58000, Testing net (#0)
I0628 19:34:22.883785 32080 data_reader.cpp:262] Starting prefetch of epoch 58
I0628 19:34:22.905979 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9084
I0628 19:34:22.905992 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:34:22.905997 32082 solver.cpp:630]     Test net output #2: loss = 0.332114 (* 1 = 0.332114 loss)
I0628 19:34:22.906010 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01943s
I0628 19:34:22.923408 32082 solver.cpp:349] Iteration 58000 (36.5006 iter/s, 2.73968s/100 iter), loss = 0.00294096
I0628 19:34:22.923430 32082 solver.cpp:371]     Train net output #0: loss = 0.00294101 (* 1 = 0.00294101 loss)
I0628 19:34:22.923434 32082 sgd_solver.cpp:137] Iteration 58000, lr = 0.0009375, m = 0.9
I0628 19:34:24.639655 32082 solver.cpp:349] Iteration 58100 (58.2588 iter/s, 1.71648s/100 iter), loss = 0.00501284
I0628 19:34:24.639680 32082 solver.cpp:371]     Train net output #0: loss = 0.00501289 (* 1 = 0.00501289 loss)
I0628 19:34:24.639685 32082 sgd_solver.cpp:137] Iteration 58100, lr = 0.000921875, m = 0.9
I0628 19:34:26.354631 32082 solver.cpp:349] Iteration 58200 (58.3023 iter/s, 1.7152s/100 iter), loss = 0.00332411
I0628 19:34:26.354656 32082 solver.cpp:371]     Train net output #0: loss = 0.00332416 (* 1 = 0.00332416 loss)
I0628 19:34:26.354662 32082 sgd_solver.cpp:137] Iteration 58200, lr = 0.00090625, m = 0.9
I0628 19:34:28.075840 32082 solver.cpp:349] Iteration 58300 (58.0913 iter/s, 1.72143s/100 iter), loss = 0.0042586
I0628 19:34:28.075865 32082 solver.cpp:371]     Train net output #0: loss = 0.00425865 (* 1 = 0.00425865 loss)
I0628 19:34:28.075870 32082 sgd_solver.cpp:137] Iteration 58300, lr = 0.000890625, m = 0.9
I0628 19:34:29.794306 32082 solver.cpp:349] Iteration 58400 (58.1842 iter/s, 1.71868s/100 iter), loss = 0.00437587
I0628 19:34:29.794328 32082 solver.cpp:371]     Train net output #0: loss = 0.00437593 (* 1 = 0.00437593 loss)
I0628 19:34:29.794332 32082 sgd_solver.cpp:137] Iteration 58400, lr = 0.000875, m = 0.9
I0628 19:34:31.516248 32082 solver.cpp:349] Iteration 58500 (58.0667 iter/s, 1.72216s/100 iter), loss = 0.00467773
I0628 19:34:31.516273 32082 solver.cpp:371]     Train net output #0: loss = 0.00467778 (* 1 = 0.00467778 loss)
I0628 19:34:31.516278 32082 sgd_solver.cpp:137] Iteration 58500, lr = 0.000859375, m = 0.9
I0628 19:34:33.060557 32050 data_reader.cpp:262] Starting prefetch of epoch 75
I0628 19:34:33.232120 32082 solver.cpp:349] Iteration 58600 (58.2723 iter/s, 1.71608s/100 iter), loss = 0.00333279
I0628 19:34:33.232143 32082 solver.cpp:371]     Train net output #0: loss = 0.00333284 (* 1 = 0.00333284 loss)
I0628 19:34:33.232148 32082 sgd_solver.cpp:137] Iteration 58600, lr = 0.00084375, m = 0.9
I0628 19:34:34.947851 32082 solver.cpp:349] Iteration 58700 (58.2771 iter/s, 1.71594s/100 iter), loss = 0.0075526
I0628 19:34:34.947872 32082 solver.cpp:371]     Train net output #0: loss = 0.00755265 (* 1 = 0.00755265 loss)
I0628 19:34:34.947876 32082 sgd_solver.cpp:137] Iteration 58700, lr = 0.000828125, m = 0.9
I0628 19:34:36.665683 32082 solver.cpp:349] Iteration 58800 (58.2058 iter/s, 1.71804s/100 iter), loss = 0.00238037
I0628 19:34:36.665709 32082 solver.cpp:371]     Train net output #0: loss = 0.00238043 (* 1 = 0.00238043 loss)
I0628 19:34:36.665714 32082 sgd_solver.cpp:137] Iteration 58800, lr = 0.0008125, m = 0.9
I0628 19:34:38.383692 32082 solver.cpp:349] Iteration 58900 (58.2002 iter/s, 1.71821s/100 iter), loss = 0.0049039
I0628 19:34:38.383718 32082 solver.cpp:371]     Train net output #0: loss = 0.00490396 (* 1 = 0.00490396 loss)
I0628 19:34:38.383723 32082 sgd_solver.cpp:137] Iteration 58900, lr = 0.000796875, m = 0.9
I0628 19:34:40.086629 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:40.087741 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:40.087749 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:40.087759 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:40.087764 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:40.087767 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:40.087771 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:40.087775 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:40.087779 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:40.087783 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:40.087787 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:40.087791 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:40.087795 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:40.087800 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:40.087810 32082 solver.cpp:545] Iteration 59000, Testing net (#0)
I0628 19:34:41.081414 32080 data_reader.cpp:262] Starting prefetch of epoch 59
I0628 19:34:41.105538 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9104
I0628 19:34:41.105557 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:34:41.105564 32082 solver.cpp:630]     Test net output #2: loss = 0.333245 (* 1 = 0.333245 loss)
I0628 19:34:41.105581 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01791s
I0628 19:34:41.123003 32082 solver.cpp:349] Iteration 59000 (36.5011 iter/s, 2.73965s/100 iter), loss = 0.0050142
I0628 19:34:41.123029 32082 solver.cpp:371]     Train net output #0: loss = 0.00501425 (* 1 = 0.00501425 loss)
I0628 19:34:41.123034 32082 sgd_solver.cpp:137] Iteration 59000, lr = 0.00078125, m = 0.9
I0628 19:34:42.846156 32082 solver.cpp:349] Iteration 59100 (58.0267 iter/s, 1.72334s/100 iter), loss = 0.00376047
I0628 19:34:42.846179 32082 solver.cpp:371]     Train net output #0: loss = 0.00376052 (* 1 = 0.00376052 loss)
I0628 19:34:42.846184 32082 sgd_solver.cpp:137] Iteration 59100, lr = 0.000765625, m = 0.9
I0628 19:34:44.566047 32082 solver.cpp:349] Iteration 59200 (58.1367 iter/s, 1.72008s/100 iter), loss = 0.00368143
I0628 19:34:44.566072 32082 solver.cpp:371]     Train net output #0: loss = 0.00368148 (* 1 = 0.00368148 loss)
I0628 19:34:44.566077 32082 sgd_solver.cpp:137] Iteration 59200, lr = 0.00075, m = 0.9
I0628 19:34:46.286880 32082 solver.cpp:349] Iteration 59300 (58.1052 iter/s, 1.72102s/100 iter), loss = 0.00295771
I0628 19:34:46.286906 32082 solver.cpp:371]     Train net output #0: loss = 0.00295776 (* 1 = 0.00295776 loss)
I0628 19:34:46.286911 32082 sgd_solver.cpp:137] Iteration 59300, lr = 0.000734375, m = 0.9
I0628 19:34:47.506872 32050 data_reader.cpp:262] Starting prefetch of epoch 76
I0628 19:34:48.005471 32082 solver.cpp:349] Iteration 59400 (58.1816 iter/s, 1.71876s/100 iter), loss = 0.0074082
I0628 19:34:48.005496 32082 solver.cpp:371]     Train net output #0: loss = 0.00740826 (* 1 = 0.00740826 loss)
I0628 19:34:48.005501 32082 sgd_solver.cpp:137] Iteration 59400, lr = 0.00071875, m = 0.9
I0628 19:34:49.723048 32082 solver.cpp:349] Iteration 59500 (58.2155 iter/s, 1.71776s/100 iter), loss = 0.00655531
I0628 19:34:49.723120 32082 solver.cpp:371]     Train net output #0: loss = 0.00655537 (* 1 = 0.00655537 loss)
I0628 19:34:49.723126 32082 sgd_solver.cpp:137] Iteration 59500, lr = 0.000703125, m = 0.9
I0628 19:34:51.440656 32082 solver.cpp:349] Iteration 59600 (58.2162 iter/s, 1.71773s/100 iter), loss = 0.00403642
I0628 19:34:51.440677 32082 solver.cpp:371]     Train net output #0: loss = 0.00403647 (* 1 = 0.00403647 loss)
I0628 19:34:51.440683 32082 sgd_solver.cpp:137] Iteration 59600, lr = 0.0006875, m = 0.9
I0628 19:34:53.161468 32082 solver.cpp:349] Iteration 59700 (58.1062 iter/s, 1.72099s/100 iter), loss = 0.0052365
I0628 19:34:53.161492 32082 solver.cpp:371]     Train net output #0: loss = 0.00523656 (* 1 = 0.00523656 loss)
I0628 19:34:53.161497 32082 sgd_solver.cpp:137] Iteration 59700, lr = 0.000671875, m = 0.9
I0628 19:34:54.881597 32082 solver.cpp:349] Iteration 59800 (58.1294 iter/s, 1.7203s/100 iter), loss = 0.00456959
I0628 19:34:54.881619 32082 solver.cpp:371]     Train net output #0: loss = 0.00456964 (* 1 = 0.00456964 loss)
I0628 19:34:54.881623 32082 sgd_solver.cpp:137] Iteration 59800, lr = 0.00065625, m = 0.9
I0628 19:34:56.611656 32082 solver.cpp:349] Iteration 59900 (57.7957 iter/s, 1.73023s/100 iter), loss = 0.00508308
I0628 19:34:56.611680 32082 solver.cpp:371]     Train net output #0: loss = 0.00508314 (* 1 = 0.00508314 loss)
I0628 19:34:56.611686 32082 sgd_solver.cpp:137] Iteration 59900, lr = 0.000640625, m = 0.9
I0628 19:34:58.311800 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0628 19:34:58.319614 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0628 19:34:58.323061 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:58.324194 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:58.324203 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:58.324209 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:58.324213 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:58.324214 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:58.324216 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:58.324219 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:58.324221 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:58.324223 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:58.324225 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:58.324228 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:58.324229 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:58.324232 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:58.324240 32082 solver.cpp:545] Iteration 60000, Testing net (#0)
I0628 19:34:59.325350 32080 data_reader.cpp:262] Starting prefetch of epoch 60
I0628 19:34:59.345502 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9096
I0628 19:34:59.345515 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:34:59.345520 32082 solver.cpp:630]     Test net output #2: loss = 0.332563 (* 1 = 0.332563 loss)
I0628 19:34:59.345532 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02141s
I0628 19:34:59.362996 32082 solver.cpp:349] Iteration 60000 (36.3421 iter/s, 2.75163s/100 iter), loss = 0.00193262
I0628 19:34:59.363020 32082 solver.cpp:371]     Train net output #0: loss = 0.00193268 (* 1 = 0.00193268 loss)
I0628 19:34:59.363024 32082 sgd_solver.cpp:137] Iteration 60000, lr = 0.000625, m = 0.9
I0628 19:35:01.079515 32082 solver.cpp:349] Iteration 60100 (58.252 iter/s, 1.71668s/100 iter), loss = 0.00833237
I0628 19:35:01.079540 32082 solver.cpp:371]     Train net output #0: loss = 0.00833242 (* 1 = 0.00833242 loss)
I0628 19:35:01.079546 32082 sgd_solver.cpp:137] Iteration 60100, lr = 0.000609375, m = 0.9
I0628 19:35:01.989837 32050 data_reader.cpp:262] Starting prefetch of epoch 77
I0628 19:35:02.797485 32082 solver.cpp:349] Iteration 60200 (58.203 iter/s, 1.71812s/100 iter), loss = 0.00399826
I0628 19:35:02.797513 32082 solver.cpp:371]     Train net output #0: loss = 0.00399831 (* 1 = 0.00399831 loss)
I0628 19:35:02.797518 32082 sgd_solver.cpp:137] Iteration 60200, lr = 0.00059375, m = 0.9
I0628 19:35:04.516842 32082 solver.cpp:349] Iteration 60300 (58.1562 iter/s, 1.71951s/100 iter), loss = 0.0044278
I0628 19:35:04.516866 32082 solver.cpp:371]     Train net output #0: loss = 0.00442785 (* 1 = 0.00442785 loss)
I0628 19:35:04.516872 32082 sgd_solver.cpp:137] Iteration 60300, lr = 0.000578125, m = 0.9
I0628 19:35:06.240447 32082 solver.cpp:349] Iteration 60400 (58.0129 iter/s, 1.72375s/100 iter), loss = 0.00541626
I0628 19:35:06.240473 32082 solver.cpp:371]     Train net output #0: loss = 0.00541632 (* 1 = 0.00541632 loss)
I0628 19:35:06.240479 32082 sgd_solver.cpp:137] Iteration 60400, lr = 0.0005625, m = 0.9
I0628 19:35:07.963264 32082 solver.cpp:349] Iteration 60500 (58.0395 iter/s, 1.72296s/100 iter), loss = 0.00467051
I0628 19:35:07.963286 32082 solver.cpp:371]     Train net output #0: loss = 0.00467056 (* 1 = 0.00467056 loss)
I0628 19:35:07.963291 32082 sgd_solver.cpp:137] Iteration 60500, lr = 0.000546875, m = 0.9
I0628 19:35:09.686712 32082 solver.cpp:349] Iteration 60600 (58.0182 iter/s, 1.7236s/100 iter), loss = 0.00577596
I0628 19:35:09.686735 32082 solver.cpp:371]     Train net output #0: loss = 0.00577602 (* 1 = 0.00577602 loss)
I0628 19:35:09.686740 32082 sgd_solver.cpp:137] Iteration 60600, lr = 0.00053125, m = 0.9
I0628 19:35:11.405675 32082 solver.cpp:349] Iteration 60700 (58.1698 iter/s, 1.71911s/100 iter), loss = 0.00359151
I0628 19:35:11.405701 32082 solver.cpp:371]     Train net output #0: loss = 0.00359157 (* 1 = 0.00359157 loss)
I0628 19:35:11.405707 32082 sgd_solver.cpp:137] Iteration 60700, lr = 0.000515625, m = 0.9
I0628 19:35:13.125854 32082 solver.cpp:349] Iteration 60800 (58.1289 iter/s, 1.72032s/100 iter), loss = 0.00308878
I0628 19:35:13.125880 32082 solver.cpp:371]     Train net output #0: loss = 0.00308884 (* 1 = 0.00308884 loss)
I0628 19:35:13.125885 32082 sgd_solver.cpp:137] Iteration 60800, lr = 0.0005, m = 0.9
I0628 19:35:14.845942 32082 solver.cpp:349] Iteration 60900 (58.132 iter/s, 1.72022s/100 iter), loss = 0.00377965
I0628 19:35:14.845966 32082 solver.cpp:371]     Train net output #0: loss = 0.00377971 (* 1 = 0.00377971 loss)
I0628 19:35:14.845973 32082 sgd_solver.cpp:137] Iteration 60900, lr = 0.000484375, m = 0.9
I0628 19:35:15.430310 32050 data_reader.cpp:262] Starting prefetch of epoch 78
I0628 19:35:16.545991 32082 solver.cpp:401] Sparsity after update:
I0628 19:35:16.547024 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:35:16.547030 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:35:16.547037 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:35:16.547039 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:35:16.547041 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:35:16.547044 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:35:16.547045 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:35:16.547047 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:35:16.547049 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:35:16.547050 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:35:16.547052 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:35:16.547055 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:35:16.547057 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:35:16.547065 32082 solver.cpp:545] Iteration 61000, Testing net (#0)
I0628 19:35:17.546247 32080 data_reader.cpp:262] Starting prefetch of epoch 61
I0628 19:35:17.567265 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9096
I0628 19:35:17.567276 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:35:17.567282 32082 solver.cpp:630]     Test net output #2: loss = 0.332143 (* 1 = 0.332143 loss)
I0628 19:35:17.567308 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02034s
I0628 19:35:17.585119 32082 solver.cpp:349] Iteration 61000 (36.5042 iter/s, 2.73941s/100 iter), loss = 0.00957494
I0628 19:35:17.585146 32082 solver.cpp:371]     Train net output #0: loss = 0.00957499 (* 1 = 0.00957499 loss)
I0628 19:35:17.585152 32082 sgd_solver.cpp:137] Iteration 61000, lr = 0.00046875, m = 0.9
I0628 19:35:19.301733 32082 solver.cpp:349] Iteration 61100 (58.25 iter/s, 1.71674s/100 iter), loss = 0.00251508
I0628 19:35:19.301753 32082 solver.cpp:371]     Train net output #0: loss = 0.00251514 (* 1 = 0.00251514 loss)
I0628 19:35:19.301758 32082 sgd_solver.cpp:137] Iteration 61100, lr = 0.000453125, m = 0.9
I0628 19:35:21.021795 32082 solver.cpp:349] Iteration 61200 (58.1329 iter/s, 1.7202s/100 iter), loss = 0.00508208
I0628 19:35:21.021852 32082 solver.cpp:371]     Train net output #0: loss = 0.00508214 (* 1 = 0.00508214 loss)
I0628 19:35:21.021857 32082 sgd_solver.cpp:137] Iteration 61200, lr = 0.0004375, m = 0.9
I0628 19:35:22.741119 32082 solver.cpp:349] Iteration 61300 (58.1593 iter/s, 1.71942s/100 iter), loss = 0.00181793
I0628 19:35:22.741145 32082 solver.cpp:371]     Train net output #0: loss = 0.00181799 (* 1 = 0.00181799 loss)
I0628 19:35:22.741152 32082 sgd_solver.cpp:137] Iteration 61300, lr = 0.000421875, m = 0.9
I0628 19:35:24.459820 32082 solver.cpp:349] Iteration 61400 (58.1795 iter/s, 1.71882s/100 iter), loss = 0.00492441
I0628 19:35:24.459844 32082 solver.cpp:371]     Train net output #0: loss = 0.00492447 (* 1 = 0.00492447 loss)
I0628 19:35:24.459851 32082 sgd_solver.cpp:137] Iteration 61400, lr = 0.00040625, m = 0.9
I0628 19:35:26.183339 32082 solver.cpp:349] Iteration 61500 (58.0169 iter/s, 1.72364s/100 iter), loss = 0.00449921
I0628 19:35:26.183387 32082 solver.cpp:371]     Train net output #0: loss = 0.00449927 (* 1 = 0.00449927 loss)
I0628 19:35:26.183394 32082 sgd_solver.cpp:137] Iteration 61500, lr = 0.000390625, m = 0.9
I0628 19:35:27.904253 32082 solver.cpp:349] Iteration 61600 (58.1057 iter/s, 1.721s/100 iter), loss = 0.00539983
I0628 19:35:27.904279 32082 solver.cpp:371]     Train net output #0: loss = 0.00539989 (* 1 = 0.00539989 loss)
I0628 19:35:27.904285 32082 sgd_solver.cpp:137] Iteration 61600, lr = 0.000375, m = 0.9
I0628 19:35:29.626201 32082 solver.cpp:349] Iteration 61700 (58.07 iter/s, 1.72206s/100 iter), loss = 0.00357078
I0628 19:35:29.626227 32082 solver.cpp:371]     Train net output #0: loss = 0.00357084 (* 1 = 0.00357084 loss)
I0628 19:35:29.626232 32082 sgd_solver.cpp:137] Iteration 61700, lr = 0.000359375, m = 0.9
I0628 19:35:29.884251 32050 data_reader.cpp:262] Starting prefetch of epoch 79
I0628 19:35:31.349999 32082 solver.cpp:349] Iteration 61800 (58.0078 iter/s, 1.72391s/100 iter), loss = 0.00486238
I0628 19:35:31.350024 32082 solver.cpp:371]     Train net output #0: loss = 0.00486243 (* 1 = 0.00486243 loss)
I0628 19:35:31.350030 32082 sgd_solver.cpp:137] Iteration 61800, lr = 0.00034375, m = 0.9
I0628 19:35:33.067368 32082 solver.cpp:349] Iteration 61900 (58.225 iter/s, 1.71747s/100 iter), loss = 0.00267896
I0628 19:35:33.067392 32082 solver.cpp:371]     Train net output #0: loss = 0.00267902 (* 1 = 0.00267902 loss)
I0628 19:35:33.067399 32082 sgd_solver.cpp:137] Iteration 61900, lr = 0.000328125, m = 0.9
I0628 19:35:34.767946 32082 solver.cpp:401] Sparsity after update:
I0628 19:35:34.769017 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:35:34.769026 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:35:34.769031 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:35:34.769032 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:35:34.769034 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:35:34.769037 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:35:34.769038 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:35:34.769040 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:35:34.769042 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:35:34.769044 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:35:34.769048 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:35:34.769050 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:35:34.769052 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:35:34.769063 32082 solver.cpp:545] Iteration 62000, Testing net (#0)
I0628 19:35:35.766456 32080 data_reader.cpp:262] Starting prefetch of epoch 62
I0628 19:35:35.787864 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9094
I0628 19:35:35.787878 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:35:35.787883 32082 solver.cpp:630]     Test net output #2: loss = 0.332727 (* 1 = 0.332727 loss)
I0628 19:35:35.787897 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01892s
I0628 19:35:35.805236 32082 solver.cpp:349] Iteration 62000 (36.5222 iter/s, 2.73806s/100 iter), loss = 0.00664685
I0628 19:35:35.805285 32082 solver.cpp:371]     Train net output #0: loss = 0.00664691 (* 1 = 0.00664691 loss)
I0628 19:35:35.805289 32082 sgd_solver.cpp:137] Iteration 62000, lr = 0.0003125, m = 0.9
I0628 19:35:37.525030 32082 solver.cpp:349] Iteration 62100 (58.1438 iter/s, 1.71987s/100 iter), loss = 0.00701395
I0628 19:35:37.525054 32082 solver.cpp:371]     Train net output #0: loss = 0.00701401 (* 1 = 0.00701401 loss)
I0628 19:35:37.525060 32082 sgd_solver.cpp:137] Iteration 62100, lr = 0.000296875, m = 0.9
I0628 19:35:39.245071 32082 solver.cpp:349] Iteration 62200 (58.1348 iter/s, 1.72014s/100 iter), loss = 0.006533
I0628 19:35:39.245098 32082 solver.cpp:371]     Train net output #0: loss = 0.00653306 (* 1 = 0.00653306 loss)
I0628 19:35:39.245103 32082 sgd_solver.cpp:137] Iteration 62200, lr = 0.00028125, m = 0.9
I0628 19:35:40.962679 32082 solver.cpp:349] Iteration 62300 (58.2173 iter/s, 1.7177s/100 iter), loss = 0.00577083
I0628 19:35:40.962705 32082 solver.cpp:371]     Train net output #0: loss = 0.00577089 (* 1 = 0.00577089 loss)
I0628 19:35:40.962710 32082 sgd_solver.cpp:137] Iteration 62300, lr = 0.000265625, m = 0.9
I0628 19:35:42.682744 32082 solver.cpp:349] Iteration 62400 (58.1342 iter/s, 1.72016s/100 iter), loss = 0.000930602
I0628 19:35:42.682766 32082 solver.cpp:371]     Train net output #0: loss = 0.00093066 (* 1 = 0.00093066 loss)
I0628 19:35:42.682770 32082 sgd_solver.cpp:137] Iteration 62400, lr = 0.00025, m = 0.9
I0628 19:35:44.334462 32050 data_reader.cpp:262] Starting prefetch of epoch 80
I0628 19:35:44.403084 32082 solver.cpp:349] Iteration 62500 (58.1248 iter/s, 1.72044s/100 iter), loss = 0.00294777
I0628 19:35:44.403106 32082 solver.cpp:371]     Train net output #0: loss = 0.00294783 (* 1 = 0.00294783 loss)
I0628 19:35:44.403110 32082 sgd_solver.cpp:137] Iteration 62500, lr = 0.000234375, m = 0.9
I0628 19:35:46.125000 32082 solver.cpp:349] Iteration 62600 (58.0717 iter/s, 1.72201s/100 iter), loss = 0.00396825
I0628 19:35:46.125021 32082 solver.cpp:371]     Train net output #0: loss = 0.0039683 (* 1 = 0.0039683 loss)
I0628 19:35:46.125025 32082 sgd_solver.cpp:137] Iteration 62600, lr = 0.00021875, m = 0.9
I0628 19:35:47.849781 32082 solver.cpp:349] Iteration 62700 (57.9754 iter/s, 1.72487s/100 iter), loss = 0.00879229
I0628 19:35:47.849807 32082 solver.cpp:371]     Train net output #0: loss = 0.00879235 (* 1 = 0.00879235 loss)
I0628 19:35:47.849812 32082 sgd_solver.cpp:137] Iteration 62700, lr = 0.000203125, m = 0.9
I0628 19:35:49.567936 32082 solver.cpp:349] Iteration 62800 (58.1992 iter/s, 1.71824s/100 iter), loss = 0.00772831
I0628 19:35:49.567963 32082 solver.cpp:371]     Train net output #0: loss = 0.00772837 (* 1 = 0.00772837 loss)
I0628 19:35:49.567968 32082 sgd_solver.cpp:137] Iteration 62800, lr = 0.0001875, m = 0.9
I0628 19:35:51.285089 32082 solver.cpp:349] Iteration 62900 (58.2333 iter/s, 1.71723s/100 iter), loss = 0.00477999
I0628 19:35:51.285166 32082 solver.cpp:371]     Train net output #0: loss = 0.00478005 (* 1 = 0.00478005 loss)
I0628 19:35:51.285171 32082 sgd_solver.cpp:137] Iteration 62900, lr = 0.000171875, m = 0.9
I0628 19:35:52.986681 32082 solver.cpp:401] Sparsity after update:
I0628 19:35:52.987745 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:35:52.987751 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:35:52.987758 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:35:52.987761 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:35:52.987762 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:35:52.987766 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:35:52.987767 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:35:52.987769 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:35:52.987771 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:35:52.987773 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:35:52.987776 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:35:52.987778 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:35:52.987781 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:35:52.987787 32082 solver.cpp:545] Iteration 63000, Testing net (#0)
I0628 19:35:53.987442 32080 data_reader.cpp:262] Starting prefetch of epoch 63
I0628 19:35:54.007498 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9096
I0628 19:35:54.007508 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:35:54.007513 32082 solver.cpp:630]     Test net output #2: loss = 0.334117 (* 1 = 0.334117 loss)
I0628 19:35:54.007529 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01981s
I0628 19:35:54.026218 32082 solver.cpp:349] Iteration 63000 (36.48 iter/s, 2.74123s/100 iter), loss = 0.00271028
I0628 19:35:54.026239 32082 solver.cpp:371]     Train net output #0: loss = 0.00271035 (* 1 = 0.00271035 loss)
I0628 19:35:54.026243 32082 sgd_solver.cpp:137] Iteration 63000, lr = 0.00015625, m = 0.9
I0628 19:35:55.744562 32082 solver.cpp:349] Iteration 63100 (58.1928 iter/s, 1.71843s/100 iter), loss = 0.00179535
I0628 19:35:55.744587 32082 solver.cpp:371]     Train net output #0: loss = 0.00179541 (* 1 = 0.00179541 loss)
I0628 19:35:55.744592 32082 sgd_solver.cpp:137] Iteration 63100, lr = 0.000140625, m = 0.9
I0628 19:35:57.462111 32082 solver.cpp:349] Iteration 63200 (58.22 iter/s, 1.71762s/100 iter), loss = 0.00727016
I0628 19:35:57.462134 32082 solver.cpp:371]     Train net output #0: loss = 0.00727022 (* 1 = 0.00727022 loss)
I0628 19:35:57.462138 32082 sgd_solver.cpp:137] Iteration 63200, lr = 0.000125, m = 0.9
I0628 19:35:58.805685 32050 data_reader.cpp:262] Starting prefetch of epoch 81
I0628 19:35:59.183284 32082 solver.cpp:349] Iteration 63300 (58.0974 iter/s, 1.72125s/100 iter), loss = 0.00304053
I0628 19:35:59.183310 32082 solver.cpp:371]     Train net output #0: loss = 0.0030406 (* 1 = 0.0030406 loss)
I0628 19:35:59.183316 32082 sgd_solver.cpp:137] Iteration 63300, lr = 0.000109375, m = 0.9
I0628 19:36:00.902621 32082 solver.cpp:349] Iteration 63400 (58.1598 iter/s, 1.7194s/100 iter), loss = 0.00524929
I0628 19:36:00.902647 32082 solver.cpp:371]     Train net output #0: loss = 0.00524936 (* 1 = 0.00524936 loss)
I0628 19:36:00.902652 32082 sgd_solver.cpp:137] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0628 19:36:02.630903 32082 solver.cpp:349] Iteration 63500 (57.8588 iter/s, 1.72835s/100 iter), loss = 0.00755569
I0628 19:36:02.630928 32082 solver.cpp:371]     Train net output #0: loss = 0.00755576 (* 1 = 0.00755576 loss)
I0628 19:36:02.630934 32082 sgd_solver.cpp:137] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0628 19:36:04.350314 32082 solver.cpp:349] Iteration 63600 (58.1573 iter/s, 1.71947s/100 iter), loss = 0.00824207
I0628 19:36:04.350340 32082 solver.cpp:371]     Train net output #0: loss = 0.00824213 (* 1 = 0.00824213 loss)
I0628 19:36:04.350347 32082 sgd_solver.cpp:137] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0628 19:36:06.071424 32082 solver.cpp:349] Iteration 63700 (58.1001 iter/s, 1.72117s/100 iter), loss = 0.00297086
I0628 19:36:06.071465 32082 solver.cpp:371]     Train net output #0: loss = 0.00297092 (* 1 = 0.00297092 loss)
I0628 19:36:06.071471 32082 sgd_solver.cpp:137] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0628 19:36:07.792111 32082 solver.cpp:349] Iteration 63800 (58.115 iter/s, 1.72073s/100 iter), loss = 0.00408494
I0628 19:36:07.792138 32082 solver.cpp:371]     Train net output #0: loss = 0.004085 (* 1 = 0.004085 loss)
I0628 19:36:07.792145 32082 sgd_solver.cpp:137] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0628 19:36:09.511823 32082 solver.cpp:349] Iteration 63900 (58.1475 iter/s, 1.71976s/100 iter), loss = 0.00561901
I0628 19:36:09.511845 32082 solver.cpp:371]     Train net output #0: loss = 0.00561907 (* 1 = 0.00561907 loss)
I0628 19:36:09.511852 32082 sgd_solver.cpp:137] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0628 19:36:11.212572 32082 solver.cpp:349] Iteration 63999 (58.2077 iter/s, 1.7008s/100 iter), loss = 0.00344106
I0628 19:36:11.212595 32082 solver.cpp:371]     Train net output #0: loss = 0.00344112 (* 1 = 0.00344112 loss)
I0628 19:36:11.212601 32082 solver.cpp:401] Sparsity after update:
I0628 19:36:11.213904 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:36:11.213912 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:36:11.213914 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:36:11.213917 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:36:11.213918 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:36:11.213922 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:36:11.213923 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:36:11.213927 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:36:11.213928 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:36:11.213930 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:36:11.213932 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:36:11.213934 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:36:11.213935 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:36:11.213971 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0628 19:36:11.221719 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0628 19:36:11.231536 32082 solver.cpp:522] Iteration 64000, loss = 0.00742856
I0628 19:36:11.231556 32082 solver.cpp:545] Iteration 64000, Testing net (#0)
I0628 19:36:12.229812 32080 data_reader.cpp:262] Starting prefetch of epoch 64
I0628 19:36:12.250552 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9088
I0628 19:36:12.250569 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:36:12.250576 32082 solver.cpp:630]     Test net output #2: loss = 0.332688 (* 1 = 0.332688 loss)
I0628 19:36:12.255136 32034 parallel.cpp:71] Root Solver performance on device 0: 53.94 * 32 = 1726 img/sec
I0628 19:36:12.255147 32034 parallel.cpp:76]      Solver performance on device 1: 53.94 * 32 = 1726 img/sec
I0628 19:36:12.255149 32034 parallel.cpp:79] Overall multi-GPU performance: 3452.12 img/sec
I0628 19:36:12.287798 32034 caffe.cpp:247] Optimization Done in 19m 50s
training/cifar10_jacintonet11v2_2017-06-28_18-56-45/test
I0628 19:36:13.357971 24941 caffe.cpp:608] This is NVCaffe 0.16.2 started at Wed Jun 28 19:36:13 2017
I0628 19:36:13.358098 24941 caffe.cpp:611] CuDNN version: 6.0.21
I0628 19:36:13.358103 24941 caffe.cpp:612] CuBLAS version: 8000
I0628 19:36:13.358104 24941 caffe.cpp:613] CUDA version: 8000
I0628 19:36:13.358106 24941 caffe.cpp:614] CUDA driver version: 8000
I0628 19:36:13.358111 24941 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0628 19:36:13.358592 24941 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0628 19:36:13.359063 24941 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8277393408, dev_info[0]: total=8506769408 free=8277393408
I0628 19:36:13.359067 24941 caffe.cpp:275] Use GPU with device ID 0
I0628 19:36:13.359324 24941 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0628 19:36:13.360569 24941 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0628 19:36:13.360674 24941 net.cpp:108] Using FLOAT as default forward math type
I0628 19:36:13.360678 24941 net.cpp:114] Using FLOAT as default backward math type
I0628 19:36:13.360682 24941 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0628 19:36:13.360684 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.360725 24941 net.cpp:183] Created Layer data (0)
I0628 19:36:13.360730 24941 net.cpp:529] data -> data
I0628 19:36:13.360738 24941 net.cpp:529] data -> label
I0628 19:36:13.360754 24941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0628 19:36:13.361040 24941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:36:13.367234 24955 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0628 19:36:13.367847 24941 data_layer.cpp:188] ReshapePrefetch 50, 3, 32, 32
I0628 19:36:13.367880 24941 data_layer.cpp:206] Output data size: 50, 3, 32, 32
I0628 19:36:13.367885 24941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:36:13.367905 24941 net.cpp:244] Setting up data
I0628 19:36:13.367918 24941 net.cpp:251] TEST Top shape for layer 0 'data' 50 3 32 32 (153600)
I0628 19:36:13.367923 24941 net.cpp:251] TEST Top shape for layer 0 'data' 50 (50)
I0628 19:36:13.367928 24941 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0628 19:36:13.367931 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.367941 24941 net.cpp:183] Created Layer label_data_1_split (1)
I0628 19:36:13.367945 24941 net.cpp:560] label_data_1_split <- label
I0628 19:36:13.367951 24941 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0628 19:36:13.367956 24941 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0628 19:36:13.367959 24941 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0628 19:36:13.367987 24941 net.cpp:244] Setting up label_data_1_split
I0628 19:36:13.368000 24941 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0628 19:36:13.368002 24941 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0628 19:36:13.368005 24941 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0628 19:36:13.368007 24941 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0628 19:36:13.368010 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.368018 24941 net.cpp:183] Created Layer data/bias (2)
I0628 19:36:13.368021 24941 net.cpp:560] data/bias <- data
I0628 19:36:13.368023 24941 net.cpp:529] data/bias -> data/bias
I0628 19:36:13.368950 24956 data_layer.cpp:188] ReshapePrefetch 50, 3, 32, 32
I0628 19:36:13.368960 24956 data_layer.cpp:206] Output data size: 50, 3, 32, 32
I0628 19:36:13.369947 24941 net.cpp:244] Setting up data/bias
I0628 19:36:13.369958 24941 net.cpp:251] TEST Top shape for layer 2 'data/bias' 50 3 32 32 (153600)
I0628 19:36:13.369968 24941 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0628 19:36:13.369972 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.369985 24941 net.cpp:183] Created Layer conv1a (3)
I0628 19:36:13.369988 24941 net.cpp:560] conv1a <- data/bias
I0628 19:36:13.369992 24941 net.cpp:529] conv1a -> conv1a
I0628 19:36:13.370004 24956 data_layer.cpp:110] [0] Parser threads: 1
I0628 19:36:13.370014 24956 data_layer.cpp:112] [0] Transformer threads: 1
I0628 19:36:13.651276 24941 net.cpp:244] Setting up conv1a
I0628 19:36:13.651298 24941 net.cpp:251] TEST Top shape for layer 3 'conv1a' 50 32 32 32 (1638400)
I0628 19:36:13.651309 24941 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0628 19:36:13.651312 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.651324 24941 net.cpp:183] Created Layer conv1a/bn (4)
I0628 19:36:13.651326 24941 net.cpp:560] conv1a/bn <- conv1a
I0628 19:36:13.651330 24941 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0628 19:36:13.652166 24941 net.cpp:244] Setting up conv1a/bn
I0628 19:36:13.652175 24941 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 50 32 32 32 (1638400)
I0628 19:36:13.652184 24941 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0628 19:36:13.652186 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.652190 24941 net.cpp:183] Created Layer conv1a/relu (5)
I0628 19:36:13.652192 24941 net.cpp:560] conv1a/relu <- conv1a
I0628 19:36:13.652195 24941 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0628 19:36:13.652206 24941 net.cpp:244] Setting up conv1a/relu
I0628 19:36:13.652210 24941 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 50 32 32 32 (1638400)
I0628 19:36:13.652214 24941 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0628 19:36:13.652215 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.652225 24941 net.cpp:183] Created Layer conv1b (6)
I0628 19:36:13.652226 24941 net.cpp:560] conv1b <- conv1a
I0628 19:36:13.652228 24941 net.cpp:529] conv1b -> conv1b
I0628 19:36:13.655524 24941 net.cpp:244] Setting up conv1b
I0628 19:36:13.655534 24941 net.cpp:251] TEST Top shape for layer 6 'conv1b' 50 32 32 32 (1638400)
I0628 19:36:13.655539 24941 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0628 19:36:13.655542 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.655550 24941 net.cpp:183] Created Layer conv1b/bn (7)
I0628 19:36:13.655551 24941 net.cpp:560] conv1b/bn <- conv1b
I0628 19:36:13.655555 24941 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0628 19:36:13.656325 24941 net.cpp:244] Setting up conv1b/bn
I0628 19:36:13.656334 24941 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 50 32 32 32 (1638400)
I0628 19:36:13.656340 24941 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0628 19:36:13.656352 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.656357 24941 net.cpp:183] Created Layer conv1b/relu (8)
I0628 19:36:13.656358 24941 net.cpp:560] conv1b/relu <- conv1b
I0628 19:36:13.656361 24941 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0628 19:36:13.656365 24941 net.cpp:244] Setting up conv1b/relu
I0628 19:36:13.656368 24941 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 50 32 32 32 (1638400)
I0628 19:36:13.656369 24941 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0628 19:36:13.656371 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.656378 24941 net.cpp:183] Created Layer pool1 (9)
I0628 19:36:13.656381 24941 net.cpp:560] pool1 <- conv1b
I0628 19:36:13.656383 24941 net.cpp:529] pool1 -> pool1
I0628 19:36:13.656425 24941 net.cpp:244] Setting up pool1
I0628 19:36:13.656430 24941 net.cpp:251] TEST Top shape for layer 9 'pool1' 50 32 32 32 (1638400)
I0628 19:36:13.656432 24941 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0628 19:36:13.656435 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.656440 24941 net.cpp:183] Created Layer res2a_branch2a (10)
I0628 19:36:13.656443 24941 net.cpp:560] res2a_branch2a <- pool1
I0628 19:36:13.656445 24941 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0628 19:36:13.661602 24941 net.cpp:244] Setting up res2a_branch2a
I0628 19:36:13.661612 24941 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 50 64 32 32 (3276800)
I0628 19:36:13.661617 24941 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0628 19:36:13.661620 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.661625 24941 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0628 19:36:13.661628 24941 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0628 19:36:13.661631 24941 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0628 19:36:13.662444 24941 net.cpp:244] Setting up res2a_branch2a/bn
I0628 19:36:13.662452 24941 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 32 32 (3276800)
I0628 19:36:13.662458 24941 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0628 19:36:13.662461 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.662464 24941 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0628 19:36:13.662467 24941 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0628 19:36:13.662468 24941 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0628 19:36:13.662473 24941 net.cpp:244] Setting up res2a_branch2a/relu
I0628 19:36:13.662477 24941 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 32 32 (3276800)
I0628 19:36:13.662478 24941 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0628 19:36:13.662482 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.662487 24941 net.cpp:183] Created Layer res2a_branch2b (13)
I0628 19:36:13.662490 24941 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0628 19:36:13.662492 24941 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0628 19:36:13.665390 24941 net.cpp:244] Setting up res2a_branch2b
I0628 19:36:13.665400 24941 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 50 64 32 32 (3276800)
I0628 19:36:13.665405 24941 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0628 19:36:13.665407 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.665411 24941 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0628 19:36:13.665415 24941 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0628 19:36:13.665416 24941 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0628 19:36:13.666261 24941 net.cpp:244] Setting up res2a_branch2b/bn
I0628 19:36:13.666270 24941 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 32 32 (3276800)
I0628 19:36:13.666276 24941 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0628 19:36:13.666280 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.666282 24941 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0628 19:36:13.666285 24941 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0628 19:36:13.666288 24941 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0628 19:36:13.666292 24941 net.cpp:244] Setting up res2a_branch2b/relu
I0628 19:36:13.666296 24941 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 32 32 (3276800)
I0628 19:36:13.666298 24941 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0628 19:36:13.666301 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.666304 24941 net.cpp:183] Created Layer pool2 (16)
I0628 19:36:13.666306 24941 net.cpp:560] pool2 <- res2a_branch2b
I0628 19:36:13.666309 24941 net.cpp:529] pool2 -> pool2
I0628 19:36:13.666342 24941 net.cpp:244] Setting up pool2
I0628 19:36:13.666345 24941 net.cpp:251] TEST Top shape for layer 16 'pool2' 50 64 16 16 (819200)
I0628 19:36:13.666348 24941 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0628 19:36:13.666350 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.666355 24941 net.cpp:183] Created Layer res3a_branch2a (17)
I0628 19:36:13.666357 24941 net.cpp:560] res3a_branch2a <- pool2
I0628 19:36:13.666360 24941 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0628 19:36:13.671283 24941 net.cpp:244] Setting up res3a_branch2a
I0628 19:36:13.671293 24941 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 50 128 16 16 (1638400)
I0628 19:36:13.671298 24941 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0628 19:36:13.671301 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.671305 24941 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0628 19:36:13.671308 24941 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0628 19:36:13.671310 24941 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0628 19:36:13.672086 24941 net.cpp:244] Setting up res3a_branch2a/bn
I0628 19:36:13.672096 24941 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 16 16 (1638400)
I0628 19:36:13.672102 24941 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0628 19:36:13.672106 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.672108 24941 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0628 19:36:13.672111 24941 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0628 19:36:13.672112 24941 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0628 19:36:13.672116 24941 net.cpp:244] Setting up res3a_branch2a/relu
I0628 19:36:13.672119 24941 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 16 16 (1638400)
I0628 19:36:13.672122 24941 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0628 19:36:13.672123 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.672130 24941 net.cpp:183] Created Layer res3a_branch2b (20)
I0628 19:36:13.672133 24941 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0628 19:36:13.672137 24941 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0628 19:36:13.674401 24941 net.cpp:244] Setting up res3a_branch2b
I0628 19:36:13.674410 24941 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 50 128 16 16 (1638400)
I0628 19:36:13.674415 24941 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0628 19:36:13.674417 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.674428 24941 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0628 19:36:13.674432 24941 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0628 19:36:13.674434 24941 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0628 19:36:13.675216 24941 net.cpp:244] Setting up res3a_branch2b/bn
I0628 19:36:13.675225 24941 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 16 16 (1638400)
I0628 19:36:13.675231 24941 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0628 19:36:13.675235 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.675238 24941 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0628 19:36:13.675240 24941 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0628 19:36:13.675243 24941 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0628 19:36:13.675248 24941 net.cpp:244] Setting up res3a_branch2b/relu
I0628 19:36:13.675251 24941 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 16 16 (1638400)
I0628 19:36:13.675253 24941 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0628 19:36:13.675256 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.675259 24941 net.cpp:183] Created Layer pool3 (23)
I0628 19:36:13.675262 24941 net.cpp:560] pool3 <- res3a_branch2b
I0628 19:36:13.675264 24941 net.cpp:529] pool3 -> pool3
I0628 19:36:13.675302 24941 net.cpp:244] Setting up pool3
I0628 19:36:13.675305 24941 net.cpp:251] TEST Top shape for layer 23 'pool3' 50 128 16 16 (1638400)
I0628 19:36:13.675307 24941 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0628 19:36:13.675310 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.675318 24941 net.cpp:183] Created Layer res4a_branch2a (24)
I0628 19:36:13.675319 24941 net.cpp:560] res4a_branch2a <- pool3
I0628 19:36:13.675323 24941 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0628 19:36:13.687798 24941 net.cpp:244] Setting up res4a_branch2a
I0628 19:36:13.687816 24941 net.cpp:251] TEST Top shape for layer 24 'res4a_branch2a' 50 256 16 16 (3276800)
I0628 19:36:13.687824 24941 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0628 19:36:13.687827 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.687834 24941 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0628 19:36:13.687837 24941 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0628 19:36:13.687840 24941 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0628 19:36:13.688693 24941 net.cpp:244] Setting up res4a_branch2a/bn
I0628 19:36:13.688700 24941 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 16 16 (3276800)
I0628 19:36:13.688706 24941 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0628 19:36:13.688709 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.688712 24941 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0628 19:36:13.688714 24941 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0628 19:36:13.688716 24941 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0628 19:36:13.688721 24941 net.cpp:244] Setting up res4a_branch2a/relu
I0628 19:36:13.688724 24941 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 16 16 (3276800)
I0628 19:36:13.688726 24941 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0628 19:36:13.688729 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.688735 24941 net.cpp:183] Created Layer res4a_branch2b (27)
I0628 19:36:13.688738 24941 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0628 19:36:13.688740 24941 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0628 19:36:13.694972 24941 net.cpp:244] Setting up res4a_branch2b
I0628 19:36:13.694990 24941 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2b' 50 256 16 16 (3276800)
I0628 19:36:13.694995 24941 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0628 19:36:13.694998 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.695003 24941 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0628 19:36:13.695005 24941 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0628 19:36:13.695008 24941 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0628 19:36:13.695829 24941 net.cpp:244] Setting up res4a_branch2b/bn
I0628 19:36:13.695838 24941 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 16 16 (3276800)
I0628 19:36:13.695844 24941 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0628 19:36:13.695847 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.695849 24941 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0628 19:36:13.695852 24941 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0628 19:36:13.695854 24941 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0628 19:36:13.695858 24941 net.cpp:244] Setting up res4a_branch2b/relu
I0628 19:36:13.695860 24941 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 16 16 (3276800)
I0628 19:36:13.695863 24941 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0628 19:36:13.695865 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.695868 24941 net.cpp:183] Created Layer pool4 (30)
I0628 19:36:13.695870 24941 net.cpp:560] pool4 <- res4a_branch2b
I0628 19:36:13.695873 24941 net.cpp:529] pool4 -> pool4
I0628 19:36:13.695909 24941 net.cpp:244] Setting up pool4
I0628 19:36:13.695912 24941 net.cpp:251] TEST Top shape for layer 30 'pool4' 50 256 8 8 (819200)
I0628 19:36:13.695914 24941 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0628 19:36:13.695917 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.695924 24941 net.cpp:183] Created Layer res5a_branch2a (31)
I0628 19:36:13.695925 24941 net.cpp:560] res5a_branch2a <- pool4
I0628 19:36:13.695927 24941 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0628 19:36:13.728518 24941 net.cpp:244] Setting up res5a_branch2a
I0628 19:36:13.728536 24941 net.cpp:251] TEST Top shape for layer 31 'res5a_branch2a' 50 512 8 8 (1638400)
I0628 19:36:13.728543 24941 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0628 19:36:13.728548 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.728556 24941 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0628 19:36:13.728560 24941 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0628 19:36:13.728564 24941 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0628 19:36:13.729400 24941 net.cpp:244] Setting up res5a_branch2a/bn
I0628 19:36:13.729409 24941 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 8 8 (1638400)
I0628 19:36:13.729415 24941 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0628 19:36:13.729418 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.729421 24941 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0628 19:36:13.729424 24941 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0628 19:36:13.729426 24941 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0628 19:36:13.729431 24941 net.cpp:244] Setting up res5a_branch2a/relu
I0628 19:36:13.729434 24941 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 8 8 (1638400)
I0628 19:36:13.729435 24941 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0628 19:36:13.729439 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.729456 24941 net.cpp:183] Created Layer res5a_branch2b (34)
I0628 19:36:13.729460 24941 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0628 19:36:13.729462 24941 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0628 19:36:13.744184 24941 net.cpp:244] Setting up res5a_branch2b
I0628 19:36:13.744202 24941 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2b' 50 512 8 8 (1638400)
I0628 19:36:13.744213 24941 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0628 19:36:13.744217 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.744225 24941 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0628 19:36:13.744227 24941 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0628 19:36:13.744230 24941 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0628 19:36:13.745059 24941 net.cpp:244] Setting up res5a_branch2b/bn
I0628 19:36:13.745069 24941 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 8 8 (1638400)
I0628 19:36:13.745074 24941 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0628 19:36:13.745079 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.745082 24941 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0628 19:36:13.745085 24941 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0628 19:36:13.745088 24941 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0628 19:36:13.745095 24941 net.cpp:244] Setting up res5a_branch2b/relu
I0628 19:36:13.745097 24941 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 8 8 (1638400)
I0628 19:36:13.745100 24941 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0628 19:36:13.745101 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.745106 24941 net.cpp:183] Created Layer pool5 (37)
I0628 19:36:13.745110 24941 net.cpp:560] pool5 <- res5a_branch2b
I0628 19:36:13.745111 24941 net.cpp:529] pool5 -> pool5
I0628 19:36:13.745129 24941 net.cpp:244] Setting up pool5
I0628 19:36:13.745133 24941 net.cpp:251] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0628 19:36:13.745136 24941 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0628 19:36:13.745137 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.745144 24941 net.cpp:183] Created Layer fc10 (38)
I0628 19:36:13.745147 24941 net.cpp:560] fc10 <- pool5
I0628 19:36:13.745149 24941 net.cpp:529] fc10 -> fc10
I0628 19:36:13.745348 24941 net.cpp:244] Setting up fc10
I0628 19:36:13.745354 24941 net.cpp:251] TEST Top shape for layer 38 'fc10' 50 10 (500)
I0628 19:36:13.745358 24941 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0628 19:36:13.745362 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.745364 24941 net.cpp:183] Created Layer fc10_fc10_0_split (39)
I0628 19:36:13.745368 24941 net.cpp:560] fc10_fc10_0_split <- fc10
I0628 19:36:13.745369 24941 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0628 19:36:13.745373 24941 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0628 19:36:13.745376 24941 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0628 19:36:13.745405 24941 net.cpp:244] Setting up fc10_fc10_0_split
I0628 19:36:13.745409 24941 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0628 19:36:13.745411 24941 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0628 19:36:13.745414 24941 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0628 19:36:13.745416 24941 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0628 19:36:13.745419 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.745429 24941 net.cpp:183] Created Layer loss (40)
I0628 19:36:13.745431 24941 net.cpp:560] loss <- fc10_fc10_0_split_0
I0628 19:36:13.745443 24941 net.cpp:560] loss <- label_data_1_split_0
I0628 19:36:13.745448 24941 net.cpp:529] loss -> loss
I0628 19:36:13.745571 24941 net.cpp:244] Setting up loss
I0628 19:36:13.745579 24941 net.cpp:251] TEST Top shape for layer 40 'loss' (1)
I0628 19:36:13.745582 24941 net.cpp:255]     with loss weight 1
I0628 19:36:13.745595 24941 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0628 19:36:13.745599 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.745609 24941 net.cpp:183] Created Layer accuracy/top1 (41)
I0628 19:36:13.745613 24941 net.cpp:560] accuracy/top1 <- fc10_fc10_0_split_1
I0628 19:36:13.745616 24941 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0628 19:36:13.745621 24941 net.cpp:529] accuracy/top1 -> accuracy/top1
I0628 19:36:13.745632 24941 net.cpp:244] Setting up accuracy/top1
I0628 19:36:13.745637 24941 net.cpp:251] TEST Top shape for layer 41 'accuracy/top1' (1)
I0628 19:36:13.745640 24941 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0628 19:36:13.745645 24941 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:36:13.745653 24941 net.cpp:183] Created Layer accuracy/top5 (42)
I0628 19:36:13.745657 24941 net.cpp:560] accuracy/top5 <- fc10_fc10_0_split_2
I0628 19:36:13.745661 24941 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0628 19:36:13.745664 24941 net.cpp:529] accuracy/top5 -> accuracy/top5
I0628 19:36:13.745671 24941 net.cpp:244] Setting up accuracy/top5
I0628 19:36:13.745676 24941 net.cpp:251] TEST Top shape for layer 42 'accuracy/top5' (1)
I0628 19:36:13.745679 24941 net.cpp:324] accuracy/top5 does not need backward computation.
I0628 19:36:13.745683 24941 net.cpp:324] accuracy/top1 does not need backward computation.
I0628 19:36:13.745687 24941 net.cpp:322] loss needs backward computation.
I0628 19:36:13.745692 24941 net.cpp:322] fc10_fc10_0_split needs backward computation.
I0628 19:36:13.745695 24941 net.cpp:322] fc10 needs backward computation.
I0628 19:36:13.745699 24941 net.cpp:322] pool5 needs backward computation.
I0628 19:36:13.745703 24941 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0628 19:36:13.745707 24941 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0628 19:36:13.745709 24941 net.cpp:322] res5a_branch2b needs backward computation.
I0628 19:36:13.745713 24941 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0628 19:36:13.745717 24941 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0628 19:36:13.745720 24941 net.cpp:322] res5a_branch2a needs backward computation.
I0628 19:36:13.745724 24941 net.cpp:322] pool4 needs backward computation.
I0628 19:36:13.745728 24941 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0628 19:36:13.745731 24941 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0628 19:36:13.745734 24941 net.cpp:322] res4a_branch2b needs backward computation.
I0628 19:36:13.745738 24941 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0628 19:36:13.745741 24941 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0628 19:36:13.745745 24941 net.cpp:322] res4a_branch2a needs backward computation.
I0628 19:36:13.745749 24941 net.cpp:322] pool3 needs backward computation.
I0628 19:36:13.745754 24941 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0628 19:36:13.745756 24941 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0628 19:36:13.745760 24941 net.cpp:322] res3a_branch2b needs backward computation.
I0628 19:36:13.745764 24941 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0628 19:36:13.745767 24941 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0628 19:36:13.745770 24941 net.cpp:322] res3a_branch2a needs backward computation.
I0628 19:36:13.745774 24941 net.cpp:322] pool2 needs backward computation.
I0628 19:36:13.745777 24941 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0628 19:36:13.745780 24941 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0628 19:36:13.745789 24941 net.cpp:322] res2a_branch2b needs backward computation.
I0628 19:36:13.745793 24941 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0628 19:36:13.745796 24941 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0628 19:36:13.745800 24941 net.cpp:322] res2a_branch2a needs backward computation.
I0628 19:36:13.745805 24941 net.cpp:322] pool1 needs backward computation.
I0628 19:36:13.745808 24941 net.cpp:322] conv1b/relu needs backward computation.
I0628 19:36:13.745812 24941 net.cpp:322] conv1b/bn needs backward computation.
I0628 19:36:13.745815 24941 net.cpp:322] conv1b needs backward computation.
I0628 19:36:13.745820 24941 net.cpp:322] conv1a/relu needs backward computation.
I0628 19:36:13.745823 24941 net.cpp:322] conv1a/bn needs backward computation.
I0628 19:36:13.745826 24941 net.cpp:322] conv1a needs backward computation.
I0628 19:36:13.745831 24941 net.cpp:324] data/bias does not need backward computation.
I0628 19:36:13.745836 24941 net.cpp:324] label_data_1_split does not need backward computation.
I0628 19:36:13.745841 24941 net.cpp:324] data does not need backward computation.
I0628 19:36:13.745844 24941 net.cpp:366] This network produces output accuracy/top1
I0628 19:36:13.745847 24941 net.cpp:366] This network produces output accuracy/top5
I0628 19:36:13.745851 24941 net.cpp:366] This network produces output loss
I0628 19:36:13.745882 24941 net.cpp:388] Top memory (TEST) required for data: 275251200 diff: 183500808
I0628 19:36:13.745887 24941 net.cpp:391] Bottom memory (TEST) required for data: 275251200 diff: 275251200
I0628 19:36:13.745889 24941 net.cpp:394] Shared (in-place) memory (TEST) by data: 183500800 diff: 183500800
I0628 19:36:13.745893 24941 net.cpp:397] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0628 19:36:13.745898 24941 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0628 19:36:13.745900 24941 net.cpp:406] Network initialization done.
I0628 19:36:13.749963 24941 net.cpp:1087] Copying source layer data Type:Data #blobs=0
I0628 19:36:13.749984 24941 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0628 19:36:13.750021 24941 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0628 19:36:13.750035 24941 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0628 19:36:13.750182 24941 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0628 19:36:13.750187 24941 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0628 19:36:13.750198 24941 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0628 19:36:13.750288 24941 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0628 19:36:13.750293 24941 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0628 19:36:13.750296 24941 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0628 19:36:13.750315 24941 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:36:13.750409 24941 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0628 19:36:13.750414 24941 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0628 19:36:13.750429 24941 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:36:13.750516 24941 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0628 19:36:13.750521 24941 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0628 19:36:13.750525 24941 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0628 19:36:13.750564 24941 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:36:13.750651 24941 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0628 19:36:13.750656 24941 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0628 19:36:13.750681 24941 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:36:13.750771 24941 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0628 19:36:13.750777 24941 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0628 19:36:13.750779 24941 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0628 19:36:13.750890 24941 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:36:13.750972 24941 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0628 19:36:13.750977 24941 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0628 19:36:13.751034 24941 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:36:13.751113 24941 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0628 19:36:13.751118 24941 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0628 19:36:13.751121 24941 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0628 19:36:13.751459 24941 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:36:13.751544 24941 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0628 19:36:13.751549 24941 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0628 19:36:13.751694 24941 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:36:13.751777 24941 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0628 19:36:13.751782 24941 net.cpp:1087] Copying source layer pool5 Type:Pooling #blobs=0
I0628 19:36:13.751785 24941 net.cpp:1087] Copying source layer fc10 Type:InnerProduct #blobs=2
I0628 19:36:13.751796 24941 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0628 19:36:13.751839 24941 caffe.cpp:290] Running for 50 iterations.
I0628 19:36:13.795778 24941 caffe.cpp:313] Batch 0, accuracy/top1 = 0.94
I0628 19:36:13.795797 24941 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0628 19:36:13.795802 24941 caffe.cpp:313] Batch 0, loss = 0.153593
I0628 19:36:13.830345 24941 caffe.cpp:313] Batch 1, accuracy/top1 = 0.94
I0628 19:36:13.830365 24941 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0628 19:36:13.830369 24941 caffe.cpp:313] Batch 1, loss = 0.174427
I0628 19:36:13.839347 24941 caffe.cpp:313] Batch 2, accuracy/top1 = 0.96
I0628 19:36:13.839359 24941 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0628 19:36:13.839361 24941 caffe.cpp:313] Batch 2, loss = 0.186237
I0628 19:36:13.848323 24941 caffe.cpp:313] Batch 3, accuracy/top1 = 0.92
I0628 19:36:13.848332 24941 caffe.cpp:313] Batch 3, accuracy/top5 = 1
I0628 19:36:13.848335 24941 caffe.cpp:313] Batch 3, loss = 0.335301
I0628 19:36:13.857259 24941 caffe.cpp:313] Batch 4, accuracy/top1 = 0.94
I0628 19:36:13.857266 24941 caffe.cpp:313] Batch 4, accuracy/top5 = 1
I0628 19:36:13.857270 24941 caffe.cpp:313] Batch 4, loss = 0.219656
I0628 19:36:13.866230 24941 caffe.cpp:313] Batch 5, accuracy/top1 = 0.86
I0628 19:36:13.866247 24941 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0628 19:36:13.866250 24941 caffe.cpp:313] Batch 5, loss = 0.337282
I0628 19:36:13.875151 24941 caffe.cpp:313] Batch 6, accuracy/top1 = 0.9
I0628 19:36:13.875159 24941 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0628 19:36:13.875162 24941 caffe.cpp:313] Batch 6, loss = 0.18802
I0628 19:36:13.884042 24941 caffe.cpp:313] Batch 7, accuracy/top1 = 0.88
I0628 19:36:13.884050 24941 caffe.cpp:313] Batch 7, accuracy/top5 = 0.98
I0628 19:36:13.884053 24941 caffe.cpp:313] Batch 7, loss = 0.513992
I0628 19:36:13.892982 24941 caffe.cpp:313] Batch 8, accuracy/top1 = 0.9
I0628 19:36:13.892993 24941 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0628 19:36:13.892997 24941 caffe.cpp:313] Batch 8, loss = 0.217647
I0628 19:36:13.901896 24941 caffe.cpp:313] Batch 9, accuracy/top1 = 0.92
I0628 19:36:13.901904 24941 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0628 19:36:13.901907 24941 caffe.cpp:313] Batch 9, loss = 0.352723
I0628 19:36:13.910786 24941 caffe.cpp:313] Batch 10, accuracy/top1 = 0.96
I0628 19:36:13.910809 24941 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0628 19:36:13.910811 24941 caffe.cpp:313] Batch 10, loss = 0.155797
I0628 19:36:13.919688 24941 caffe.cpp:313] Batch 11, accuracy/top1 = 0.96
I0628 19:36:13.919698 24941 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0628 19:36:13.919700 24941 caffe.cpp:313] Batch 11, loss = 0.170298
I0628 19:36:13.928652 24941 caffe.cpp:313] Batch 12, accuracy/top1 = 0.98
I0628 19:36:13.928670 24941 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0628 19:36:13.928673 24941 caffe.cpp:313] Batch 12, loss = 0.133915
I0628 19:36:13.937598 24941 caffe.cpp:313] Batch 13, accuracy/top1 = 0.9
I0628 19:36:13.937608 24941 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0628 19:36:13.937610 24941 caffe.cpp:313] Batch 13, loss = 0.407794
I0628 19:36:13.946472 24941 caffe.cpp:313] Batch 14, accuracy/top1 = 0.88
I0628 19:36:13.946480 24941 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0628 19:36:13.946482 24941 caffe.cpp:313] Batch 14, loss = 0.397547
I0628 19:36:13.955391 24941 caffe.cpp:313] Batch 15, accuracy/top1 = 0.9
I0628 19:36:13.955406 24941 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0628 19:36:13.955410 24941 caffe.cpp:313] Batch 15, loss = 0.377622
I0628 19:36:13.964313 24941 caffe.cpp:313] Batch 16, accuracy/top1 = 0.92
I0628 19:36:13.964321 24941 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0628 19:36:13.964324 24941 caffe.cpp:313] Batch 16, loss = 0.385913
I0628 19:36:13.973203 24941 caffe.cpp:313] Batch 17, accuracy/top1 = 0.88
I0628 19:36:13.973212 24941 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0628 19:36:13.973214 24941 caffe.cpp:313] Batch 17, loss = 0.332212
I0628 19:36:13.982111 24941 caffe.cpp:313] Batch 18, accuracy/top1 = 0.9
I0628 19:36:13.982121 24941 caffe.cpp:313] Batch 18, accuracy/top5 = 1
I0628 19:36:13.982125 24941 caffe.cpp:313] Batch 18, loss = 0.29718
I0628 19:36:13.991036 24941 caffe.cpp:313] Batch 19, accuracy/top1 = 0.92
I0628 19:36:13.991046 24941 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0628 19:36:13.991050 24941 caffe.cpp:313] Batch 19, loss = 0.258859
I0628 19:36:13.999928 24941 caffe.cpp:313] Batch 20, accuracy/top1 = 0.86
I0628 19:36:13.999936 24941 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0628 19:36:13.999939 24941 caffe.cpp:313] Batch 20, loss = 0.319892
I0628 19:36:14.008817 24941 caffe.cpp:313] Batch 21, accuracy/top1 = 0.96
I0628 19:36:14.008826 24941 caffe.cpp:313] Batch 21, accuracy/top5 = 1
I0628 19:36:14.008828 24941 caffe.cpp:313] Batch 21, loss = 0.261385
I0628 19:36:14.017765 24941 caffe.cpp:313] Batch 22, accuracy/top1 = 0.88
I0628 19:36:14.017784 24941 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0628 19:36:14.017787 24941 caffe.cpp:313] Batch 22, loss = 0.515505
I0628 19:36:14.026691 24941 caffe.cpp:313] Batch 23, accuracy/top1 = 0.9
I0628 19:36:14.026700 24941 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0628 19:36:14.026702 24941 caffe.cpp:313] Batch 23, loss = 0.343264
I0628 19:36:14.035588 24941 caffe.cpp:313] Batch 24, accuracy/top1 = 0.88
I0628 19:36:14.035596 24941 caffe.cpp:313] Batch 24, accuracy/top5 = 0.98
I0628 19:36:14.035599 24941 caffe.cpp:313] Batch 24, loss = 0.301905
I0628 19:36:14.044505 24941 caffe.cpp:313] Batch 25, accuracy/top1 = 0.92
I0628 19:36:14.044517 24941 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0628 19:36:14.044520 24941 caffe.cpp:313] Batch 25, loss = 0.22202
I0628 19:36:14.053453 24941 caffe.cpp:313] Batch 26, accuracy/top1 = 0.9
I0628 19:36:14.053462 24941 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0628 19:36:14.053465 24941 caffe.cpp:313] Batch 26, loss = 0.442399
I0628 19:36:14.062230 24941 caffe.cpp:313] Batch 27, accuracy/top1 = 0.94
I0628 19:36:14.062238 24941 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0628 19:36:14.062242 24941 caffe.cpp:313] Batch 27, loss = 0.199298
I0628 19:36:14.071121 24941 caffe.cpp:313] Batch 28, accuracy/top1 = 0.88
I0628 19:36:14.071130 24941 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0628 19:36:14.071132 24941 caffe.cpp:313] Batch 28, loss = 0.399529
I0628 19:36:14.080050 24941 caffe.cpp:313] Batch 29, accuracy/top1 = 0.9
I0628 19:36:14.080066 24941 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0628 19:36:14.080081 24941 caffe.cpp:313] Batch 29, loss = 0.456926
I0628 19:36:14.088963 24941 caffe.cpp:313] Batch 30, accuracy/top1 = 0.9
I0628 19:36:14.088971 24941 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0628 19:36:14.088973 24941 caffe.cpp:313] Batch 30, loss = 0.465783
I0628 19:36:14.097831 24941 caffe.cpp:313] Batch 31, accuracy/top1 = 0.88
I0628 19:36:14.097839 24941 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0628 19:36:14.097842 24941 caffe.cpp:313] Batch 31, loss = 0.249379
I0628 19:36:14.106710 24941 caffe.cpp:313] Batch 32, accuracy/top1 = 0.9
I0628 19:36:14.106724 24941 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0628 19:36:14.106726 24941 caffe.cpp:313] Batch 32, loss = 0.252434
I0628 19:36:14.115635 24941 caffe.cpp:313] Batch 33, accuracy/top1 = 0.96
I0628 19:36:14.115643 24941 caffe.cpp:313] Batch 33, accuracy/top5 = 0.98
I0628 19:36:14.115646 24941 caffe.cpp:313] Batch 33, loss = 0.195247
I0628 19:36:14.124500 24941 caffe.cpp:313] Batch 34, accuracy/top1 = 0.88
I0628 19:36:14.124506 24941 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0628 19:36:14.124510 24941 caffe.cpp:313] Batch 34, loss = 0.454933
I0628 19:36:14.133380 24941 caffe.cpp:313] Batch 35, accuracy/top1 = 0.94
I0628 19:36:14.133388 24941 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0628 19:36:14.133391 24941 caffe.cpp:313] Batch 35, loss = 0.20312
I0628 19:36:14.142289 24941 caffe.cpp:313] Batch 36, accuracy/top1 = 0.86
I0628 19:36:14.142308 24941 caffe.cpp:313] Batch 36, accuracy/top5 = 0.98
I0628 19:36:14.142310 24941 caffe.cpp:313] Batch 36, loss = 0.368316
I0628 19:36:14.151217 24941 caffe.cpp:313] Batch 37, accuracy/top1 = 0.84
I0628 19:36:14.151226 24941 caffe.cpp:313] Batch 37, accuracy/top5 = 0.98
I0628 19:36:14.151227 24941 caffe.cpp:313] Batch 37, loss = 0.668027
I0628 19:36:14.160101 24941 caffe.cpp:313] Batch 38, accuracy/top1 = 0.86
I0628 19:36:14.160109 24941 caffe.cpp:313] Batch 38, accuracy/top5 = 0.96
I0628 19:36:14.160111 24941 caffe.cpp:313] Batch 38, loss = 0.678779
I0628 19:36:14.169024 24941 caffe.cpp:313] Batch 39, accuracy/top1 = 0.9
I0628 19:36:14.169036 24941 caffe.cpp:313] Batch 39, accuracy/top5 = 0.96
I0628 19:36:14.169039 24941 caffe.cpp:313] Batch 39, loss = 0.37776
I0628 19:36:14.177937 24941 caffe.cpp:313] Batch 40, accuracy/top1 = 0.88
I0628 19:36:14.177945 24941 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0628 19:36:14.177948 24941 caffe.cpp:313] Batch 40, loss = 0.630621
I0628 19:36:14.186707 24941 caffe.cpp:313] Batch 41, accuracy/top1 = 0.94
I0628 19:36:14.186714 24941 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0628 19:36:14.186717 24941 caffe.cpp:313] Batch 41, loss = 0.229581
I0628 19:36:14.195590 24941 caffe.cpp:313] Batch 42, accuracy/top1 = 0.92
I0628 19:36:14.195597 24941 caffe.cpp:313] Batch 42, accuracy/top5 = 0.98
I0628 19:36:14.195600 24941 caffe.cpp:313] Batch 42, loss = 0.26178
I0628 19:36:14.204525 24941 caffe.cpp:313] Batch 43, accuracy/top1 = 0.88
I0628 19:36:14.204540 24941 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0628 19:36:14.204543 24941 caffe.cpp:313] Batch 43, loss = 0.289845
I0628 19:36:14.213433 24941 caffe.cpp:313] Batch 44, accuracy/top1 = 0.9
I0628 19:36:14.213440 24941 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0628 19:36:14.213443 24941 caffe.cpp:313] Batch 44, loss = 0.41315
I0628 19:36:14.222287 24941 caffe.cpp:313] Batch 45, accuracy/top1 = 0.9
I0628 19:36:14.222295 24941 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0628 19:36:14.222297 24941 caffe.cpp:313] Batch 45, loss = 0.364181
I0628 19:36:14.231191 24941 caffe.cpp:313] Batch 46, accuracy/top1 = 0.98
I0628 19:36:14.231205 24941 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0628 19:36:14.231209 24941 caffe.cpp:313] Batch 46, loss = 0.0612557
I0628 19:36:14.240114 24941 caffe.cpp:313] Batch 47, accuracy/top1 = 0.82
I0628 19:36:14.240123 24941 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0628 19:36:14.240125 24941 caffe.cpp:313] Batch 47, loss = 0.501736
I0628 19:36:14.249007 24941 caffe.cpp:313] Batch 48, accuracy/top1 = 0.9
I0628 19:36:14.249014 24941 caffe.cpp:313] Batch 48, accuracy/top5 = 0.98
I0628 19:36:14.249027 24941 caffe.cpp:313] Batch 48, loss = 0.703735
I0628 19:36:14.257895 24941 caffe.cpp:313] Batch 49, accuracy/top1 = 0.92
I0628 19:36:14.257906 24941 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0628 19:36:14.257910 24941 caffe.cpp:313] Batch 49, loss = 0.43355
I0628 19:36:14.257911 24941 caffe.cpp:318] Loss: 0.337227
I0628 19:36:14.257920 24941 caffe.cpp:330] accuracy/top1 = 0.9068
I0628 19:36:14.257925 24941 caffe.cpp:330] accuracy/top5 = 0.9956
I0628 19:36:14.257930 24941 caffe.cpp:330] loss = 0.337227 (* 1 = 0.337227 loss)
Logging output to training/cifar10_jacintonet11v2_2017-06-28_18-56-45/train-log_2017-06-28_18-56-45.txt
mkdir: cannot create directory training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial: File exists
training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse
mkdir: cannot create directory training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse: File exists
training/cifar10_jacintonet11v2_2017-06-28_18-56-45/test
mkdir: cannot create directory training/cifar10_jacintonet11v2_2017-06-28_18-56-45/test: File exists
I0629 14:05:54.989639 15179 caffe.cpp:608] This is NVCaffe 0.16.2 started at Thu Jun 29 14:05:54 2017
I0629 14:05:54.989766 15179 caffe.cpp:611] CuDNN version: 6.0.21
I0629 14:05:54.989770 15179 caffe.cpp:612] CuBLAS version: 8000
I0629 14:05:54.989773 15179 caffe.cpp:613] CUDA version: 8000
I0629 14:05:54.989774 15179 caffe.cpp:614] CUDA driver version: 8000
I0629 14:05:54.989779 15179 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0629 14:05:54.990298 15179 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0629 14:05:54.990787 15179 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8277393408, dev_info[0]: total=8506769408 free=8277393408
I0629 14:05:54.990792 15179 caffe.cpp:275] Use GPU with device ID 0
I0629 14:05:54.991070 15179 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0629 14:05:54.992336 15179 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0629 14:05:54.992453 15179 net.cpp:108] Using FLOAT as default forward math type
I0629 14:05:54.992460 15179 net.cpp:114] Using FLOAT as default backward math type
I0629 14:05:54.992465 15179 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0629 14:05:54.992468 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:54.992517 15179 net.cpp:183] Created Layer data (0)
I0629 14:05:54.992524 15179 net.cpp:529] data -> data
I0629 14:05:54.992535 15179 net.cpp:529] data -> label
I0629 14:05:54.992557 15179 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0629 14:05:54.992854 15179 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 14:05:55.038408 15193 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0629 14:05:55.042590 15179 data_layer.cpp:188] ReshapePrefetch 50, 3, 32, 32
I0629 14:05:55.042628 15179 data_layer.cpp:206] Output data size: 50, 3, 32, 32
I0629 14:05:55.042632 15179 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 14:05:55.042656 15179 net.cpp:244] Setting up data
I0629 14:05:55.042667 15179 net.cpp:251] TEST Top shape for layer 0 'data' 50 3 32 32 (153600)
I0629 14:05:55.042673 15179 net.cpp:251] TEST Top shape for layer 0 'data' 50 (50)
I0629 14:05:55.042680 15179 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0629 14:05:55.042683 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.042695 15179 net.cpp:183] Created Layer label_data_1_split (1)
I0629 14:05:55.042699 15179 net.cpp:560] label_data_1_split <- label
I0629 14:05:55.042706 15179 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0629 14:05:55.042711 15179 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0629 14:05:55.042714 15179 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0629 14:05:55.042747 15179 net.cpp:244] Setting up label_data_1_split
I0629 14:05:55.042762 15179 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0629 14:05:55.042765 15179 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0629 14:05:55.042768 15179 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0629 14:05:55.042771 15179 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0629 14:05:55.042774 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.042784 15179 net.cpp:183] Created Layer data/bias (2)
I0629 14:05:55.042788 15179 net.cpp:560] data/bias <- data
I0629 14:05:55.042790 15179 net.cpp:529] data/bias -> data/bias
I0629 14:05:55.043846 15194 data_layer.cpp:188] ReshapePrefetch 50, 3, 32, 32
I0629 14:05:55.043858 15194 data_layer.cpp:206] Output data size: 50, 3, 32, 32
I0629 14:05:55.044900 15194 data_layer.cpp:110] [0] Parser threads: 1
I0629 14:05:55.044909 15194 data_layer.cpp:112] [0] Transformer threads: 1
I0629 14:05:55.044911 15179 net.cpp:244] Setting up data/bias
I0629 14:05:55.044921 15179 net.cpp:251] TEST Top shape for layer 2 'data/bias' 50 3 32 32 (153600)
I0629 14:05:55.044931 15179 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0629 14:05:55.044934 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.044950 15179 net.cpp:183] Created Layer conv1a (3)
I0629 14:05:55.044953 15179 net.cpp:560] conv1a <- data/bias
I0629 14:05:55.044956 15179 net.cpp:529] conv1a -> conv1a
I0629 14:05:55.338217 15179 net.cpp:244] Setting up conv1a
I0629 14:05:55.338238 15179 net.cpp:251] TEST Top shape for layer 3 'conv1a' 50 32 32 32 (1638400)
I0629 14:05:55.338249 15179 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0629 14:05:55.338254 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.338265 15179 net.cpp:183] Created Layer conv1a/bn (4)
I0629 14:05:55.338269 15179 net.cpp:560] conv1a/bn <- conv1a
I0629 14:05:55.338273 15179 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0629 14:05:55.339105 15179 net.cpp:244] Setting up conv1a/bn
I0629 14:05:55.339114 15179 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 50 32 32 32 (1638400)
I0629 14:05:55.339121 15179 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0629 14:05:55.339124 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.339128 15179 net.cpp:183] Created Layer conv1a/relu (5)
I0629 14:05:55.339130 15179 net.cpp:560] conv1a/relu <- conv1a
I0629 14:05:55.339133 15179 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0629 14:05:55.339144 15179 net.cpp:244] Setting up conv1a/relu
I0629 14:05:55.339148 15179 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 50 32 32 32 (1638400)
I0629 14:05:55.339150 15179 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0629 14:05:55.339154 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.339161 15179 net.cpp:183] Created Layer conv1b (6)
I0629 14:05:55.339164 15179 net.cpp:560] conv1b <- conv1a
I0629 14:05:55.339165 15179 net.cpp:529] conv1b -> conv1b
I0629 14:05:55.342173 15179 net.cpp:244] Setting up conv1b
I0629 14:05:55.342182 15179 net.cpp:251] TEST Top shape for layer 6 'conv1b' 50 32 32 32 (1638400)
I0629 14:05:55.342188 15179 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0629 14:05:55.342191 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.342198 15179 net.cpp:183] Created Layer conv1b/bn (7)
I0629 14:05:55.342201 15179 net.cpp:560] conv1b/bn <- conv1b
I0629 14:05:55.342203 15179 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0629 14:05:55.342988 15179 net.cpp:244] Setting up conv1b/bn
I0629 14:05:55.342996 15179 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 50 32 32 32 (1638400)
I0629 14:05:55.343003 15179 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0629 14:05:55.343014 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.343019 15179 net.cpp:183] Created Layer conv1b/relu (8)
I0629 14:05:55.343020 15179 net.cpp:560] conv1b/relu <- conv1b
I0629 14:05:55.343024 15179 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0629 14:05:55.343027 15179 net.cpp:244] Setting up conv1b/relu
I0629 14:05:55.343030 15179 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 50 32 32 32 (1638400)
I0629 14:05:55.343032 15179 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0629 14:05:55.343034 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.343040 15179 net.cpp:183] Created Layer pool1 (9)
I0629 14:05:55.343044 15179 net.cpp:560] pool1 <- conv1b
I0629 14:05:55.343045 15179 net.cpp:529] pool1 -> pool1
I0629 14:05:55.343087 15179 net.cpp:244] Setting up pool1
I0629 14:05:55.343091 15179 net.cpp:251] TEST Top shape for layer 9 'pool1' 50 32 32 32 (1638400)
I0629 14:05:55.343093 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0629 14:05:55.343096 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.343101 15179 net.cpp:183] Created Layer res2a_branch2a (10)
I0629 14:05:55.343103 15179 net.cpp:560] res2a_branch2a <- pool1
I0629 14:05:55.343106 15179 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0629 14:05:55.348008 15179 net.cpp:244] Setting up res2a_branch2a
I0629 14:05:55.348019 15179 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 50 64 32 32 (3276800)
I0629 14:05:55.348026 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.348028 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.348033 15179 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0629 14:05:55.348036 15179 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0629 14:05:55.348038 15179 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0629 14:05:55.348865 15179 net.cpp:244] Setting up res2a_branch2a/bn
I0629 14:05:55.348875 15179 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 32 32 (3276800)
I0629 14:05:55.348881 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.348882 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.348886 15179 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0629 14:05:55.348887 15179 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0629 14:05:55.348891 15179 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0629 14:05:55.348893 15179 net.cpp:244] Setting up res2a_branch2a/relu
I0629 14:05:55.348896 15179 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 32 32 (3276800)
I0629 14:05:55.348898 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0629 14:05:55.348901 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.348906 15179 net.cpp:183] Created Layer res2a_branch2b (13)
I0629 14:05:55.348907 15179 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0629 14:05:55.348912 15179 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0629 14:05:55.351924 15179 net.cpp:244] Setting up res2a_branch2b
I0629 14:05:55.351935 15179 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 50 64 32 32 (3276800)
I0629 14:05:55.351940 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.351943 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.351948 15179 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0629 14:05:55.351950 15179 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0629 14:05:55.351953 15179 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0629 14:05:55.352797 15179 net.cpp:244] Setting up res2a_branch2b/bn
I0629 14:05:55.352807 15179 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 32 32 (3276800)
I0629 14:05:55.352813 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.352815 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.352818 15179 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0629 14:05:55.352821 15179 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0629 14:05:55.352823 15179 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0629 14:05:55.352828 15179 net.cpp:244] Setting up res2a_branch2b/relu
I0629 14:05:55.352829 15179 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 32 32 (3276800)
I0629 14:05:55.352831 15179 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0629 14:05:55.352833 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.352838 15179 net.cpp:183] Created Layer pool2 (16)
I0629 14:05:55.352840 15179 net.cpp:560] pool2 <- res2a_branch2b
I0629 14:05:55.352843 15179 net.cpp:529] pool2 -> pool2
I0629 14:05:55.352876 15179 net.cpp:244] Setting up pool2
I0629 14:05:55.352880 15179 net.cpp:251] TEST Top shape for layer 16 'pool2' 50 64 16 16 (819200)
I0629 14:05:55.352882 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0629 14:05:55.352885 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.352890 15179 net.cpp:183] Created Layer res3a_branch2a (17)
I0629 14:05:55.352892 15179 net.cpp:560] res3a_branch2a <- pool2
I0629 14:05:55.352895 15179 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0629 14:05:55.358002 15179 net.cpp:244] Setting up res3a_branch2a
I0629 14:05:55.358012 15179 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 50 128 16 16 (1638400)
I0629 14:05:55.358017 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.358019 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.358024 15179 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0629 14:05:55.358026 15179 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0629 14:05:55.358029 15179 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0629 14:05:55.358814 15179 net.cpp:244] Setting up res3a_branch2a/bn
I0629 14:05:55.358821 15179 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 16 16 (1638400)
I0629 14:05:55.358829 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.358831 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.358835 15179 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0629 14:05:55.358837 15179 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0629 14:05:55.358839 15179 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0629 14:05:55.358844 15179 net.cpp:244] Setting up res3a_branch2a/relu
I0629 14:05:55.358846 15179 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 16 16 (1638400)
I0629 14:05:55.358849 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0629 14:05:55.358850 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.358857 15179 net.cpp:183] Created Layer res3a_branch2b (20)
I0629 14:05:55.358860 15179 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0629 14:05:55.358862 15179 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0629 14:05:55.361106 15179 net.cpp:244] Setting up res3a_branch2b
I0629 14:05:55.361115 15179 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 50 128 16 16 (1638400)
I0629 14:05:55.361120 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.361124 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.361135 15179 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0629 14:05:55.361138 15179 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0629 14:05:55.361141 15179 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0629 14:05:55.361925 15179 net.cpp:244] Setting up res3a_branch2b/bn
I0629 14:05:55.361934 15179 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 16 16 (1638400)
I0629 14:05:55.361940 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.361943 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.361945 15179 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0629 14:05:55.361948 15179 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0629 14:05:55.361950 15179 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0629 14:05:55.361954 15179 net.cpp:244] Setting up res3a_branch2b/relu
I0629 14:05:55.361956 15179 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 16 16 (1638400)
I0629 14:05:55.361958 15179 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0629 14:05:55.361960 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.361963 15179 net.cpp:183] Created Layer pool3 (23)
I0629 14:05:55.361965 15179 net.cpp:560] pool3 <- res3a_branch2b
I0629 14:05:55.361968 15179 net.cpp:529] pool3 -> pool3
I0629 14:05:55.362002 15179 net.cpp:244] Setting up pool3
I0629 14:05:55.362006 15179 net.cpp:251] TEST Top shape for layer 23 'pool3' 50 128 16 16 (1638400)
I0629 14:05:55.362009 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0629 14:05:55.362010 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.362016 15179 net.cpp:183] Created Layer res4a_branch2a (24)
I0629 14:05:55.362018 15179 net.cpp:560] res4a_branch2a <- pool3
I0629 14:05:55.362021 15179 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0629 14:05:55.375603 15179 net.cpp:244] Setting up res4a_branch2a
I0629 14:05:55.375623 15179 net.cpp:251] TEST Top shape for layer 24 'res4a_branch2a' 50 256 16 16 (3276800)
I0629 14:05:55.375630 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.375634 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.375641 15179 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0629 14:05:55.375644 15179 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0629 14:05:55.375648 15179 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0629 14:05:55.376619 15179 net.cpp:244] Setting up res4a_branch2a/bn
I0629 14:05:55.376629 15179 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 16 16 (3276800)
I0629 14:05:55.376636 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.376638 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.376641 15179 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0629 14:05:55.376643 15179 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0629 14:05:55.376646 15179 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0629 14:05:55.376651 15179 net.cpp:244] Setting up res4a_branch2a/relu
I0629 14:05:55.376652 15179 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 16 16 (3276800)
I0629 14:05:55.376654 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0629 14:05:55.376657 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.376663 15179 net.cpp:183] Created Layer res4a_branch2b (27)
I0629 14:05:55.376667 15179 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0629 14:05:55.376668 15179 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0629 14:05:55.382719 15179 net.cpp:244] Setting up res4a_branch2b
I0629 14:05:55.382740 15179 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2b' 50 256 16 16 (3276800)
I0629 14:05:55.382745 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.382748 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.382752 15179 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0629 14:05:55.382755 15179 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0629 14:05:55.382757 15179 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0629 14:05:55.383599 15179 net.cpp:244] Setting up res4a_branch2b/bn
I0629 14:05:55.383608 15179 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 16 16 (3276800)
I0629 14:05:55.383615 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.383617 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.383620 15179 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0629 14:05:55.383622 15179 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0629 14:05:55.383625 15179 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0629 14:05:55.383630 15179 net.cpp:244] Setting up res4a_branch2b/relu
I0629 14:05:55.383631 15179 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 16 16 (3276800)
I0629 14:05:55.383633 15179 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0629 14:05:55.383635 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.383640 15179 net.cpp:183] Created Layer pool4 (30)
I0629 14:05:55.383641 15179 net.cpp:560] pool4 <- res4a_branch2b
I0629 14:05:55.383644 15179 net.cpp:529] pool4 -> pool4
I0629 14:05:55.383678 15179 net.cpp:244] Setting up pool4
I0629 14:05:55.383682 15179 net.cpp:251] TEST Top shape for layer 30 'pool4' 50 256 8 8 (819200)
I0629 14:05:55.383684 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0629 14:05:55.383687 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.383693 15179 net.cpp:183] Created Layer res5a_branch2a (31)
I0629 14:05:55.383697 15179 net.cpp:560] res5a_branch2a <- pool4
I0629 14:05:55.383698 15179 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0629 14:05:55.420037 15179 net.cpp:244] Setting up res5a_branch2a
I0629 14:05:55.420058 15179 net.cpp:251] TEST Top shape for layer 31 'res5a_branch2a' 50 512 8 8 (1638400)
I0629 14:05:55.420063 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.420068 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.420075 15179 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0629 14:05:55.420079 15179 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0629 14:05:55.420083 15179 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0629 14:05:55.420928 15179 net.cpp:244] Setting up res5a_branch2a/bn
I0629 14:05:55.420936 15179 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 8 8 (1638400)
I0629 14:05:55.420943 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.420945 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.420948 15179 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0629 14:05:55.420951 15179 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0629 14:05:55.420953 15179 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0629 14:05:55.420958 15179 net.cpp:244] Setting up res5a_branch2a/relu
I0629 14:05:55.420960 15179 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 8 8 (1638400)
I0629 14:05:55.420963 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0629 14:05:55.420964 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.420986 15179 net.cpp:183] Created Layer res5a_branch2b (34)
I0629 14:05:55.420989 15179 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0629 14:05:55.420992 15179 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0629 14:05:55.438122 15179 net.cpp:244] Setting up res5a_branch2b
I0629 14:05:55.438144 15179 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2b' 50 512 8 8 (1638400)
I0629 14:05:55.438158 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.438163 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.438172 15179 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0629 14:05:55.438177 15179 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0629 14:05:55.438182 15179 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0629 14:05:55.439018 15179 net.cpp:244] Setting up res5a_branch2b/bn
I0629 14:05:55.439025 15179 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 8 8 (1638400)
I0629 14:05:55.439031 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.439034 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439038 15179 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0629 14:05:55.439040 15179 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0629 14:05:55.439043 15179 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0629 14:05:55.439046 15179 net.cpp:244] Setting up res5a_branch2b/relu
I0629 14:05:55.439049 15179 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 8 8 (1638400)
I0629 14:05:55.439051 15179 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0629 14:05:55.439054 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439057 15179 net.cpp:183] Created Layer pool5 (37)
I0629 14:05:55.439060 15179 net.cpp:560] pool5 <- res5a_branch2b
I0629 14:05:55.439064 15179 net.cpp:529] pool5 -> pool5
I0629 14:05:55.439082 15179 net.cpp:244] Setting up pool5
I0629 14:05:55.439086 15179 net.cpp:251] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0629 14:05:55.439088 15179 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0629 14:05:55.439090 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439096 15179 net.cpp:183] Created Layer fc10 (38)
I0629 14:05:55.439098 15179 net.cpp:560] fc10 <- pool5
I0629 14:05:55.439100 15179 net.cpp:529] fc10 -> fc10
I0629 14:05:55.439301 15179 net.cpp:244] Setting up fc10
I0629 14:05:55.439307 15179 net.cpp:251] TEST Top shape for layer 38 'fc10' 50 10 (500)
I0629 14:05:55.439311 15179 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0629 14:05:55.439313 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439316 15179 net.cpp:183] Created Layer fc10_fc10_0_split (39)
I0629 14:05:55.439318 15179 net.cpp:560] fc10_fc10_0_split <- fc10
I0629 14:05:55.439321 15179 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0629 14:05:55.439323 15179 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0629 14:05:55.439327 15179 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0629 14:05:55.439354 15179 net.cpp:244] Setting up fc10_fc10_0_split
I0629 14:05:55.439358 15179 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0629 14:05:55.439360 15179 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0629 14:05:55.439363 15179 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0629 14:05:55.439364 15179 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0629 14:05:55.439366 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439374 15179 net.cpp:183] Created Layer loss (40)
I0629 14:05:55.439376 15179 net.cpp:560] loss <- fc10_fc10_0_split_0
I0629 14:05:55.439391 15179 net.cpp:560] loss <- label_data_1_split_0
I0629 14:05:55.439394 15179 net.cpp:529] loss -> loss
I0629 14:05:55.439499 15179 net.cpp:244] Setting up loss
I0629 14:05:55.439505 15179 net.cpp:251] TEST Top shape for layer 40 'loss' (1)
I0629 14:05:55.439507 15179 net.cpp:255]     with loss weight 1
I0629 14:05:55.439518 15179 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0629 14:05:55.439522 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439529 15179 net.cpp:183] Created Layer accuracy/top1 (41)
I0629 14:05:55.439532 15179 net.cpp:560] accuracy/top1 <- fc10_fc10_0_split_1
I0629 14:05:55.439535 15179 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0629 14:05:55.439538 15179 net.cpp:529] accuracy/top1 -> accuracy/top1
I0629 14:05:55.439546 15179 net.cpp:244] Setting up accuracy/top1
I0629 14:05:55.439550 15179 net.cpp:251] TEST Top shape for layer 41 'accuracy/top1' (1)
I0629 14:05:55.439553 15179 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0629 14:05:55.439554 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439560 15179 net.cpp:183] Created Layer accuracy/top5 (42)
I0629 14:05:55.439563 15179 net.cpp:560] accuracy/top5 <- fc10_fc10_0_split_2
I0629 14:05:55.439565 15179 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0629 14:05:55.439568 15179 net.cpp:529] accuracy/top5 -> accuracy/top5
I0629 14:05:55.439571 15179 net.cpp:244] Setting up accuracy/top5
I0629 14:05:55.439574 15179 net.cpp:251] TEST Top shape for layer 42 'accuracy/top5' (1)
I0629 14:05:55.439576 15179 net.cpp:324] accuracy/top5 does not need backward computation.
I0629 14:05:55.439579 15179 net.cpp:324] accuracy/top1 does not need backward computation.
I0629 14:05:55.439581 15179 net.cpp:322] loss needs backward computation.
I0629 14:05:55.439584 15179 net.cpp:322] fc10_fc10_0_split needs backward computation.
I0629 14:05:55.439585 15179 net.cpp:322] fc10 needs backward computation.
I0629 14:05:55.439586 15179 net.cpp:322] pool5 needs backward computation.
I0629 14:05:55.439589 15179 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0629 14:05:55.439590 15179 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0629 14:05:55.439592 15179 net.cpp:322] res5a_branch2b needs backward computation.
I0629 14:05:55.439595 15179 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0629 14:05:55.439599 15179 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0629 14:05:55.439602 15179 net.cpp:322] res5a_branch2a needs backward computation.
I0629 14:05:55.439605 15179 net.cpp:322] pool4 needs backward computation.
I0629 14:05:55.439609 15179 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0629 14:05:55.439611 15179 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0629 14:05:55.439615 15179 net.cpp:322] res4a_branch2b needs backward computation.
I0629 14:05:55.439620 15179 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0629 14:05:55.439621 15179 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0629 14:05:55.439625 15179 net.cpp:322] res4a_branch2a needs backward computation.
I0629 14:05:55.439627 15179 net.cpp:322] pool3 needs backward computation.
I0629 14:05:55.439631 15179 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0629 14:05:55.439633 15179 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0629 14:05:55.439635 15179 net.cpp:322] res3a_branch2b needs backward computation.
I0629 14:05:55.439637 15179 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0629 14:05:55.439640 15179 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0629 14:05:55.439643 15179 net.cpp:322] res3a_branch2a needs backward computation.
I0629 14:05:55.439646 15179 net.cpp:322] pool2 needs backward computation.
I0629 14:05:55.439649 15179 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0629 14:05:55.439652 15179 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0629 14:05:55.439661 15179 net.cpp:322] res2a_branch2b needs backward computation.
I0629 14:05:55.439663 15179 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0629 14:05:55.439667 15179 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0629 14:05:55.439669 15179 net.cpp:322] res2a_branch2a needs backward computation.
I0629 14:05:55.439673 15179 net.cpp:322] pool1 needs backward computation.
I0629 14:05:55.439677 15179 net.cpp:322] conv1b/relu needs backward computation.
I0629 14:05:55.439680 15179 net.cpp:322] conv1b/bn needs backward computation.
I0629 14:05:55.439682 15179 net.cpp:322] conv1b needs backward computation.
I0629 14:05:55.439687 15179 net.cpp:322] conv1a/relu needs backward computation.
I0629 14:05:55.439688 15179 net.cpp:322] conv1a/bn needs backward computation.
I0629 14:05:55.439692 15179 net.cpp:322] conv1a needs backward computation.
I0629 14:05:55.439695 15179 net.cpp:324] data/bias does not need backward computation.
I0629 14:05:55.439699 15179 net.cpp:324] label_data_1_split does not need backward computation.
I0629 14:05:55.439704 15179 net.cpp:324] data does not need backward computation.
I0629 14:05:55.439707 15179 net.cpp:366] This network produces output accuracy/top1
I0629 14:05:55.439710 15179 net.cpp:366] This network produces output accuracy/top5
I0629 14:05:55.439713 15179 net.cpp:366] This network produces output loss
I0629 14:05:55.439744 15179 net.cpp:388] Top memory (TEST) required for data: 275251200 diff: 183500808
I0629 14:05:55.439748 15179 net.cpp:391] Bottom memory (TEST) required for data: 275251200 diff: 275251200
I0629 14:05:55.439749 15179 net.cpp:394] Shared (in-place) memory (TEST) by data: 183500800 diff: 183500800
I0629 14:05:55.439751 15179 net.cpp:397] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0629 14:05:55.439754 15179 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0629 14:05:55.439757 15179 net.cpp:406] Network initialization done.
I0629 14:05:55.512706 15179 net.cpp:1087] Copying source layer data Type:Data #blobs=0
I0629 14:05:55.512760 15179 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0629 14:05:55.512840 15179 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0629 14:05:55.512902 15179 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.513427 15179 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0629 14:05:55.513453 15179 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0629 14:05:55.513489 15179 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.513931 15179 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0629 14:05:55.513957 15179 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0629 14:05:55.513967 15179 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.514017 15179 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.514402 15179 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.514425 15179 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.514467 15179 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.514830 15179 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.514853 15179 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0629 14:05:55.514863 15179 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.514960 15179 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.515283 15179 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.515303 15179 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.515368 15179 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.515745 15179 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.515769 15179 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0629 14:05:55.515780 15179 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.516077 15179 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.516429 15179 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.516451 15179 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.516618 15179 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.516950 15179 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.516973 15179 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0629 14:05:55.516983 15179 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.518025 15179 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.518213 15179 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.518216 15179 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.518380 15179 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.518461 15179 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.518465 15179 net.cpp:1087] Copying source layer pool5 Type:Pooling #blobs=0
I0629 14:05:55.518466 15179 net.cpp:1087] Copying source layer fc10 Type:InnerProduct #blobs=2
I0629 14:05:55.518476 15179 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0629 14:05:55.518517 15179 caffe.cpp:290] Running for 200 iterations.
I0629 14:05:55.568866 15179 caffe.cpp:313] Batch 0, accuracy/top1 = 0.94
I0629 14:05:55.568887 15179 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0629 14:05:55.568892 15179 caffe.cpp:313] Batch 0, loss = 0.153593
I0629 14:05:55.610738 15179 caffe.cpp:313] Batch 1, accuracy/top1 = 0.94
I0629 14:05:55.610757 15179 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0629 14:05:55.610760 15179 caffe.cpp:313] Batch 1, loss = 0.174427
I0629 14:05:55.620101 15179 caffe.cpp:313] Batch 2, accuracy/top1 = 0.96
I0629 14:05:55.620110 15179 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0629 14:05:55.620112 15179 caffe.cpp:313] Batch 2, loss = 0.186237
I0629 14:05:55.629480 15179 caffe.cpp:313] Batch 3, accuracy/top1 = 0.92
I0629 14:05:55.629489 15179 caffe.cpp:313] Batch 3, accuracy/top5 = 1
I0629 14:05:55.629492 15179 caffe.cpp:313] Batch 3, loss = 0.335301
I0629 14:05:55.638900 15179 caffe.cpp:313] Batch 4, accuracy/top1 = 0.94
I0629 14:05:55.638914 15179 caffe.cpp:313] Batch 4, accuracy/top5 = 1
I0629 14:05:55.638918 15179 caffe.cpp:313] Batch 4, loss = 0.219656
I0629 14:05:55.648586 15179 caffe.cpp:313] Batch 5, accuracy/top1 = 0.86
I0629 14:05:55.648603 15179 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0629 14:05:55.648609 15179 caffe.cpp:313] Batch 5, loss = 0.337282
I0629 14:05:55.657981 15179 caffe.cpp:313] Batch 6, accuracy/top1 = 0.9
I0629 14:05:55.657990 15179 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0629 14:05:55.657994 15179 caffe.cpp:313] Batch 6, loss = 0.18802
I0629 14:05:55.667291 15179 caffe.cpp:313] Batch 7, accuracy/top1 = 0.88
I0629 14:05:55.667299 15179 caffe.cpp:313] Batch 7, accuracy/top5 = 0.98
I0629 14:05:55.667301 15179 caffe.cpp:313] Batch 7, loss = 0.513992
I0629 14:05:55.676977 15179 caffe.cpp:313] Batch 8, accuracy/top1 = 0.9
I0629 14:05:55.676998 15179 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0629 14:05:55.677002 15179 caffe.cpp:313] Batch 8, loss = 0.217647
I0629 14:05:55.686403 15179 caffe.cpp:313] Batch 9, accuracy/top1 = 0.92
I0629 14:05:55.686414 15179 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0629 14:05:55.686417 15179 caffe.cpp:313] Batch 9, loss = 0.352723
I0629 14:05:55.695765 15179 caffe.cpp:313] Batch 10, accuracy/top1 = 0.96
I0629 14:05:55.695782 15179 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0629 14:05:55.695785 15179 caffe.cpp:313] Batch 10, loss = 0.155797
I0629 14:05:55.696086 15194 blocking_queue.cpp:40] Waiting for datum
I0629 14:05:55.705329 15179 caffe.cpp:313] Batch 11, accuracy/top1 = 0.96
I0629 14:05:55.705346 15179 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0629 14:05:55.705351 15179 caffe.cpp:313] Batch 11, loss = 0.170298
I0629 14:05:55.714689 15179 caffe.cpp:313] Batch 12, accuracy/top1 = 0.98
I0629 14:05:55.714699 15179 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0629 14:05:55.714702 15179 caffe.cpp:313] Batch 12, loss = 0.133915
I0629 14:05:55.723978 15179 caffe.cpp:313] Batch 13, accuracy/top1 = 0.9
I0629 14:05:55.723986 15179 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0629 14:05:55.723989 15179 caffe.cpp:313] Batch 13, loss = 0.407794
I0629 14:05:55.733103 15179 caffe.cpp:313] Batch 14, accuracy/top1 = 0.88
I0629 14:05:55.733119 15179 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0629 14:05:55.733124 15179 caffe.cpp:313] Batch 14, loss = 0.397547
I0629 14:05:55.742380 15179 caffe.cpp:313] Batch 15, accuracy/top1 = 0.9
I0629 14:05:55.742391 15179 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0629 14:05:55.742394 15179 caffe.cpp:313] Batch 15, loss = 0.377622
I0629 14:05:55.751356 15179 caffe.cpp:313] Batch 16, accuracy/top1 = 0.92
I0629 14:05:55.751363 15179 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0629 14:05:55.751366 15179 caffe.cpp:313] Batch 16, loss = 0.385913
I0629 14:05:55.760404 15179 caffe.cpp:313] Batch 17, accuracy/top1 = 0.88
I0629 14:05:55.760418 15179 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0629 14:05:55.760424 15179 caffe.cpp:313] Batch 17, loss = 0.332212
I0629 14:05:55.769727 15179 caffe.cpp:313] Batch 18, accuracy/top1 = 0.9
I0629 14:05:55.769742 15179 caffe.cpp:313] Batch 18, accuracy/top5 = 1
I0629 14:05:55.769744 15179 caffe.cpp:313] Batch 18, loss = 0.29718
I0629 14:05:55.778771 15179 caffe.cpp:313] Batch 19, accuracy/top1 = 0.92
I0629 14:05:55.778780 15179 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0629 14:05:55.778784 15179 caffe.cpp:313] Batch 19, loss = 0.258859
I0629 14:05:55.787740 15179 caffe.cpp:313] Batch 20, accuracy/top1 = 0.86
I0629 14:05:55.787748 15179 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0629 14:05:55.787751 15179 caffe.cpp:313] Batch 20, loss = 0.319892
I0629 14:05:55.796998 15179 caffe.cpp:313] Batch 21, accuracy/top1 = 0.96
I0629 14:05:55.797015 15179 caffe.cpp:313] Batch 21, accuracy/top5 = 1
I0629 14:05:55.797020 15179 caffe.cpp:313] Batch 21, loss = 0.261385
I0629 14:05:55.806157 15179 caffe.cpp:313] Batch 22, accuracy/top1 = 0.88
I0629 14:05:55.806169 15179 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0629 14:05:55.806171 15179 caffe.cpp:313] Batch 22, loss = 0.515505
I0629 14:05:55.815160 15179 caffe.cpp:313] Batch 23, accuracy/top1 = 0.9
I0629 14:05:55.815170 15179 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0629 14:05:55.815171 15179 caffe.cpp:313] Batch 23, loss = 0.343264
I0629 14:05:55.824321 15179 caffe.cpp:313] Batch 24, accuracy/top1 = 0.88
I0629 14:05:55.824338 15179 caffe.cpp:313] Batch 24, accuracy/top5 = 0.98
I0629 14:05:55.824342 15179 caffe.cpp:313] Batch 24, loss = 0.301905
I0629 14:05:55.833505 15179 caffe.cpp:313] Batch 25, accuracy/top1 = 0.92
I0629 14:05:55.833516 15179 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0629 14:05:55.833519 15179 caffe.cpp:313] Batch 25, loss = 0.22202
I0629 14:05:55.842497 15179 caffe.cpp:313] Batch 26, accuracy/top1 = 0.9
I0629 14:05:55.842505 15179 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0629 14:05:55.842509 15179 caffe.cpp:313] Batch 26, loss = 0.442399
I0629 14:05:55.851617 15179 caffe.cpp:313] Batch 27, accuracy/top1 = 0.94
I0629 14:05:55.851644 15179 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0629 14:05:55.851650 15179 caffe.cpp:313] Batch 27, loss = 0.199298
I0629 14:05:55.861085 15179 caffe.cpp:313] Batch 28, accuracy/top1 = 0.88
I0629 14:05:55.861104 15179 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0629 14:05:55.861107 15179 caffe.cpp:313] Batch 28, loss = 0.399529
I0629 14:05:55.870127 15179 caffe.cpp:313] Batch 29, accuracy/top1 = 0.9
I0629 14:05:55.870153 15179 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0629 14:05:55.870157 15179 caffe.cpp:313] Batch 29, loss = 0.456926
I0629 14:05:55.879230 15179 caffe.cpp:313] Batch 30, accuracy/top1 = 0.9
I0629 14:05:55.879246 15179 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0629 14:05:55.879248 15179 caffe.cpp:313] Batch 30, loss = 0.465783
I0629 14:05:55.888532 15179 caffe.cpp:313] Batch 31, accuracy/top1 = 0.88
I0629 14:05:55.888551 15179 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0629 14:05:55.888553 15179 caffe.cpp:313] Batch 31, loss = 0.249379
I0629 14:05:55.897526 15179 caffe.cpp:313] Batch 32, accuracy/top1 = 0.9
I0629 14:05:55.897541 15179 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0629 14:05:55.897544 15179 caffe.cpp:313] Batch 32, loss = 0.252434
I0629 14:05:55.906442 15179 caffe.cpp:313] Batch 33, accuracy/top1 = 0.96
I0629 14:05:55.906461 15179 caffe.cpp:313] Batch 33, accuracy/top5 = 0.98
I0629 14:05:55.906463 15179 caffe.cpp:313] Batch 33, loss = 0.195247
I0629 14:05:55.915365 15179 caffe.cpp:313] Batch 34, accuracy/top1 = 0.88
I0629 14:05:55.915382 15179 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0629 14:05:55.915385 15179 caffe.cpp:313] Batch 34, loss = 0.454933
I0629 14:05:55.924346 15179 caffe.cpp:313] Batch 35, accuracy/top1 = 0.94
I0629 14:05:55.924363 15179 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0629 14:05:55.924365 15179 caffe.cpp:313] Batch 35, loss = 0.20312
I0629 14:05:55.933226 15179 caffe.cpp:313] Batch 36, accuracy/top1 = 0.86
I0629 14:05:55.933240 15179 caffe.cpp:313] Batch 36, accuracy/top5 = 0.98
I0629 14:05:55.933243 15179 caffe.cpp:313] Batch 36, loss = 0.368316
I0629 14:05:55.942157 15179 caffe.cpp:313] Batch 37, accuracy/top1 = 0.84
I0629 14:05:55.942173 15179 caffe.cpp:313] Batch 37, accuracy/top5 = 0.98
I0629 14:05:55.942175 15179 caffe.cpp:313] Batch 37, loss = 0.668027
I0629 14:05:55.951097 15179 caffe.cpp:313] Batch 38, accuracy/top1 = 0.86
I0629 14:05:55.951115 15179 caffe.cpp:313] Batch 38, accuracy/top5 = 0.96
I0629 14:05:55.951118 15179 caffe.cpp:313] Batch 38, loss = 0.678779
I0629 14:05:55.959897 15179 caffe.cpp:313] Batch 39, accuracy/top1 = 0.9
I0629 14:05:55.959908 15179 caffe.cpp:313] Batch 39, accuracy/top5 = 0.96
I0629 14:05:55.959911 15179 caffe.cpp:313] Batch 39, loss = 0.37776
I0629 14:05:55.968797 15179 caffe.cpp:313] Batch 40, accuracy/top1 = 0.88
I0629 14:05:55.968816 15179 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0629 14:05:55.968817 15179 caffe.cpp:313] Batch 40, loss = 0.630621
I0629 14:05:55.977754 15179 caffe.cpp:313] Batch 41, accuracy/top1 = 0.94
I0629 14:05:55.977771 15179 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0629 14:05:55.977773 15179 caffe.cpp:313] Batch 41, loss = 0.229581
I0629 14:05:55.986656 15179 caffe.cpp:313] Batch 42, accuracy/top1 = 0.92
I0629 14:05:55.986672 15179 caffe.cpp:313] Batch 42, accuracy/top5 = 0.98
I0629 14:05:55.986675 15179 caffe.cpp:313] Batch 42, loss = 0.26178
I0629 14:05:55.995764 15179 caffe.cpp:313] Batch 43, accuracy/top1 = 0.88
I0629 14:05:55.995779 15179 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0629 14:05:55.995782 15179 caffe.cpp:313] Batch 43, loss = 0.289845
I0629 14:05:56.004725 15179 caffe.cpp:313] Batch 44, accuracy/top1 = 0.9
I0629 14:05:56.004750 15179 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0629 14:05:56.004752 15179 caffe.cpp:313] Batch 44, loss = 0.41315
I0629 14:05:56.013706 15179 caffe.cpp:313] Batch 45, accuracy/top1 = 0.9
I0629 14:05:56.013721 15179 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0629 14:05:56.013723 15179 caffe.cpp:313] Batch 45, loss = 0.364181
I0629 14:05:56.022577 15179 caffe.cpp:313] Batch 46, accuracy/top1 = 0.98
I0629 14:05:56.022586 15179 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0629 14:05:56.022588 15179 caffe.cpp:313] Batch 46, loss = 0.0612557
I0629 14:05:56.031394 15179 caffe.cpp:313] Batch 47, accuracy/top1 = 0.82
I0629 14:05:56.031404 15179 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0629 14:05:56.031406 15179 caffe.cpp:313] Batch 47, loss = 0.501736
I0629 14:05:56.040259 15179 caffe.cpp:313] Batch 48, accuracy/top1 = 0.9
I0629 14:05:56.040288 15179 caffe.cpp:313] Batch 48, accuracy/top5 = 0.98
I0629 14:05:56.040292 15179 caffe.cpp:313] Batch 48, loss = 0.703735
I0629 14:05:56.049124 15179 caffe.cpp:313] Batch 49, accuracy/top1 = 0.92
I0629 14:05:56.049134 15179 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0629 14:05:56.049136 15179 caffe.cpp:313] Batch 49, loss = 0.43355
I0629 14:05:56.057945 15179 caffe.cpp:313] Batch 50, accuracy/top1 = 0.8
I0629 14:05:56.057953 15179 caffe.cpp:313] Batch 50, accuracy/top5 = 0.98
I0629 14:05:56.057956 15179 caffe.cpp:313] Batch 50, loss = 0.751408
I0629 14:05:56.066776 15179 caffe.cpp:313] Batch 51, accuracy/top1 = 0.88
I0629 14:05:56.066787 15179 caffe.cpp:313] Batch 51, accuracy/top5 = 1
I0629 14:05:56.066790 15179 caffe.cpp:313] Batch 51, loss = 0.350897
I0629 14:05:56.075639 15179 caffe.cpp:313] Batch 52, accuracy/top1 = 0.88
I0629 14:05:56.075649 15179 caffe.cpp:313] Batch 52, accuracy/top5 = 1
I0629 14:05:56.075650 15179 caffe.cpp:313] Batch 52, loss = 0.247005
I0629 14:05:56.084450 15179 caffe.cpp:313] Batch 53, accuracy/top1 = 0.96
I0629 14:05:56.084458 15179 caffe.cpp:313] Batch 53, accuracy/top5 = 1
I0629 14:05:56.084461 15179 caffe.cpp:313] Batch 53, loss = 0.230147
I0629 14:05:56.093271 15179 caffe.cpp:313] Batch 54, accuracy/top1 = 0.92
I0629 14:05:56.093279 15179 caffe.cpp:313] Batch 54, accuracy/top5 = 1
I0629 14:05:56.093281 15179 caffe.cpp:313] Batch 54, loss = 0.270494
I0629 14:05:56.102157 15179 caffe.cpp:313] Batch 55, accuracy/top1 = 0.86
I0629 14:05:56.102174 15179 caffe.cpp:313] Batch 55, accuracy/top5 = 1
I0629 14:05:56.102177 15179 caffe.cpp:313] Batch 55, loss = 0.429864
I0629 14:05:56.111063 15179 caffe.cpp:313] Batch 56, accuracy/top1 = 0.84
I0629 14:05:56.111071 15179 caffe.cpp:313] Batch 56, accuracy/top5 = 0.96
I0629 14:05:56.111074 15179 caffe.cpp:313] Batch 56, loss = 0.646045
I0629 14:05:56.119858 15179 caffe.cpp:313] Batch 57, accuracy/top1 = 0.92
I0629 14:05:56.119865 15179 caffe.cpp:313] Batch 57, accuracy/top5 = 1
I0629 14:05:56.119868 15179 caffe.cpp:313] Batch 57, loss = 0.264563
I0629 14:05:56.128718 15179 caffe.cpp:313] Batch 58, accuracy/top1 = 0.9
I0629 14:05:56.128731 15179 caffe.cpp:313] Batch 58, accuracy/top5 = 0.98
I0629 14:05:56.128732 15179 caffe.cpp:313] Batch 58, loss = 0.357769
I0629 14:05:56.137574 15179 caffe.cpp:313] Batch 59, accuracy/top1 = 0.88
I0629 14:05:56.137583 15179 caffe.cpp:313] Batch 59, accuracy/top5 = 1
I0629 14:05:56.137585 15179 caffe.cpp:313] Batch 59, loss = 0.414792
I0629 14:05:56.146410 15179 caffe.cpp:313] Batch 60, accuracy/top1 = 0.92
I0629 14:05:56.146419 15179 caffe.cpp:313] Batch 60, accuracy/top5 = 0.98
I0629 14:05:56.146420 15179 caffe.cpp:313] Batch 60, loss = 0.203291
I0629 14:05:56.155230 15179 caffe.cpp:313] Batch 61, accuracy/top1 = 0.84
I0629 14:05:56.155238 15179 caffe.cpp:313] Batch 61, accuracy/top5 = 0.98
I0629 14:05:56.155241 15179 caffe.cpp:313] Batch 61, loss = 0.444668
I0629 14:05:56.164078 15179 caffe.cpp:313] Batch 62, accuracy/top1 = 0.98
I0629 14:05:56.164096 15179 caffe.cpp:313] Batch 62, accuracy/top5 = 1
I0629 14:05:56.164098 15179 caffe.cpp:313] Batch 62, loss = 0.074199
I0629 14:05:56.172924 15179 caffe.cpp:313] Batch 63, accuracy/top1 = 0.82
I0629 14:05:56.172932 15179 caffe.cpp:313] Batch 63, accuracy/top5 = 1
I0629 14:05:56.172935 15179 caffe.cpp:313] Batch 63, loss = 0.428295
I0629 14:05:56.181732 15179 caffe.cpp:313] Batch 64, accuracy/top1 = 0.92
I0629 14:05:56.181740 15179 caffe.cpp:313] Batch 64, accuracy/top5 = 0.98
I0629 14:05:56.181742 15179 caffe.cpp:313] Batch 64, loss = 0.317228
I0629 14:05:56.190567 15179 caffe.cpp:313] Batch 65, accuracy/top1 = 0.94
I0629 14:05:56.190580 15179 caffe.cpp:313] Batch 65, accuracy/top5 = 1
I0629 14:05:56.190582 15179 caffe.cpp:313] Batch 65, loss = 0.199978
I0629 14:05:56.199424 15179 caffe.cpp:313] Batch 66, accuracy/top1 = 0.9
I0629 14:05:56.199434 15179 caffe.cpp:313] Batch 66, accuracy/top5 = 1
I0629 14:05:56.199435 15179 caffe.cpp:313] Batch 66, loss = 0.274872
I0629 14:05:56.208261 15179 caffe.cpp:313] Batch 67, accuracy/top1 = 0.96
I0629 14:05:56.208278 15179 caffe.cpp:313] Batch 67, accuracy/top5 = 1
I0629 14:05:56.208281 15179 caffe.cpp:313] Batch 67, loss = 0.181648
I0629 14:05:56.217080 15179 caffe.cpp:313] Batch 68, accuracy/top1 = 0.88
I0629 14:05:56.217087 15179 caffe.cpp:313] Batch 68, accuracy/top5 = 1
I0629 14:05:56.217090 15179 caffe.cpp:313] Batch 68, loss = 0.357861
I0629 14:05:56.225951 15179 caffe.cpp:313] Batch 69, accuracy/top1 = 0.92
I0629 14:05:56.225968 15179 caffe.cpp:313] Batch 69, accuracy/top5 = 0.98
I0629 14:05:56.225970 15179 caffe.cpp:313] Batch 69, loss = 0.314714
I0629 14:05:56.234789 15179 caffe.cpp:313] Batch 70, accuracy/top1 = 0.92
I0629 14:05:56.234797 15179 caffe.cpp:313] Batch 70, accuracy/top5 = 0.98
I0629 14:05:56.234799 15179 caffe.cpp:313] Batch 70, loss = 0.444917
I0629 14:05:56.243644 15179 caffe.cpp:313] Batch 71, accuracy/top1 = 0.92
I0629 14:05:56.243652 15179 caffe.cpp:313] Batch 71, accuracy/top5 = 1
I0629 14:05:56.243655 15179 caffe.cpp:313] Batch 71, loss = 0.345279
I0629 14:05:56.252363 15179 caffe.cpp:313] Batch 72, accuracy/top1 = 0.88
I0629 14:05:56.252377 15179 caffe.cpp:313] Batch 72, accuracy/top5 = 0.96
I0629 14:05:56.252379 15179 caffe.cpp:313] Batch 72, loss = 0.579998
I0629 14:05:56.261251 15179 caffe.cpp:313] Batch 73, accuracy/top1 = 0.94
I0629 14:05:56.261260 15179 caffe.cpp:313] Batch 73, accuracy/top5 = 1
I0629 14:05:56.261263 15179 caffe.cpp:313] Batch 73, loss = 0.308165
I0629 14:05:56.270061 15179 caffe.cpp:313] Batch 74, accuracy/top1 = 0.96
I0629 14:05:56.270069 15179 caffe.cpp:313] Batch 74, accuracy/top5 = 1
I0629 14:05:56.270071 15179 caffe.cpp:313] Batch 74, loss = 0.133947
I0629 14:05:56.278892 15179 caffe.cpp:313] Batch 75, accuracy/top1 = 0.86
I0629 14:05:56.278899 15179 caffe.cpp:313] Batch 75, accuracy/top5 = 1
I0629 14:05:56.278903 15179 caffe.cpp:313] Batch 75, loss = 0.563858
I0629 14:05:56.287755 15179 caffe.cpp:313] Batch 76, accuracy/top1 = 0.94
I0629 14:05:56.287770 15179 caffe.cpp:313] Batch 76, accuracy/top5 = 1
I0629 14:05:56.287771 15179 caffe.cpp:313] Batch 76, loss = 0.183117
I0629 14:05:56.296620 15179 caffe.cpp:313] Batch 77, accuracy/top1 = 0.94
I0629 14:05:56.296628 15179 caffe.cpp:313] Batch 77, accuracy/top5 = 1
I0629 14:05:56.296630 15179 caffe.cpp:313] Batch 77, loss = 0.318662
I0629 14:05:56.305423 15179 caffe.cpp:313] Batch 78, accuracy/top1 = 0.96
I0629 14:05:56.305430 15179 caffe.cpp:313] Batch 78, accuracy/top5 = 1
I0629 14:05:56.305433 15179 caffe.cpp:313] Batch 78, loss = 0.104554
I0629 14:05:56.314255 15179 caffe.cpp:313] Batch 79, accuracy/top1 = 0.96
I0629 14:05:56.314270 15179 caffe.cpp:313] Batch 79, accuracy/top5 = 0.98
I0629 14:05:56.314272 15179 caffe.cpp:313] Batch 79, loss = 0.155901
I0629 14:05:56.323112 15179 caffe.cpp:313] Batch 80, accuracy/top1 = 0.88
I0629 14:05:56.323120 15179 caffe.cpp:313] Batch 80, accuracy/top5 = 1
I0629 14:05:56.323123 15179 caffe.cpp:313] Batch 80, loss = 0.404304
I0629 14:05:56.331929 15179 caffe.cpp:313] Batch 81, accuracy/top1 = 0.9
I0629 14:05:56.331938 15179 caffe.cpp:313] Batch 81, accuracy/top5 = 1
I0629 14:05:56.331939 15179 caffe.cpp:313] Batch 81, loss = 0.25662
I0629 14:05:56.340785 15179 caffe.cpp:313] Batch 82, accuracy/top1 = 0.86
I0629 14:05:56.340796 15179 caffe.cpp:313] Batch 82, accuracy/top5 = 1
I0629 14:05:56.340800 15179 caffe.cpp:313] Batch 82, loss = 0.386467
I0629 14:05:56.349637 15179 caffe.cpp:313] Batch 83, accuracy/top1 = 0.9
I0629 14:05:56.349647 15179 caffe.cpp:313] Batch 83, accuracy/top5 = 1
I0629 14:05:56.349649 15179 caffe.cpp:313] Batch 83, loss = 0.237622
I0629 14:05:56.358515 15179 caffe.cpp:313] Batch 84, accuracy/top1 = 0.94
I0629 14:05:56.358523 15179 caffe.cpp:313] Batch 84, accuracy/top5 = 0.98
I0629 14:05:56.358526 15179 caffe.cpp:313] Batch 84, loss = 0.227262
I0629 14:05:56.367317 15179 caffe.cpp:313] Batch 85, accuracy/top1 = 0.98
I0629 14:05:56.367326 15179 caffe.cpp:313] Batch 85, accuracy/top5 = 1
I0629 14:05:56.367328 15179 caffe.cpp:313] Batch 85, loss = 0.0551999
I0629 14:05:56.376188 15179 caffe.cpp:313] Batch 86, accuracy/top1 = 0.92
I0629 14:05:56.376205 15179 caffe.cpp:313] Batch 86, accuracy/top5 = 1
I0629 14:05:56.376220 15179 caffe.cpp:313] Batch 86, loss = 0.495665
I0629 14:05:56.385042 15179 caffe.cpp:313] Batch 87, accuracy/top1 = 0.98
I0629 14:05:56.385051 15179 caffe.cpp:313] Batch 87, accuracy/top5 = 1
I0629 14:05:56.385052 15179 caffe.cpp:313] Batch 87, loss = 0.0877222
I0629 14:05:56.393877 15179 caffe.cpp:313] Batch 88, accuracy/top1 = 0.92
I0629 14:05:56.393884 15179 caffe.cpp:313] Batch 88, accuracy/top5 = 1
I0629 14:05:56.393887 15179 caffe.cpp:313] Batch 88, loss = 0.305231
I0629 14:05:56.402709 15179 caffe.cpp:313] Batch 89, accuracy/top1 = 0.9
I0629 14:05:56.402721 15179 caffe.cpp:313] Batch 89, accuracy/top5 = 0.98
I0629 14:05:56.402724 15179 caffe.cpp:313] Batch 89, loss = 0.454218
I0629 14:05:56.411590 15179 caffe.cpp:313] Batch 90, accuracy/top1 = 0.9
I0629 14:05:56.411599 15179 caffe.cpp:313] Batch 90, accuracy/top5 = 1
I0629 14:05:56.411602 15179 caffe.cpp:313] Batch 90, loss = 0.334359
I0629 14:05:56.420403 15179 caffe.cpp:313] Batch 91, accuracy/top1 = 0.86
I0629 14:05:56.420411 15179 caffe.cpp:313] Batch 91, accuracy/top5 = 0.98
I0629 14:05:56.420414 15179 caffe.cpp:313] Batch 91, loss = 0.413313
I0629 14:05:56.429241 15179 caffe.cpp:313] Batch 92, accuracy/top1 = 0.9
I0629 14:05:56.429250 15179 caffe.cpp:313] Batch 92, accuracy/top5 = 1
I0629 14:05:56.429251 15179 caffe.cpp:313] Batch 92, loss = 0.361656
I0629 14:05:56.438122 15179 caffe.cpp:313] Batch 93, accuracy/top1 = 0.96
I0629 14:05:56.438138 15179 caffe.cpp:313] Batch 93, accuracy/top5 = 1
I0629 14:05:56.438140 15179 caffe.cpp:313] Batch 93, loss = 0.13497
I0629 14:05:56.446997 15179 caffe.cpp:313] Batch 94, accuracy/top1 = 0.86
I0629 14:05:56.447005 15179 caffe.cpp:313] Batch 94, accuracy/top5 = 0.98
I0629 14:05:56.447007 15179 caffe.cpp:313] Batch 94, loss = 0.634508
I0629 14:05:56.455813 15179 caffe.cpp:313] Batch 95, accuracy/top1 = 0.84
I0629 14:05:56.455821 15179 caffe.cpp:313] Batch 95, accuracy/top5 = 0.98
I0629 14:05:56.455823 15179 caffe.cpp:313] Batch 95, loss = 0.640646
I0629 14:05:56.464648 15179 caffe.cpp:313] Batch 96, accuracy/top1 = 1
I0629 14:05:56.464660 15179 caffe.cpp:313] Batch 96, accuracy/top5 = 1
I0629 14:05:56.464663 15179 caffe.cpp:313] Batch 96, loss = 0.046008
I0629 14:05:56.473503 15179 caffe.cpp:313] Batch 97, accuracy/top1 = 0.94
I0629 14:05:56.473512 15179 caffe.cpp:313] Batch 97, accuracy/top5 = 1
I0629 14:05:56.473515 15179 caffe.cpp:313] Batch 97, loss = 0.116041
I0629 14:05:56.482313 15179 caffe.cpp:313] Batch 98, accuracy/top1 = 0.9
I0629 14:05:56.482321 15179 caffe.cpp:313] Batch 98, accuracy/top5 = 1
I0629 14:05:56.482324 15179 caffe.cpp:313] Batch 98, loss = 0.398619
I0629 14:05:56.491158 15179 caffe.cpp:313] Batch 99, accuracy/top1 = 0.82
I0629 14:05:56.491165 15179 caffe.cpp:313] Batch 99, accuracy/top5 = 0.96
I0629 14:05:56.491168 15179 caffe.cpp:313] Batch 99, loss = 0.88456
I0629 14:05:56.500017 15179 caffe.cpp:313] Batch 100, accuracy/top1 = 0.94
I0629 14:05:56.500035 15179 caffe.cpp:313] Batch 100, accuracy/top5 = 1
I0629 14:05:56.500037 15179 caffe.cpp:313] Batch 100, loss = 0.166234
I0629 14:05:56.508896 15179 caffe.cpp:313] Batch 101, accuracy/top1 = 0.94
I0629 14:05:56.508904 15179 caffe.cpp:313] Batch 101, accuracy/top5 = 1
I0629 14:05:56.508908 15179 caffe.cpp:313] Batch 101, loss = 0.179935
I0629 14:05:56.517683 15179 caffe.cpp:313] Batch 102, accuracy/top1 = 0.94
I0629 14:05:56.517689 15179 caffe.cpp:313] Batch 102, accuracy/top5 = 1
I0629 14:05:56.517693 15179 caffe.cpp:313] Batch 102, loss = 0.140146
I0629 14:05:56.526553 15179 caffe.cpp:313] Batch 103, accuracy/top1 = 0.88
I0629 14:05:56.526566 15179 caffe.cpp:313] Batch 103, accuracy/top5 = 1
I0629 14:05:56.526569 15179 caffe.cpp:313] Batch 103, loss = 0.604123
I0629 14:05:56.535399 15179 caffe.cpp:313] Batch 104, accuracy/top1 = 0.88
I0629 14:05:56.535408 15179 caffe.cpp:313] Batch 104, accuracy/top5 = 0.98
I0629 14:05:56.535411 15179 caffe.cpp:313] Batch 104, loss = 0.535996
I0629 14:05:56.544265 15179 caffe.cpp:313] Batch 105, accuracy/top1 = 0.92
I0629 14:05:56.544271 15179 caffe.cpp:313] Batch 105, accuracy/top5 = 1
I0629 14:05:56.544282 15179 caffe.cpp:313] Batch 105, loss = 0.271488
I0629 14:05:56.553074 15179 caffe.cpp:313] Batch 106, accuracy/top1 = 0.92
I0629 14:05:56.553082 15179 caffe.cpp:313] Batch 106, accuracy/top5 = 1
I0629 14:05:56.553086 15179 caffe.cpp:313] Batch 106, loss = 0.249427
I0629 14:05:56.561856 15179 caffe.cpp:313] Batch 107, accuracy/top1 = 0.88
I0629 14:05:56.561872 15179 caffe.cpp:313] Batch 107, accuracy/top5 = 0.98
I0629 14:05:56.561874 15179 caffe.cpp:313] Batch 107, loss = 0.474796
I0629 14:05:56.570580 15179 caffe.cpp:313] Batch 108, accuracy/top1 = 0.9
I0629 14:05:56.570588 15179 caffe.cpp:313] Batch 108, accuracy/top5 = 1
I0629 14:05:56.570590 15179 caffe.cpp:313] Batch 108, loss = 0.335896
I0629 14:05:56.579437 15179 caffe.cpp:313] Batch 109, accuracy/top1 = 0.88
I0629 14:05:56.579445 15179 caffe.cpp:313] Batch 109, accuracy/top5 = 1
I0629 14:05:56.579447 15179 caffe.cpp:313] Batch 109, loss = 0.394468
I0629 14:05:56.588287 15179 caffe.cpp:313] Batch 110, accuracy/top1 = 0.88
I0629 14:05:56.588300 15179 caffe.cpp:313] Batch 110, accuracy/top5 = 1
I0629 14:05:56.588302 15179 caffe.cpp:313] Batch 110, loss = 0.689324
I0629 14:05:56.597148 15179 caffe.cpp:313] Batch 111, accuracy/top1 = 0.96
I0629 14:05:56.597157 15179 caffe.cpp:313] Batch 111, accuracy/top5 = 1
I0629 14:05:56.597159 15179 caffe.cpp:313] Batch 111, loss = 0.333748
I0629 14:05:56.605996 15179 caffe.cpp:313] Batch 112, accuracy/top1 = 0.86
I0629 14:05:56.606004 15179 caffe.cpp:313] Batch 112, accuracy/top5 = 1
I0629 14:05:56.606007 15179 caffe.cpp:313] Batch 112, loss = 0.567695
I0629 14:05:56.614827 15179 caffe.cpp:313] Batch 113, accuracy/top1 = 0.92
I0629 14:05:56.614840 15179 caffe.cpp:313] Batch 113, accuracy/top5 = 1
I0629 14:05:56.614842 15179 caffe.cpp:313] Batch 113, loss = 0.135976
I0629 14:05:56.623684 15179 caffe.cpp:313] Batch 114, accuracy/top1 = 0.9
I0629 14:05:56.623694 15179 caffe.cpp:313] Batch 114, accuracy/top5 = 1
I0629 14:05:56.623697 15179 caffe.cpp:313] Batch 114, loss = 0.389026
I0629 14:05:56.632510 15179 caffe.cpp:313] Batch 115, accuracy/top1 = 0.94
I0629 14:05:56.632519 15179 caffe.cpp:313] Batch 115, accuracy/top5 = 0.98
I0629 14:05:56.632521 15179 caffe.cpp:313] Batch 115, loss = 0.147252
I0629 14:05:56.641330 15179 caffe.cpp:313] Batch 116, accuracy/top1 = 0.9
I0629 14:05:56.641338 15179 caffe.cpp:313] Batch 116, accuracy/top5 = 1
I0629 14:05:56.641340 15179 caffe.cpp:313] Batch 116, loss = 0.550396
I0629 14:05:56.650183 15179 caffe.cpp:313] Batch 117, accuracy/top1 = 0.82
I0629 14:05:56.650200 15179 caffe.cpp:313] Batch 117, accuracy/top5 = 0.98
I0629 14:05:56.650202 15179 caffe.cpp:313] Batch 117, loss = 0.695791
I0629 14:05:56.659056 15179 caffe.cpp:313] Batch 118, accuracy/top1 = 0.9
I0629 14:05:56.659065 15179 caffe.cpp:313] Batch 118, accuracy/top5 = 1
I0629 14:05:56.659067 15179 caffe.cpp:313] Batch 118, loss = 0.321466
I0629 14:05:56.667868 15179 caffe.cpp:313] Batch 119, accuracy/top1 = 0.94
I0629 14:05:56.667876 15179 caffe.cpp:313] Batch 119, accuracy/top5 = 1
I0629 14:05:56.667879 15179 caffe.cpp:313] Batch 119, loss = 0.130195
I0629 14:05:56.676717 15179 caffe.cpp:313] Batch 120, accuracy/top1 = 0.88
I0629 14:05:56.676729 15179 caffe.cpp:313] Batch 120, accuracy/top5 = 1
I0629 14:05:56.676733 15179 caffe.cpp:313] Batch 120, loss = 0.316664
I0629 14:05:56.685561 15179 caffe.cpp:313] Batch 121, accuracy/top1 = 0.88
I0629 14:05:56.685570 15179 caffe.cpp:313] Batch 121, accuracy/top5 = 1
I0629 14:05:56.685572 15179 caffe.cpp:313] Batch 121, loss = 0.496821
I0629 14:05:56.694437 15179 caffe.cpp:313] Batch 122, accuracy/top1 = 0.86
I0629 14:05:56.694444 15179 caffe.cpp:313] Batch 122, accuracy/top5 = 1
I0629 14:05:56.694447 15179 caffe.cpp:313] Batch 122, loss = 0.294566
I0629 14:05:56.703233 15179 caffe.cpp:313] Batch 123, accuracy/top1 = 0.88
I0629 14:05:56.703240 15179 caffe.cpp:313] Batch 123, accuracy/top5 = 1
I0629 14:05:56.703243 15179 caffe.cpp:313] Batch 123, loss = 0.307435
I0629 14:05:56.712266 15179 caffe.cpp:313] Batch 124, accuracy/top1 = 0.88
I0629 14:05:56.712309 15179 caffe.cpp:313] Batch 124, accuracy/top5 = 0.98
I0629 14:05:56.712314 15179 caffe.cpp:313] Batch 124, loss = 0.517442
I0629 14:05:56.721184 15179 caffe.cpp:313] Batch 125, accuracy/top1 = 0.94
I0629 14:05:56.721194 15179 caffe.cpp:313] Batch 125, accuracy/top5 = 1
I0629 14:05:56.721196 15179 caffe.cpp:313] Batch 125, loss = 0.338362
I0629 14:05:56.730046 15179 caffe.cpp:313] Batch 126, accuracy/top1 = 1
I0629 14:05:56.730056 15179 caffe.cpp:313] Batch 126, accuracy/top5 = 1
I0629 14:05:56.730058 15179 caffe.cpp:313] Batch 126, loss = 0.0210553
I0629 14:05:56.738909 15179 caffe.cpp:313] Batch 127, accuracy/top1 = 0.96
I0629 14:05:56.738922 15179 caffe.cpp:313] Batch 127, accuracy/top5 = 1
I0629 14:05:56.738925 15179 caffe.cpp:313] Batch 127, loss = 0.114014
I0629 14:05:56.747781 15179 caffe.cpp:313] Batch 128, accuracy/top1 = 0.8
I0629 14:05:56.747790 15179 caffe.cpp:313] Batch 128, accuracy/top5 = 1
I0629 14:05:56.747793 15179 caffe.cpp:313] Batch 128, loss = 0.452436
I0629 14:05:56.756606 15179 caffe.cpp:313] Batch 129, accuracy/top1 = 0.88
I0629 14:05:56.756614 15179 caffe.cpp:313] Batch 129, accuracy/top5 = 1
I0629 14:05:56.756618 15179 caffe.cpp:313] Batch 129, loss = 0.378014
I0629 14:05:56.765419 15179 caffe.cpp:313] Batch 130, accuracy/top1 = 0.88
I0629 14:05:56.765426 15179 caffe.cpp:313] Batch 130, accuracy/top5 = 1
I0629 14:05:56.765429 15179 caffe.cpp:313] Batch 130, loss = 0.465846
I0629 14:05:56.774338 15179 caffe.cpp:313] Batch 131, accuracy/top1 = 0.92
I0629 14:05:56.774355 15179 caffe.cpp:313] Batch 131, accuracy/top5 = 1
I0629 14:05:56.774358 15179 caffe.cpp:313] Batch 131, loss = 0.223119
I0629 14:05:56.783166 15179 caffe.cpp:313] Batch 132, accuracy/top1 = 0.98
I0629 14:05:56.783174 15179 caffe.cpp:313] Batch 132, accuracy/top5 = 1
I0629 14:05:56.783177 15179 caffe.cpp:313] Batch 132, loss = 0.124597
I0629 14:05:56.792019 15179 caffe.cpp:313] Batch 133, accuracy/top1 = 0.92
I0629 14:05:56.792027 15179 caffe.cpp:313] Batch 133, accuracy/top5 = 1
I0629 14:05:56.792029 15179 caffe.cpp:313] Batch 133, loss = 0.282957
I0629 14:05:56.800850 15179 caffe.cpp:313] Batch 134, accuracy/top1 = 0.96
I0629 14:05:56.800865 15179 caffe.cpp:313] Batch 134, accuracy/top5 = 1
I0629 14:05:56.800868 15179 caffe.cpp:313] Batch 134, loss = 0.109786
I0629 14:05:56.809734 15179 caffe.cpp:313] Batch 135, accuracy/top1 = 0.88
I0629 14:05:56.809744 15179 caffe.cpp:313] Batch 135, accuracy/top5 = 0.98
I0629 14:05:56.809746 15179 caffe.cpp:313] Batch 135, loss = 0.409425
I0629 14:05:56.818558 15179 caffe.cpp:313] Batch 136, accuracy/top1 = 0.96
I0629 14:05:56.818567 15179 caffe.cpp:313] Batch 136, accuracy/top5 = 1
I0629 14:05:56.818569 15179 caffe.cpp:313] Batch 136, loss = 0.0595348
I0629 14:05:56.827410 15179 caffe.cpp:313] Batch 137, accuracy/top1 = 0.82
I0629 14:05:56.827419 15179 caffe.cpp:313] Batch 137, accuracy/top5 = 1
I0629 14:05:56.827420 15179 caffe.cpp:313] Batch 137, loss = 0.462451
I0629 14:05:56.836273 15179 caffe.cpp:313] Batch 138, accuracy/top1 = 0.94
I0629 14:05:56.836288 15179 caffe.cpp:313] Batch 138, accuracy/top5 = 0.98
I0629 14:05:56.836292 15179 caffe.cpp:313] Batch 138, loss = 0.237493
I0629 14:05:56.845158 15179 caffe.cpp:313] Batch 139, accuracy/top1 = 0.86
I0629 14:05:56.845166 15179 caffe.cpp:313] Batch 139, accuracy/top5 = 1
I0629 14:05:56.845170 15179 caffe.cpp:313] Batch 139, loss = 0.548756
I0629 14:05:56.853984 15179 caffe.cpp:313] Batch 140, accuracy/top1 = 0.9
I0629 14:05:56.853992 15179 caffe.cpp:313] Batch 140, accuracy/top5 = 1
I0629 14:05:56.853994 15179 caffe.cpp:313] Batch 140, loss = 0.354012
I0629 14:05:56.862854 15179 caffe.cpp:313] Batch 141, accuracy/top1 = 0.92
I0629 14:05:56.862867 15179 caffe.cpp:313] Batch 141, accuracy/top5 = 1
I0629 14:05:56.862870 15179 caffe.cpp:313] Batch 141, loss = 0.365016
I0629 14:05:56.871726 15179 caffe.cpp:313] Batch 142, accuracy/top1 = 0.9
I0629 14:05:56.871734 15179 caffe.cpp:313] Batch 142, accuracy/top5 = 1
I0629 14:05:56.871737 15179 caffe.cpp:313] Batch 142, loss = 0.277866
I0629 14:05:56.880550 15179 caffe.cpp:313] Batch 143, accuracy/top1 = 0.94
I0629 14:05:56.880566 15179 caffe.cpp:313] Batch 143, accuracy/top5 = 1
I0629 14:05:56.880569 15179 caffe.cpp:313] Batch 143, loss = 0.224683
I0629 14:05:56.889425 15179 caffe.cpp:313] Batch 144, accuracy/top1 = 0.94
I0629 14:05:56.889436 15179 caffe.cpp:313] Batch 144, accuracy/top5 = 1
I0629 14:05:56.889439 15179 caffe.cpp:313] Batch 144, loss = 0.224579
I0629 14:05:56.898288 15179 caffe.cpp:313] Batch 145, accuracy/top1 = 0.96
I0629 14:05:56.898298 15179 caffe.cpp:313] Batch 145, accuracy/top5 = 1
I0629 14:05:56.898300 15179 caffe.cpp:313] Batch 145, loss = 0.221321
I0629 14:05:56.907104 15179 caffe.cpp:313] Batch 146, accuracy/top1 = 0.92
I0629 14:05:56.907111 15179 caffe.cpp:313] Batch 146, accuracy/top5 = 1
I0629 14:05:56.907114 15179 caffe.cpp:313] Batch 146, loss = 0.172173
I0629 14:05:56.915905 15179 caffe.cpp:313] Batch 147, accuracy/top1 = 0.92
I0629 14:05:56.915913 15179 caffe.cpp:313] Batch 147, accuracy/top5 = 1
I0629 14:05:56.915916 15179 caffe.cpp:313] Batch 147, loss = 0.234113
I0629 14:05:56.924788 15179 caffe.cpp:313] Batch 148, accuracy/top1 = 0.9
I0629 14:05:56.924808 15179 caffe.cpp:313] Batch 148, accuracy/top5 = 1
I0629 14:05:56.924811 15179 caffe.cpp:313] Batch 148, loss = 0.363068
I0629 14:05:56.933666 15179 caffe.cpp:313] Batch 149, accuracy/top1 = 0.88
I0629 14:05:56.933676 15179 caffe.cpp:313] Batch 149, accuracy/top5 = 1
I0629 14:05:56.933677 15179 caffe.cpp:313] Batch 149, loss = 0.207936
I0629 14:05:56.942476 15179 caffe.cpp:313] Batch 150, accuracy/top1 = 0.9
I0629 14:05:56.942483 15179 caffe.cpp:313] Batch 150, accuracy/top5 = 0.98
I0629 14:05:56.942487 15179 caffe.cpp:313] Batch 150, loss = 0.269476
I0629 14:05:56.951285 15179 caffe.cpp:313] Batch 151, accuracy/top1 = 0.94
I0629 14:05:56.951297 15179 caffe.cpp:313] Batch 151, accuracy/top5 = 1
I0629 14:05:56.951301 15179 caffe.cpp:313] Batch 151, loss = 0.259803
I0629 14:05:56.960180 15179 caffe.cpp:313] Batch 152, accuracy/top1 = 0.86
I0629 14:05:56.960189 15179 caffe.cpp:313] Batch 152, accuracy/top5 = 1
I0629 14:05:56.960191 15179 caffe.cpp:313] Batch 152, loss = 0.414384
I0629 14:05:56.968989 15179 caffe.cpp:313] Batch 153, accuracy/top1 = 0.94
I0629 14:05:56.968997 15179 caffe.cpp:313] Batch 153, accuracy/top5 = 0.98
I0629 14:05:56.968999 15179 caffe.cpp:313] Batch 153, loss = 0.364942
I0629 14:05:56.977820 15179 caffe.cpp:313] Batch 154, accuracy/top1 = 0.96
I0629 14:05:56.977828 15179 caffe.cpp:313] Batch 154, accuracy/top5 = 1
I0629 14:05:56.977831 15179 caffe.cpp:313] Batch 154, loss = 0.129967
I0629 14:05:56.986671 15179 caffe.cpp:313] Batch 155, accuracy/top1 = 0.88
I0629 14:05:56.986690 15179 caffe.cpp:313] Batch 155, accuracy/top5 = 1
I0629 14:05:56.986692 15179 caffe.cpp:313] Batch 155, loss = 0.477463
I0629 14:05:56.995561 15179 caffe.cpp:313] Batch 156, accuracy/top1 = 0.88
I0629 14:05:56.995569 15179 caffe.cpp:313] Batch 156, accuracy/top5 = 1
I0629 14:05:56.995571 15179 caffe.cpp:313] Batch 156, loss = 0.475964
I0629 14:05:57.004359 15179 caffe.cpp:313] Batch 157, accuracy/top1 = 0.92
I0629 14:05:57.004367 15179 caffe.cpp:313] Batch 157, accuracy/top5 = 1
I0629 14:05:57.004370 15179 caffe.cpp:313] Batch 157, loss = 0.337381
I0629 14:05:57.013211 15179 caffe.cpp:313] Batch 158, accuracy/top1 = 0.86
I0629 14:05:57.013223 15179 caffe.cpp:313] Batch 158, accuracy/top5 = 1
I0629 14:05:57.013226 15179 caffe.cpp:313] Batch 158, loss = 0.324694
I0629 14:05:57.022076 15179 caffe.cpp:313] Batch 159, accuracy/top1 = 0.86
I0629 14:05:57.022086 15179 caffe.cpp:313] Batch 159, accuracy/top5 = 1
I0629 14:05:57.022089 15179 caffe.cpp:313] Batch 159, loss = 0.269764
I0629 14:05:57.030902 15179 caffe.cpp:313] Batch 160, accuracy/top1 = 0.94
I0629 14:05:57.030911 15179 caffe.cpp:313] Batch 160, accuracy/top5 = 1
I0629 14:05:57.030913 15179 caffe.cpp:313] Batch 160, loss = 0.197345
I0629 14:05:57.039718 15179 caffe.cpp:313] Batch 161, accuracy/top1 = 0.96
I0629 14:05:57.039726 15179 caffe.cpp:313] Batch 161, accuracy/top5 = 1
I0629 14:05:57.039729 15179 caffe.cpp:313] Batch 161, loss = 0.14668
I0629 14:05:57.048593 15179 caffe.cpp:313] Batch 162, accuracy/top1 = 0.94
I0629 14:05:57.048609 15179 caffe.cpp:313] Batch 162, accuracy/top5 = 1
I0629 14:05:57.048612 15179 caffe.cpp:313] Batch 162, loss = 0.28568
I0629 14:05:57.057473 15179 caffe.cpp:313] Batch 163, accuracy/top1 = 0.96
I0629 14:05:57.057482 15179 caffe.cpp:313] Batch 163, accuracy/top5 = 0.98
I0629 14:05:57.057484 15179 caffe.cpp:313] Batch 163, loss = 0.218996
I0629 14:05:57.066274 15179 caffe.cpp:313] Batch 164, accuracy/top1 = 0.86
I0629 14:05:57.066283 15179 caffe.cpp:313] Batch 164, accuracy/top5 = 1
I0629 14:05:57.066287 15179 caffe.cpp:313] Batch 164, loss = 0.517037
I0629 14:05:57.075152 15179 caffe.cpp:313] Batch 165, accuracy/top1 = 0.92
I0629 14:05:57.075166 15179 caffe.cpp:313] Batch 165, accuracy/top5 = 1
I0629 14:05:57.075170 15179 caffe.cpp:313] Batch 165, loss = 0.452105
I0629 14:05:57.083991 15179 caffe.cpp:313] Batch 166, accuracy/top1 = 0.96
I0629 14:05:57.083999 15179 caffe.cpp:313] Batch 166, accuracy/top5 = 1
I0629 14:05:57.084002 15179 caffe.cpp:313] Batch 166, loss = 0.126141
I0629 14:05:57.092818 15179 caffe.cpp:313] Batch 167, accuracy/top1 = 0.94
I0629 14:05:57.092825 15179 caffe.cpp:313] Batch 167, accuracy/top5 = 1
I0629 14:05:57.092828 15179 caffe.cpp:313] Batch 167, loss = 0.190324
I0629 14:05:57.101606 15179 caffe.cpp:313] Batch 168, accuracy/top1 = 0.88
I0629 14:05:57.101614 15179 caffe.cpp:313] Batch 168, accuracy/top5 = 1
I0629 14:05:57.101617 15179 caffe.cpp:313] Batch 168, loss = 0.430708
I0629 14:05:57.110488 15179 caffe.cpp:313] Batch 169, accuracy/top1 = 0.8
I0629 14:05:57.110507 15179 caffe.cpp:313] Batch 169, accuracy/top5 = 1
I0629 14:05:57.110508 15179 caffe.cpp:313] Batch 169, loss = 0.576801
I0629 14:05:57.119339 15179 caffe.cpp:313] Batch 170, accuracy/top1 = 0.88
I0629 14:05:57.119348 15179 caffe.cpp:313] Batch 170, accuracy/top5 = 1
I0629 14:05:57.119350 15179 caffe.cpp:313] Batch 170, loss = 0.613094
I0629 14:05:57.128175 15179 caffe.cpp:313] Batch 171, accuracy/top1 = 0.86
I0629 14:05:57.128183 15179 caffe.cpp:313] Batch 171, accuracy/top5 = 1
I0629 14:05:57.128185 15179 caffe.cpp:313] Batch 171, loss = 0.557645
I0629 14:05:57.136888 15179 caffe.cpp:313] Batch 172, accuracy/top1 = 0.94
I0629 14:05:57.136900 15179 caffe.cpp:313] Batch 172, accuracy/top5 = 1
I0629 14:05:57.136904 15179 caffe.cpp:313] Batch 172, loss = 0.355065
I0629 14:05:57.145788 15179 caffe.cpp:313] Batch 173, accuracy/top1 = 0.98
I0629 14:05:57.145797 15179 caffe.cpp:313] Batch 173, accuracy/top5 = 1
I0629 14:05:57.145800 15179 caffe.cpp:313] Batch 173, loss = 0.0744457
I0629 14:05:57.154580 15179 caffe.cpp:313] Batch 174, accuracy/top1 = 0.84
I0629 14:05:57.154588 15179 caffe.cpp:313] Batch 174, accuracy/top5 = 1
I0629 14:05:57.154590 15179 caffe.cpp:313] Batch 174, loss = 0.640624
I0629 14:05:57.163444 15179 caffe.cpp:313] Batch 175, accuracy/top1 = 0.94
I0629 14:05:57.163451 15179 caffe.cpp:313] Batch 175, accuracy/top5 = 1
I0629 14:05:57.163453 15179 caffe.cpp:313] Batch 175, loss = 0.264908
I0629 14:05:57.172302 15179 caffe.cpp:313] Batch 176, accuracy/top1 = 0.9
I0629 14:05:57.172315 15179 caffe.cpp:313] Batch 176, accuracy/top5 = 1
I0629 14:05:57.172318 15179 caffe.cpp:313] Batch 176, loss = 0.199097
I0629 14:05:57.181020 15179 caffe.cpp:313] Batch 177, accuracy/top1 = 0.98
I0629 14:05:57.181027 15179 caffe.cpp:313] Batch 177, accuracy/top5 = 1
I0629 14:05:57.181030 15179 caffe.cpp:313] Batch 177, loss = 0.0928101
I0629 14:05:57.189822 15179 caffe.cpp:313] Batch 178, accuracy/top1 = 0.86
I0629 14:05:57.189831 15179 caffe.cpp:313] Batch 178, accuracy/top5 = 1
I0629 14:05:57.189832 15179 caffe.cpp:313] Batch 178, loss = 0.438751
I0629 14:05:57.198642 15179 caffe.cpp:313] Batch 179, accuracy/top1 = 0.92
I0629 14:05:57.198655 15179 caffe.cpp:313] Batch 179, accuracy/top5 = 1
I0629 14:05:57.198657 15179 caffe.cpp:313] Batch 179, loss = 0.379267
I0629 14:05:57.207556 15179 caffe.cpp:313] Batch 180, accuracy/top1 = 0.9
I0629 14:05:57.207566 15179 caffe.cpp:313] Batch 180, accuracy/top5 = 1
I0629 14:05:57.207568 15179 caffe.cpp:313] Batch 180, loss = 0.420223
I0629 14:05:57.216375 15179 caffe.cpp:313] Batch 181, accuracy/top1 = 0.92
I0629 14:05:57.216383 15179 caffe.cpp:313] Batch 181, accuracy/top5 = 1
I0629 14:05:57.216385 15179 caffe.cpp:313] Batch 181, loss = 0.219923
I0629 14:05:57.225240 15179 caffe.cpp:313] Batch 182, accuracy/top1 = 0.9
I0629 14:05:57.225252 15179 caffe.cpp:313] Batch 182, accuracy/top5 = 0.98
I0629 14:05:57.225255 15179 caffe.cpp:313] Batch 182, loss = 0.42175
I0629 14:05:57.234107 15179 caffe.cpp:313] Batch 183, accuracy/top1 = 0.98
I0629 14:05:57.234117 15179 caffe.cpp:313] Batch 183, accuracy/top5 = 1
I0629 14:05:57.234120 15179 caffe.cpp:313] Batch 183, loss = 0.164773
I0629 14:05:57.242983 15179 caffe.cpp:313] Batch 184, accuracy/top1 = 0.88
I0629 14:05:57.242990 15179 caffe.cpp:313] Batch 184, accuracy/top5 = 1
I0629 14:05:57.242993 15179 caffe.cpp:313] Batch 184, loss = 0.862406
I0629 14:05:57.251791 15179 caffe.cpp:313] Batch 185, accuracy/top1 = 0.88
I0629 14:05:57.251798 15179 caffe.cpp:313] Batch 185, accuracy/top5 = 1
I0629 14:05:57.251801 15179 caffe.cpp:313] Batch 185, loss = 0.612708
I0629 14:05:57.260659 15179 caffe.cpp:313] Batch 186, accuracy/top1 = 0.9
I0629 14:05:57.260677 15179 caffe.cpp:313] Batch 186, accuracy/top5 = 1
I0629 14:05:57.260680 15179 caffe.cpp:313] Batch 186, loss = 0.263868
I0629 14:05:57.269518 15179 caffe.cpp:313] Batch 187, accuracy/top1 = 0.84
I0629 14:05:57.269526 15179 caffe.cpp:313] Batch 187, accuracy/top5 = 1
I0629 14:05:57.269529 15179 caffe.cpp:313] Batch 187, loss = 0.628898
I0629 14:05:57.278364 15179 caffe.cpp:313] Batch 188, accuracy/top1 = 0.92
I0629 14:05:57.278372 15179 caffe.cpp:313] Batch 188, accuracy/top5 = 0.98
I0629 14:05:57.278374 15179 caffe.cpp:313] Batch 188, loss = 0.326559
I0629 14:05:57.287170 15179 caffe.cpp:313] Batch 189, accuracy/top1 = 0.9
I0629 14:05:57.287181 15179 caffe.cpp:313] Batch 189, accuracy/top5 = 0.98
I0629 14:05:57.287184 15179 caffe.cpp:313] Batch 189, loss = 0.354482
I0629 14:05:57.296066 15179 caffe.cpp:313] Batch 190, accuracy/top1 = 0.9
I0629 14:05:57.296075 15179 caffe.cpp:313] Batch 190, accuracy/top5 = 1
I0629 14:05:57.296077 15179 caffe.cpp:313] Batch 190, loss = 0.315572
I0629 14:05:57.304891 15179 caffe.cpp:313] Batch 191, accuracy/top1 = 0.9
I0629 14:05:57.304899 15179 caffe.cpp:313] Batch 191, accuracy/top5 = 1
I0629 14:05:57.304901 15179 caffe.cpp:313] Batch 191, loss = 0.25199
I0629 14:05:57.313694 15179 caffe.cpp:313] Batch 192, accuracy/top1 = 0.9
I0629 14:05:57.313701 15179 caffe.cpp:313] Batch 192, accuracy/top5 = 0.98
I0629 14:05:57.313704 15179 caffe.cpp:313] Batch 192, loss = 0.431302
I0629 14:05:57.322582 15179 caffe.cpp:313] Batch 193, accuracy/top1 = 0.96
I0629 14:05:57.322600 15179 caffe.cpp:313] Batch 193, accuracy/top5 = 1
I0629 14:05:57.322603 15179 caffe.cpp:313] Batch 193, loss = 0.0733694
I0629 14:05:57.331430 15179 caffe.cpp:313] Batch 194, accuracy/top1 = 0.92
I0629 14:05:57.331437 15179 caffe.cpp:313] Batch 194, accuracy/top5 = 0.98
I0629 14:05:57.331440 15179 caffe.cpp:313] Batch 194, loss = 0.340824
I0629 14:05:57.340236 15179 caffe.cpp:313] Batch 195, accuracy/top1 = 0.92
I0629 14:05:57.340245 15179 caffe.cpp:313] Batch 195, accuracy/top5 = 1
I0629 14:05:57.340246 15179 caffe.cpp:313] Batch 195, loss = 0.27203
I0629 14:05:57.349057 15179 caffe.cpp:313] Batch 196, accuracy/top1 = 0.8
I0629 14:05:57.349071 15179 caffe.cpp:313] Batch 196, accuracy/top5 = 1
I0629 14:05:57.349073 15179 caffe.cpp:313] Batch 196, loss = 0.646502
I0629 14:05:57.349474 15193 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 14:05:57.357945 15179 caffe.cpp:313] Batch 197, accuracy/top1 = 0.9
I0629 14:05:57.357954 15179 caffe.cpp:313] Batch 197, accuracy/top5 = 1
I0629 14:05:57.357956 15179 caffe.cpp:313] Batch 197, loss = 0.193363
I0629 14:05:57.366761 15179 caffe.cpp:313] Batch 198, accuracy/top1 = 0.92
I0629 14:05:57.366770 15179 caffe.cpp:313] Batch 198, accuracy/top5 = 1
I0629 14:05:57.366771 15179 caffe.cpp:313] Batch 198, loss = 0.222679
I0629 14:05:57.375579 15179 caffe.cpp:313] Batch 199, accuracy/top1 = 0.86
I0629 14:05:57.375596 15179 caffe.cpp:313] Batch 199, accuracy/top5 = 1
I0629 14:05:57.375597 15179 caffe.cpp:313] Batch 199, loss = 0.515246
I0629 14:05:57.375599 15179 caffe.cpp:318] Loss: 0.336866
I0629 14:05:57.375607 15179 caffe.cpp:330] accuracy/top1 = 0.9066
I0629 14:05:57.375612 15179 caffe.cpp:330] accuracy/top5 = 0.9955
I0629 14:05:57.375615 15179 caffe.cpp:330] loss = 0.336866 (* 1 = 0.336866 loss)
